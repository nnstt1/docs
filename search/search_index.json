{"config":{"indexing":"full","lang":["en","ja"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Author \u306e\u306e\u3057 @nnstt1","title":"Author"},{"location":"#author","text":"\u306e\u306e\u3057 @nnstt1","title":"Author"},{"location":"esxi/","text":"ESXi SNMP ssh \u3067 ESXi \u306b\u30ed\u30b0\u30a4\u30f3 esxcli system snmp set --communities public esxcli system snmp set --enable true esxcli network firewall ruleset set --ruleset-id = snmp --allowed-all true esxcli network firewall ruleset set --ruleset-id = snmp --enabled true /etc/init.d/snmpd start","title":"ESXi"},{"location":"esxi/#esxi","text":"","title":"ESXi"},{"location":"esxi/#snmp","text":"ssh \u3067 ESXi \u306b\u30ed\u30b0\u30a4\u30f3 esxcli system snmp set --communities public esxcli system snmp set --enable true esxcli network firewall ruleset set --ruleset-id = snmp --allowed-all true esxcli network firewall ruleset set --ruleset-id = snmp --enabled true /etc/init.d/snmpd start","title":"SNMP"},{"location":"mkdocs/","text":"MkDocs \u9759\u7684\u30b5\u30a4\u30c8\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf MkDocs \u306e\u4f7f\u3044\u65b9\u3067\u3059\u3002 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb pip install mkdocs mkdocs-material mkdocs-minify-plugin fontawesome-markdown \u30d3\u30eb\u30c9 \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u5165\u529b\u5143\u306f docs \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3001\u51fa\u529b\u5148\u306f site \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u3057\u3066\u30d3\u30eb\u30c9\u3055\u308c\u307e\u3059\u3002 \u3057\u304b\u3057\u3001GitHub Pages \u3067\u306f docs \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u53c2\u7167\u3057\u3066\u30da\u30fc\u30b8\u304c\u751f\u6210\u3055\u308c\u308b\u305f\u3081\u3001 mkdocs.yml \u3067\u5165\u529b\u5143\u3068\u51fa\u529b\u5148\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 # mkdocs.yml docs_dir : source site_dir : docs mkdocs build","title":"MkDocs"},{"location":"mkdocs/#mkdocs","text":"\u9759\u7684\u30b5\u30a4\u30c8\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf MkDocs \u306e\u4f7f\u3044\u65b9\u3067\u3059\u3002","title":"MkDocs"},{"location":"mkdocs/#_1","text":"pip install mkdocs mkdocs-material mkdocs-minify-plugin fontawesome-markdown","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"mkdocs/#_2","text":"\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u5165\u529b\u5143\u306f docs \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3001\u51fa\u529b\u5148\u306f site \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u3057\u3066\u30d3\u30eb\u30c9\u3055\u308c\u307e\u3059\u3002 \u3057\u304b\u3057\u3001GitHub Pages \u3067\u306f docs \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u53c2\u7167\u3057\u3066\u30da\u30fc\u30b8\u304c\u751f\u6210\u3055\u308c\u308b\u305f\u3081\u3001 mkdocs.yml \u3067\u5165\u529b\u5143\u3068\u51fa\u529b\u5148\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 # mkdocs.yml docs_dir : source site_dir : docs mkdocs build","title":"\u30d3\u30eb\u30c9"},{"location":"powerdns/","text":"PowerDNS OSS \u306e PowerDNS \u3092\u81ea\u5b85\u30e9\u30dc\u7528 DNS \u30b5\u30fc\u30d0\u3068\u3057\u3066\u69cb\u7bc9\u3057\u307e\u3059\u3002 https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_openstack_platform/7/html/dns-as-a-service_guide/install_and_configure_powerdns PostgreSQL \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo dnf module disable postgresql sudo dnf install -y postgresql13-server sudo /usr/pgsql-13/bin/postgresql-13-setup initdb sudo systemctl enable --now postgresql-13 # postgres \u30e6\u30fc\u30b6\u306e\u30d1\u30b9\u30ef\u30fc\u30c9\u8a2d\u5b9a sudo passwd postgres su - postgres postgres = \\# ALTER ROLE postgres WITH PASSWORD 'postgres' ; postgres = \\# \\\\ q exit # \u30ed\u30b0\u30a4\u30f3\u65b9\u6cd5\u5909\u66f4 sudo vim /var/lib/pgsql/13/data/pg_hba.conf sudo systemctl restart postgresql-13 # postgres \u30e6\u30fc\u30b6\u3067 PowerDNS \u7528\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u4f5c\u6210 psql -U postgres postgres = \\# CREATE ROLE pdns WITH LOGIN PASSWORD 'pdns' ; postgres = \\# CREATE DATABASE pdns OWNER 'pdns' ; postgres = \\# \\\\ q # PowerDNS \u7528\u30b9\u30ad\u30fc\u30de\u4f5c\u6210 wget https://github.com/PowerDNS/pdns/blob/auth-4.5.1/modules/gpgsqlbackend/schema.pgsql.sql psql -U pdns -d pdns -a -f schema.pgsql.sql PowerDNS \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm sudo dnf install -y pdns pdns-backend-postgresql pdns-tools 2021/8/14 \u6642\u70b9\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u306f 4.5.1 \u3067\u3057\u305f\u3002 ================================================================================================================================= \u30d1\u30c3\u30b1\u30fc\u30b8 \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc \u30d0\u30fc\u30b8\u30e7\u30f3 \u30ea\u30dd\u30b8\u30c8\u30ea\u30fc \u30b5\u30a4\u30ba ================================================================================================================================= \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb: pdns x86_64 4.5.1-1.el8 epel 3.6 M pdns-backend-postgresql x86_64 4.5.1-1.el8 epel 67 k pdns-tools x86_64 45.1-1.el8 epel 2.7 M ``` # # PowerDNS \u8a2d\u5b9a ```ini # /etc/pdns/pdns.conf api=yes api-key=7fe343a8-c445-ed83-f815-81a369883984 launch=gpgsql setgid=pdns setuid=pdns webserver=yes webserver-address=0.0.0.0 webserver-allow-from=0.0.0.0/0,::1 gpgsql-host=/run/postgresql gpgsql-dbname=pdns gpgsql-user=pdns gpgsql-password=pdns PowerDNS-Admin PowerDNS \u7528 WebUI \u30c4\u30fc\u30eb PowerDNS-Admin \u3092\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 \u30b3\u30f3\u30c6\u30ca \u30b3\u30f3\u30c6\u30ca\u3092\u4f7f\u3063\u3066 PowerDNS-Admin \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 podman run -d \\ --name pdns_admin \\ --restart always \\ -e SQLALCHEMY_DATABASE_URI = 'postgresql://powerdnsadmin:powerdnsadmin@192.168.2.3/powerdnsadmindb' \\ -e SECRET_KEY = 'a-very-secret-key' \\ -v pda-data:/data \\ -p 9191 :80 \\ ngoduykhanh/powerdns-admin:latest \u4e8b\u524d\u6e96\u5099 \u30b3\u30f3\u30c6\u30ca\u3092\u4f7f\u308f\u305a\u306b PowerDNS \u306e Web UI \u3068\u3057\u3066 PowerDNS-Admin \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 \u4ee5\u4e0b\u306e Wiki \u3092\u53c2\u7167\u3057\u307e\u3059\u3002 https://github.com/ngoduykhanh/PowerDNS-Admin/wiki sudo dnf install -y postgresql-libs python3 python3-devel postgresql13-devel git gcc openldap-devel libxml2-devel python3-xmlsec export PATH = $PATH :/usr/pgsql-13/bin/ pip3 install psycopg2 --user pip3 install virtualenv --user PostgreSQL \u306b PowerDNS-Admin \u7528\u306e\u30e6\u30fc\u30b6\u3068\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 su - postgres createuser powerdnsadmin # postgres \u30e6\u30fc\u30b6\u306e\u30d1\u30b9\u30ef\u30fc\u30c9\u3092\u5165\u529b createdb powerdnsadmindb psql postgres = \\# ALTER USER powerdnsadmin WITH ENCRYPTED PASSWORD 'powerdnsadmin' ; postgres = \\# GRANT ALL PRIVILEGES ON DATABASE powerdnsadmindb TO powerdnsadmin ; postgres = \\# \\\\ q exit \u5f8c\u8ff0\u306e pip \u3067 xmlsec \u3092\u30d3\u30eb\u30c9\u3059\u308b\u969b\u306b\u5fc5\u8981\u306a xmlsec1-devel \u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3067\u306f\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u3002 \u305d\u3053\u3067\u3001\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u306a\u3044\u8ffd\u52a0\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u304c\u65b0\u3057\u3044 CodeReady Linux Builder \u3092\u6709\u52b9\u5316\u3057\u307e\u3059\u3002 CodeReady Linux Builder \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u306f xmlsec1-devel \u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002 https://access.redhat.com/ja/articles/5304171 sudo subscription-manager repos --enable codeready-builder-for-rhel-8-x86_64-rpms sudo yum install -y xmlsec1-devel yarn + Nodejs14 Wiki \u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u308b Nodejs \u306f 10 \u3067\u3059\u304c\u3001\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u53e4\u3044\u305f\u3081 14 \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 curl -sL https://rpm.nodesource.com/setup_14.x | sudo bash - sudo curl -sL https://dl.yarnpkg.com/rpm/yarn.repo -o /etc/yum.repos.d/yarn.repo sudo dnf install -y yarn checkout sudo git clone https://github.com/ngoduykhanh/PowerDNS-Admin.git /opt/web/powerdns-admin sudo chmod -R 777 /opt/web/powerdns-admin cd /opt/web/powerdns-admin virtualenv -p python3 flask . ./flask/bin/activate ( flask ) pip install python-dotenv # requirements.txt \u304b\u3089 mysqlclient \u3092\u524a\u9664\u3057\u3066\u304a\u304f ( flask ) pip install --upgrade pip setuptools wheel ( flask ) pip install -r requirements.txt Running ( flask ) export FLASK_APP = powerdnsadmin/__init__.py ( flask ) flask db upgrade flask db upgrade \u3067\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u305f\u304c\u3001\u3053\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u3067\u30b3\u30f3\u30c6\u30ca\u3092\u4f7f\u3046\u65b9\u6cd5\u304c\u3042\u308b\u3068\u5224\u660e\u3057\u3001\u4e2d\u65ad\u3057\u307e\u3057\u305f\u3002","title":"PowerDNS"},{"location":"powerdns/#powerdns","text":"OSS \u306e PowerDNS \u3092\u81ea\u5b85\u30e9\u30dc\u7528 DNS \u30b5\u30fc\u30d0\u3068\u3057\u3066\u69cb\u7bc9\u3057\u307e\u3059\u3002 https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_openstack_platform/7/html/dns-as-a-service_guide/install_and_configure_powerdns","title":"PowerDNS"},{"location":"powerdns/#postgresql","text":"sudo dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo dnf module disable postgresql sudo dnf install -y postgresql13-server sudo /usr/pgsql-13/bin/postgresql-13-setup initdb sudo systemctl enable --now postgresql-13 # postgres \u30e6\u30fc\u30b6\u306e\u30d1\u30b9\u30ef\u30fc\u30c9\u8a2d\u5b9a sudo passwd postgres su - postgres postgres = \\# ALTER ROLE postgres WITH PASSWORD 'postgres' ; postgres = \\# \\\\ q exit # \u30ed\u30b0\u30a4\u30f3\u65b9\u6cd5\u5909\u66f4 sudo vim /var/lib/pgsql/13/data/pg_hba.conf sudo systemctl restart postgresql-13 # postgres \u30e6\u30fc\u30b6\u3067 PowerDNS \u7528\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u4f5c\u6210 psql -U postgres postgres = \\# CREATE ROLE pdns WITH LOGIN PASSWORD 'pdns' ; postgres = \\# CREATE DATABASE pdns OWNER 'pdns' ; postgres = \\# \\\\ q # PowerDNS \u7528\u30b9\u30ad\u30fc\u30de\u4f5c\u6210 wget https://github.com/PowerDNS/pdns/blob/auth-4.5.1/modules/gpgsqlbackend/schema.pgsql.sql psql -U pdns -d pdns -a -f schema.pgsql.sql","title":"PostgreSQL \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"powerdns/#powerdns_1","text":"sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm sudo dnf install -y pdns pdns-backend-postgresql pdns-tools 2021/8/14 \u6642\u70b9\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u306f 4.5.1 \u3067\u3057\u305f\u3002 ================================================================================================================================= \u30d1\u30c3\u30b1\u30fc\u30b8 \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc \u30d0\u30fc\u30b8\u30e7\u30f3 \u30ea\u30dd\u30b8\u30c8\u30ea\u30fc \u30b5\u30a4\u30ba ================================================================================================================================= \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb: pdns x86_64 4.5.1-1.el8 epel 3.6 M pdns-backend-postgresql x86_64 4.5.1-1.el8 epel 67 k pdns-tools x86_64 45.1-1.el8 epel 2.7 M ``` # # PowerDNS \u8a2d\u5b9a ```ini # /etc/pdns/pdns.conf api=yes api-key=7fe343a8-c445-ed83-f815-81a369883984 launch=gpgsql setgid=pdns setuid=pdns webserver=yes webserver-address=0.0.0.0 webserver-allow-from=0.0.0.0/0,::1 gpgsql-host=/run/postgresql gpgsql-dbname=pdns gpgsql-user=pdns gpgsql-password=pdns","title":"PowerDNS \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"powerdns/#powerdns-admin","text":"PowerDNS \u7528 WebUI \u30c4\u30fc\u30eb PowerDNS-Admin \u3092\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002","title":"PowerDNS-Admin"},{"location":"powerdns/#_1","text":"\u30b3\u30f3\u30c6\u30ca\u3092\u4f7f\u3063\u3066 PowerDNS-Admin \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 podman run -d \\ --name pdns_admin \\ --restart always \\ -e SQLALCHEMY_DATABASE_URI = 'postgresql://powerdnsadmin:powerdnsadmin@192.168.2.3/powerdnsadmindb' \\ -e SECRET_KEY = 'a-very-secret-key' \\ -v pda-data:/data \\ -p 9191 :80 \\ ngoduykhanh/powerdns-admin:latest","title":"\u30b3\u30f3\u30c6\u30ca"},{"location":"powerdns/#_2","text":"\u30b3\u30f3\u30c6\u30ca\u3092\u4f7f\u308f\u305a\u306b PowerDNS \u306e Web UI \u3068\u3057\u3066 PowerDNS-Admin \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 \u4ee5\u4e0b\u306e Wiki \u3092\u53c2\u7167\u3057\u307e\u3059\u3002 https://github.com/ngoduykhanh/PowerDNS-Admin/wiki sudo dnf install -y postgresql-libs python3 python3-devel postgresql13-devel git gcc openldap-devel libxml2-devel python3-xmlsec export PATH = $PATH :/usr/pgsql-13/bin/ pip3 install psycopg2 --user pip3 install virtualenv --user PostgreSQL \u306b PowerDNS-Admin \u7528\u306e\u30e6\u30fc\u30b6\u3068\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 su - postgres createuser powerdnsadmin # postgres \u30e6\u30fc\u30b6\u306e\u30d1\u30b9\u30ef\u30fc\u30c9\u3092\u5165\u529b createdb powerdnsadmindb psql postgres = \\# ALTER USER powerdnsadmin WITH ENCRYPTED PASSWORD 'powerdnsadmin' ; postgres = \\# GRANT ALL PRIVILEGES ON DATABASE powerdnsadmindb TO powerdnsadmin ; postgres = \\# \\\\ q exit \u5f8c\u8ff0\u306e pip \u3067 xmlsec \u3092\u30d3\u30eb\u30c9\u3059\u308b\u969b\u306b\u5fc5\u8981\u306a xmlsec1-devel \u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3067\u306f\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u3002 \u305d\u3053\u3067\u3001\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u306a\u3044\u8ffd\u52a0\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u304c\u65b0\u3057\u3044 CodeReady Linux Builder \u3092\u6709\u52b9\u5316\u3057\u307e\u3059\u3002 CodeReady Linux Builder \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u306f xmlsec1-devel \u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002 https://access.redhat.com/ja/articles/5304171 sudo subscription-manager repos --enable codeready-builder-for-rhel-8-x86_64-rpms sudo yum install -y xmlsec1-devel","title":"\u4e8b\u524d\u6e96\u5099"},{"location":"powerdns/#yarn-nodejs14","text":"Wiki \u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u308b Nodejs \u306f 10 \u3067\u3059\u304c\u3001\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u53e4\u3044\u305f\u3081 14 \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 curl -sL https://rpm.nodesource.com/setup_14.x | sudo bash - sudo curl -sL https://dl.yarnpkg.com/rpm/yarn.repo -o /etc/yum.repos.d/yarn.repo sudo dnf install -y yarn","title":"yarn + Nodejs14"},{"location":"powerdns/#checkout","text":"sudo git clone https://github.com/ngoduykhanh/PowerDNS-Admin.git /opt/web/powerdns-admin sudo chmod -R 777 /opt/web/powerdns-admin cd /opt/web/powerdns-admin virtualenv -p python3 flask . ./flask/bin/activate ( flask ) pip install python-dotenv # requirements.txt \u304b\u3089 mysqlclient \u3092\u524a\u9664\u3057\u3066\u304a\u304f ( flask ) pip install --upgrade pip setuptools wheel ( flask ) pip install -r requirements.txt","title":"checkout"},{"location":"powerdns/#running","text":"( flask ) export FLASK_APP = powerdnsadmin/__init__.py ( flask ) flask db upgrade flask db upgrade \u3067\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u305f\u304c\u3001\u3053\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u3067\u30b3\u30f3\u30c6\u30ca\u3092\u4f7f\u3046\u65b9\u6cd5\u304c\u3042\u308b\u3068\u5224\u660e\u3057\u3001\u4e2d\u65ad\u3057\u307e\u3057\u305f\u3002","title":"Running"},{"location":"ansible/","text":"","title":"Index"},{"location":"ansible/tower/","text":"Ansible Tower \u30e9\u30a4\u30bb\u30f3\u30b9 \u8a55\u4fa1\u7248 (Evaluation) Ansible Tower \u304c\u542b\u307e\u308c\u3066\u3044\u308b Red Hat Ansible Automation Platform \u306b\u306f 60 \u65e5\u9593\u5229\u7528\u3067\u304d\u308b\u8a55\u4fa1\u30e9\u30a4\u30bb\u30f3\u30b9\u304c\u3042\u308a\u307e\u3059\u3002 https://www.redhat.com/ja/technologies/management/ansible/try-it Developer Program Red Hat Developer Program \u306b\u767b\u9332\u3059\u308b\u3068\u5229\u7528\u3067\u304d\u308b\u958b\u767a\u7528\u30e9\u30a4\u30bb\u30f3\u30b9\u3067\u3059\u3002 https://developers.redhat.com/products/ansible/getting-started \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb OS \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5148\u306e OS \u306f RHEL 8.3 \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 Ansible Tower \u306e\u6b21\u671f\u30e1\u30b8\u30e3\u30fc\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u306f RHEL 7 \u3084 CentOS \u304c\u30b5\u30dd\u30fc\u30c8\u5bfe\u8c61\u5916\u3068\u306a\u308b\u305f\u3081\u3067\u3059\u3002 #/etc/os-release NAME = \"Red Hat Enterprise Linux\" VERSION = \"8.3 (Ootpa)\" ID = \"rhel\" ID_LIKE = \"fedora\" VERSION_ID = \"8.3\" PLATFORM_ID = \"platform:el8\" PRETTY_NAME = \"Red Hat Enterprise Linux 8.3 (Ootpa)\" ANSI_COLOR = \"0;31\" CPE_NAME = \"cpe:/o:redhat:enterprise_linux:8.3:GA\" HOME_URL = \"https://www.redhat.com/\" BUG_REPORT_URL = \"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT = \"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION = 8 .3 REDHAT_SUPPORT_PRODUCT = \"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION = \"8.3\" \u30a4\u30f3\u30d9\u30f3\u30c8\u30ea\u7de8\u96c6 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u524d\u306b inventory \u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3057\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u8a2d\u5b9a\u306f\u5fc5\u9808\u3067\u3059\u3002 admin_password pg_password \u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u5b9f\u884c ./setup.sh \u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 \u30b9\u30af\u30ea\u30d7\u30c8\u5185\u3067 ansible \u30b3\u30de\u30f3\u30c9\u7b49\u3082\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u307e\u3059\u3002 $ sudo ./setup.sh ( snip ) PLAY RECAP ****************************************************************************************************************** localhost : ok = 191 changed = 79 unreachable = 0 failed = 0 skipped = 86 rescued = 0 ignored = 2 The setup process completed successfully. Setup log saved to /var/log/tower/setup-2021-01-05-01:10:04.log. \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u5931\u6557\u3057\u305f\u5834\u5408 \u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5931\u6557\u3057\u305f\u5834\u5408\u3001en \u306e locale \u304c\u4e0d\u8db3\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 $ sudo ./setup.sh ( snip ) TASK [ restart postgresql when authentication settings changed ] ************************************************************* fatal: [ localhost ] : FAILED! = > { \"changed\" : false, \"msg\" : \"Unable to restart service postgresql: Job for postgresql.service failed because the control process exited with error code.\\nSee \\\"systemctl status postgresql.service\\\" and \\\"journalctl -xe\\\" for details.\\n\" } ( snip ) $ locale -a C C.utf8 POSIX ja_JP.eucjp ja_JP.utf8 \u3053\u306e\u3088\u3046\u306a\u5834\u5408\u306f\u3001 glibc-langpack-en \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ sudo yum install glibc-langpack-en \u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u767b\u9332 Ansible Tower \u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b\u3068\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u767b\u9332\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002 USERNAME \u3068 PASSWORD \u3092\u5165\u529b\u3057\u3066\u300cGET SUBSCRIPTIONS\u300d\u3092\u62bc\u4e0b\u3059\u308b\u3068\u3001\u30e9\u30a4\u30bb\u30f3\u30b9\u9078\u629e\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002","title":"Ansible Tower"},{"location":"ansible/tower/#ansible-tower","text":"","title":"Ansible Tower"},{"location":"ansible/tower/#_1","text":"","title":"\u30e9\u30a4\u30bb\u30f3\u30b9"},{"location":"ansible/tower/#evaluation","text":"Ansible Tower \u304c\u542b\u307e\u308c\u3066\u3044\u308b Red Hat Ansible Automation Platform \u306b\u306f 60 \u65e5\u9593\u5229\u7528\u3067\u304d\u308b\u8a55\u4fa1\u30e9\u30a4\u30bb\u30f3\u30b9\u304c\u3042\u308a\u307e\u3059\u3002 https://www.redhat.com/ja/technologies/management/ansible/try-it","title":"\u8a55\u4fa1\u7248 (Evaluation)"},{"location":"ansible/tower/#developer-program","text":"Red Hat Developer Program \u306b\u767b\u9332\u3059\u308b\u3068\u5229\u7528\u3067\u304d\u308b\u958b\u767a\u7528\u30e9\u30a4\u30bb\u30f3\u30b9\u3067\u3059\u3002 https://developers.redhat.com/products/ansible/getting-started","title":"Developer Program"},{"location":"ansible/tower/#_2","text":"","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"ansible/tower/#os","text":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5148\u306e OS \u306f RHEL 8.3 \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 Ansible Tower \u306e\u6b21\u671f\u30e1\u30b8\u30e3\u30fc\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u306f RHEL 7 \u3084 CentOS \u304c\u30b5\u30dd\u30fc\u30c8\u5bfe\u8c61\u5916\u3068\u306a\u308b\u305f\u3081\u3067\u3059\u3002 #/etc/os-release NAME = \"Red Hat Enterprise Linux\" VERSION = \"8.3 (Ootpa)\" ID = \"rhel\" ID_LIKE = \"fedora\" VERSION_ID = \"8.3\" PLATFORM_ID = \"platform:el8\" PRETTY_NAME = \"Red Hat Enterprise Linux 8.3 (Ootpa)\" ANSI_COLOR = \"0;31\" CPE_NAME = \"cpe:/o:redhat:enterprise_linux:8.3:GA\" HOME_URL = \"https://www.redhat.com/\" BUG_REPORT_URL = \"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT = \"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION = 8 .3 REDHAT_SUPPORT_PRODUCT = \"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION = \"8.3\"","title":"OS"},{"location":"ansible/tower/#_3","text":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u524d\u306b inventory \u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3057\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u8a2d\u5b9a\u306f\u5fc5\u9808\u3067\u3059\u3002 admin_password pg_password","title":"\u30a4\u30f3\u30d9\u30f3\u30c8\u30ea\u7de8\u96c6"},{"location":"ansible/tower/#_4","text":"./setup.sh \u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 \u30b9\u30af\u30ea\u30d7\u30c8\u5185\u3067 ansible \u30b3\u30de\u30f3\u30c9\u7b49\u3082\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u307e\u3059\u3002 $ sudo ./setup.sh ( snip ) PLAY RECAP ****************************************************************************************************************** localhost : ok = 191 changed = 79 unreachable = 0 failed = 0 skipped = 86 rescued = 0 ignored = 2 The setup process completed successfully. Setup log saved to /var/log/tower/setup-2021-01-05-01:10:04.log.","title":"\u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u5b9f\u884c"},{"location":"ansible/tower/#_5","text":"\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5931\u6557\u3057\u305f\u5834\u5408\u3001en \u306e locale \u304c\u4e0d\u8db3\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 $ sudo ./setup.sh ( snip ) TASK [ restart postgresql when authentication settings changed ] ************************************************************* fatal: [ localhost ] : FAILED! = > { \"changed\" : false, \"msg\" : \"Unable to restart service postgresql: Job for postgresql.service failed because the control process exited with error code.\\nSee \\\"systemctl status postgresql.service\\\" and \\\"journalctl -xe\\\" for details.\\n\" } ( snip ) $ locale -a C C.utf8 POSIX ja_JP.eucjp ja_JP.utf8 \u3053\u306e\u3088\u3046\u306a\u5834\u5408\u306f\u3001 glibc-langpack-en \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ sudo yum install glibc-langpack-en","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u5931\u6557\u3057\u305f\u5834\u5408"},{"location":"ansible/tower/#_6","text":"Ansible Tower \u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b\u3068\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u767b\u9332\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002 USERNAME \u3068 PASSWORD \u3092\u5165\u529b\u3057\u3066\u300cGET SUBSCRIPTIONS\u300d\u3092\u62bc\u4e0b\u3059\u308b\u3068\u3001\u30e9\u30a4\u30bb\u30f3\u30b9\u9078\u629e\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002","title":"\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u767b\u9332"},{"location":"kubernetes/argocd/","text":"ArgoCD https://argoproj.github.io/argo-cd/getting_started/ # ArgoCD kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml # CLI VERSION = $( curl --silent \"https://api.github.com/repos/argoproj/argo-cd/releases/latest\" | grep '\"tag_name\"' | sed -E 's/.*\"([^\"]+)\".*/\\1/' ) sudo curl -SL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/ $VERSION /argocd-linux-amd64 sudo chmod +x /usr/local/bin/argocd kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 argocd login 192 .168.2.243 Username: admin Password: 'admin' logged in successfully Context '192.168.2.243' updated argocd account update-password *** Enter current password: *** Enter new password: *** Confirm new password: Password updated Context '192.168.2.243' updated","title":"ArgoCD"},{"location":"kubernetes/argocd/#argocd","text":"https://argoproj.github.io/argo-cd/getting_started/ # ArgoCD kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml # CLI VERSION = $( curl --silent \"https://api.github.com/repos/argoproj/argo-cd/releases/latest\" | grep '\"tag_name\"' | sed -E 's/.*\"([^\"]+)\".*/\\1/' ) sudo curl -SL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/ $VERSION /argocd-linux-amd64 sudo chmod +x /usr/local/bin/argocd kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 argocd login 192 .168.2.243 Username: admin Password: 'admin' logged in successfully Context '192.168.2.243' updated argocd account update-password *** Enter current password: *** Enter new password: *** Confirm new password: Password updated Context '192.168.2.243' updated","title":"ArgoCD"},{"location":"kubernetes/container-runtime/","text":"\u30b3\u30f3\u30c6\u30ca\u30e9\u30f3\u30bf\u30a4\u30e0 containerd Kubernetes \u306e\u30b3\u30f3\u30c6\u30ca\u30e9\u30f3\u30bf\u30a4\u30e0\u3092 containerd \u306b\u5909\u66f4\u3059\u308b\u3002 kubectl drain <k8s-node> --ignore-daemonsets --delete-local-data # k8s-node yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo yum update -y && yum install -y containerd.io mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml systemctl restart containerd yum remove -y docker-ce cat <<EOF > /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS=\"--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock\" EOF systemctl restart kubelet kubectl uncordon <k8s-node>","title":"\u30b3\u30f3\u30c6\u30ca\u30e9\u30f3\u30bf\u30a4\u30e0"},{"location":"kubernetes/container-runtime/#_1","text":"","title":"\u30b3\u30f3\u30c6\u30ca\u30e9\u30f3\u30bf\u30a4\u30e0"},{"location":"kubernetes/container-runtime/#containerd","text":"Kubernetes \u306e\u30b3\u30f3\u30c6\u30ca\u30e9\u30f3\u30bf\u30a4\u30e0\u3092 containerd \u306b\u5909\u66f4\u3059\u308b\u3002 kubectl drain <k8s-node> --ignore-daemonsets --delete-local-data # k8s-node yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo yum update -y && yum install -y containerd.io mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml systemctl restart containerd yum remove -y docker-ce cat <<EOF > /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS=\"--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock\" EOF systemctl restart kubelet kubectl uncordon <k8s-node>","title":"containerd"},{"location":"kubernetes/istio/","text":"Istio curl -L https://istio.io/downloadIstio | sh - cd istio-1.7.0 export PATH = \" $PATH :/home/nnstt1/istio/istio-1.7.0/bin\" istioctl x precheck istioctl install --set profile = demo kubectl label namespace default istio-injection = enabled ## sample kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl exec \" $( kubectl get pod -l app = ratings -o jsonpath = '{.items[0].metadata.name}' ) \" -c ratings -- curl -s productpage:9080/productpage | grep -o \"<title>.*</title>\" kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml istioctl analyze","title":"Istio"},{"location":"kubernetes/istio/#istio","text":"curl -L https://istio.io/downloadIstio | sh - cd istio-1.7.0 export PATH = \" $PATH :/home/nnstt1/istio/istio-1.7.0/bin\" istioctl x precheck istioctl install --set profile = demo kubectl label namespace default istio-injection = enabled ## sample kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl exec \" $( kubectl get pod -l app = ratings -o jsonpath = '{.items[0].metadata.name}' ) \" -c ratings -- curl -s productpage:9080/productpage | grep -o \"<title>.*</title>\" kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml istioctl analyze","title":"Istio"},{"location":"kubernetes/krew/","text":"Krew Krew \u306f kubectl \u306e\u30d7\u30e9\u30b0\u30a4\u30f3\u30de\u30cd\u30fc\u30b8\u30e3\u30fc\u30c4\u30fc\u30eb\u3067\u3059\u3002 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f https://krew.sigs.k8s.io/docs/user-guide/setup/install/ \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 $ ( set -x ; cd \" $( mktemp -d ) \" && OS = \" $( uname | tr '[:upper:]' '[:lower:]' ) \" && ARCH = \" $( uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/' ) \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz\" && tar zxvf krew.tar.gz && KREW = ./krew- \" ${ OS } _ ${ ARCH } \" && \" $KREW \" install krew ) ++ mktemp -d + cd /tmp/tmp.lVsaffJkBo ++ uname ++ tr '[:upper:]' '[:lower:]' + OS = linux ++ uname -m ++ sed -e s/x86_64/amd64/ -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/' + ARCH = amd64 + curl -fsSLO https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz + tar zxvf krew.tar.gz ./LICENSE ./krew-darwin_amd64 ./krew-darwin_arm64 ./krew-linux_amd64 ./krew-linux_arm ./krew-linux_arm64 ./krew-windows_amd64.exe + KREW = ./krew-linux_amd64 + ./krew-linux_amd64 install krew Adding \"default\" plugin index from https://github.com/kubernetes-sigs/krew-index.git. Updated the local copy of plugin index. Installing plugin: krew Installed plugin: krew \\ | Use this plugin: | kubectl krew | Documentation: | https://krew.sigs.k8s.io/ | Caveats: | \\ | | krew is now installed! To start using kubectl plugins, you need to add | | krew \\' s installation directory to your PATH: | | | | * macOS/Linux: | | - Add the following to your ~/.bashrc or ~/.zshrc: | | export PATH = \" ${ KREW_ROOT :- $HOME /.krew } /bin: $PATH \" | | - Restart your shell. | | | | * Windows: Add %USERPROFILE% \\. krew \\b in to your PATH environment variable | | | | To list krew commands and to get help, run: | | $ kubectl krew | | For a full list of available plugins, run: | | $ kubectl krew search | | | | You can find documentation at | | https://krew.sigs.k8s.io/docs/user-guide/quickstart/. | / / $ export PATH = \" ${ KREW_ROOT :- $HOME /.krew } /bin: $PATH \"","title":"Krew"},{"location":"kubernetes/krew/#krew","text":"Krew \u306f kubectl \u306e\u30d7\u30e9\u30b0\u30a4\u30f3\u30de\u30cd\u30fc\u30b8\u30e3\u30fc\u30c4\u30fc\u30eb\u3067\u3059\u3002 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f https://krew.sigs.k8s.io/docs/user-guide/setup/install/ \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 $ ( set -x ; cd \" $( mktemp -d ) \" && OS = \" $( uname | tr '[:upper:]' '[:lower:]' ) \" && ARCH = \" $( uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/' ) \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz\" && tar zxvf krew.tar.gz && KREW = ./krew- \" ${ OS } _ ${ ARCH } \" && \" $KREW \" install krew ) ++ mktemp -d + cd /tmp/tmp.lVsaffJkBo ++ uname ++ tr '[:upper:]' '[:lower:]' + OS = linux ++ uname -m ++ sed -e s/x86_64/amd64/ -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/' + ARCH = amd64 + curl -fsSLO https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz + tar zxvf krew.tar.gz ./LICENSE ./krew-darwin_amd64 ./krew-darwin_arm64 ./krew-linux_amd64 ./krew-linux_arm ./krew-linux_arm64 ./krew-windows_amd64.exe + KREW = ./krew-linux_amd64 + ./krew-linux_amd64 install krew Adding \"default\" plugin index from https://github.com/kubernetes-sigs/krew-index.git. Updated the local copy of plugin index. Installing plugin: krew Installed plugin: krew \\ | Use this plugin: | kubectl krew | Documentation: | https://krew.sigs.k8s.io/ | Caveats: | \\ | | krew is now installed! To start using kubectl plugins, you need to add | | krew \\' s installation directory to your PATH: | | | | * macOS/Linux: | | - Add the following to your ~/.bashrc or ~/.zshrc: | | export PATH = \" ${ KREW_ROOT :- $HOME /.krew } /bin: $PATH \" | | - Restart your shell. | | | | * Windows: Add %USERPROFILE% \\. krew \\b in to your PATH environment variable | | | | To list krew commands and to get help, run: | | $ kubectl krew | | For a full list of available plugins, run: | | $ kubectl krew search | | | | You can find documentation at | | https://krew.sigs.k8s.io/docs/user-guide/quickstart/. | / / $ export PATH = \" ${ KREW_ROOT :- $HOME /.krew } /bin: $PATH \"","title":"Krew"},{"location":"kubernetes/maintenance/","text":"Maintenance worker \u30ce\u30fc\u30c9\u306e\u505c\u6b62 worker \u30ce\u30fc\u30c9\u3092\u505c\u6b62\u3059\u308b\u524d\u306b\u3001 kubectl drain <node name> \u3092\u4f7f\u7528\u3057\u3066\u5bfe\u8c61\u30ce\u30fc\u30c9\u3067\u52d5\u3044\u3066\u3044\u308b Pod \u3092\u4ed6\u306e\u30ce\u30fc\u30c9\u306b\u79fb\u52d5\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 # worker \u30ce\u30fc\u30c9\u306e\u78ba\u8a8d $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kubeadm-master1.nnstt1.work Ready master 58d v1.19.0 192 .168.2.20 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker1.nnstt1.work Ready <none> 58d v1.19.0 192 .168.2.22 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker2.nnstt1.work Ready <none> 58d v1.19.0 192 .168.2.23 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker3.nnstt1.work Ready <none> 57d v1.19.0 192 .168.2.24 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 # drain $ kubectl drain <node name> --delete-local-data --ignore-daemonsets --delete-local-data \u5bfe\u8c61\u30ce\u30fc\u30c9\u3067\u52d5\u3044\u3066\u3044\u308b Pod \u304c local storage \u3092\u4f7f\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3067 drain \u304c\u5931\u6557\u3059\u308b\u3002 cannot delete Pods with local storage (use --delete-local-data to override): arc/logsdb-0, (\u4ee5\u4e0b Pod \u540d) \u5f53\u8a72\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3064\u3051\u308b\u3053\u3068\u3067\u3001local storage \u306e\u30c7\u30fc\u30bf\u3082\u542b\u3081\u3066 Pod \u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308b\u3002 Attention local storage \u306b\u6c38\u7d9a\u30c7\u30fc\u30bf\u3092\u6b8b\u3059\u3053\u3068\u306f\u904b\u7528\u7684\u306b\u4e0d\u9069\u5207\u306a\u306e\u3067\u3001Storage Class \u3092\u4f7f\u7528\u3059\u308b\u3088\u3046\u306b\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4fee\u6b63\u3057\u305f\u307b\u3046\u304c\u3088\u3044 --ignore-daemonsets daemonsets \u30ea\u30bd\u30fc\u30b9\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u30a8\u30e9\u30fc\u3068\u306a\u308b\u3002 error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): arc/metricsdc-rmv74, (\u4ee5\u4e0b Pod \u540d) \u5f53\u8a72\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3064\u3051\u308b\u3053\u3068\u3067\u3001daemonsets \u306e Pod \u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308b\u3002 kubectl drain \u304c\u6210\u529f\u3059\u308c\u3070\u30ce\u30fc\u30c9\u3092\u518d\u8d77\u52d5\u3057\u3066\u3082\u5b89\u5168\u306a\u72b6\u614b\u3068\u306a\u308b\u3002 \u30ce\u30fc\u30c9\u518d\u8d77\u52d5\u5f8c\u306f Pod \u304c\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u306a\u3044\u72b6\u614b\u3068\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3001 kubectl uncordon <node name> \u3067\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3092\u6709\u52b9\u5316\u3059\u308b\u3002 $ kubectl uncordon kubeadm-worker1.nnstt1.work node/kubeadm-worker1.nnstt1.work uncordoned","title":"Maintenance"},{"location":"kubernetes/maintenance/#maintenance","text":"","title":"Maintenance"},{"location":"kubernetes/maintenance/#worker","text":"worker \u30ce\u30fc\u30c9\u3092\u505c\u6b62\u3059\u308b\u524d\u306b\u3001 kubectl drain <node name> \u3092\u4f7f\u7528\u3057\u3066\u5bfe\u8c61\u30ce\u30fc\u30c9\u3067\u52d5\u3044\u3066\u3044\u308b Pod \u3092\u4ed6\u306e\u30ce\u30fc\u30c9\u306b\u79fb\u52d5\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 # worker \u30ce\u30fc\u30c9\u306e\u78ba\u8a8d $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kubeadm-master1.nnstt1.work Ready master 58d v1.19.0 192 .168.2.20 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker1.nnstt1.work Ready <none> 58d v1.19.0 192 .168.2.22 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker2.nnstt1.work Ready <none> 58d v1.19.0 192 .168.2.23 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker3.nnstt1.work Ready <none> 57d v1.19.0 192 .168.2.24 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 # drain $ kubectl drain <node name> --delete-local-data --ignore-daemonsets --delete-local-data \u5bfe\u8c61\u30ce\u30fc\u30c9\u3067\u52d5\u3044\u3066\u3044\u308b Pod \u304c local storage \u3092\u4f7f\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3067 drain \u304c\u5931\u6557\u3059\u308b\u3002 cannot delete Pods with local storage (use --delete-local-data to override): arc/logsdb-0, (\u4ee5\u4e0b Pod \u540d) \u5f53\u8a72\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3064\u3051\u308b\u3053\u3068\u3067\u3001local storage \u306e\u30c7\u30fc\u30bf\u3082\u542b\u3081\u3066 Pod \u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308b\u3002 Attention local storage \u306b\u6c38\u7d9a\u30c7\u30fc\u30bf\u3092\u6b8b\u3059\u3053\u3068\u306f\u904b\u7528\u7684\u306b\u4e0d\u9069\u5207\u306a\u306e\u3067\u3001Storage Class \u3092\u4f7f\u7528\u3059\u308b\u3088\u3046\u306b\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4fee\u6b63\u3057\u305f\u307b\u3046\u304c\u3088\u3044 --ignore-daemonsets daemonsets \u30ea\u30bd\u30fc\u30b9\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u30a8\u30e9\u30fc\u3068\u306a\u308b\u3002 error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): arc/metricsdc-rmv74, (\u4ee5\u4e0b Pod \u540d) \u5f53\u8a72\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3064\u3051\u308b\u3053\u3068\u3067\u3001daemonsets \u306e Pod \u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308b\u3002 kubectl drain \u304c\u6210\u529f\u3059\u308c\u3070\u30ce\u30fc\u30c9\u3092\u518d\u8d77\u52d5\u3057\u3066\u3082\u5b89\u5168\u306a\u72b6\u614b\u3068\u306a\u308b\u3002 \u30ce\u30fc\u30c9\u518d\u8d77\u52d5\u5f8c\u306f Pod \u304c\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u306a\u3044\u72b6\u614b\u3068\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3001 kubectl uncordon <node name> \u3067\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3092\u6709\u52b9\u5316\u3059\u308b\u3002 $ kubectl uncordon kubeadm-worker1.nnstt1.work node/kubeadm-worker1.nnstt1.work uncordoned","title":"worker \u30ce\u30fc\u30c9\u306e\u505c\u6b62"},{"location":"kubernetes/metallb/","text":"MetalLB kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml kubectl create secret generic -n metallb-system memberlist --from-literal = secretkey = \" $( openssl rand -base64 128 ) \" kubectl apply -f layer2-config.yaml # layer2-config.yaml apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools: - name: default protocol: layer2 addresses: - 192.168.2.240/28 # MetalLB \u304c\u6255\u3044\u51fa\u3059\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a2\u30c9\u30ec\u30b9","title":"MetalLB"},{"location":"kubernetes/metallb/#metallb","text":"kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml kubectl create secret generic -n metallb-system memberlist --from-literal = secretkey = \" $( openssl rand -base64 128 ) \" kubectl apply -f layer2-config.yaml # layer2-config.yaml apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools: - name: default protocol: layer2 addresses: - 192.168.2.240/28 # MetalLB \u304c\u6255\u3044\u51fa\u3059\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a2\u30c9\u30ec\u30b9","title":"MetalLB"},{"location":"kubernetes/minio-operator/","text":"MinIO Operator S3 \u4e92\u63db\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3067\u3042\u308b MinIO \u3092 Kubernetes \u4e0a\u3067\u904b\u7528\u3059\u308b\u305f\u3081\u306e Minio Operator \u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002 \u524d\u63d0\u6761\u4ef6 \u30d0\u30fc\u30b8\u30e7\u30f3 Kubernetes \u30af\u30e9\u30b9\u30bf\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304c 1.17.0 \u4ee5\u4e0a\u3067\u3042\u308b\u3053\u3068 1.17.0 \u672a\u6e80\u3060\u3063\u305f\u3089 Kubernetes \u516c\u5f0f\u3092\u53c2\u7167\u3057\u3066\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3057\u307e\u3059\u3002 $ kubectl version Client Version: version.Info { Major: \"1\" , Minor: \"21\" , GitVersion: \"v1.21.0\" , GitCommit: \"cb303e613a121a29364f75cc67d3d580833a7479\" , GitTreeState: \"clean\" , BuildDate: \"2021-04-08T16:31:21Z\" , GoVersion: \"go1.16.1\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } Server Version: version.Info { Major: \"1\" , Minor: \"21\" , GitVersion: \"v1.21.0\" , GitCommit: \"cb303e613a121a29364f75cc67d3d580833a7479\" , GitTreeState: \"clean\" , BuildDate: \"2021-04-08T16:25:06Z\" , GoVersion: \"go1.16.1\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } namespace Kubernetes \u30af\u30e9\u30b9\u30bf\u306b MinIO \u30c6\u30ca\u30f3\u30c8\u7528\u306e namespace \u304c\u3042\u308b\u3053\u3068 \u30c6\u30ca\u30f3\u30c8\u6bce\u306b namespace \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 $ kubectl create namespace minio-tenant-1 namespace/minio-tenant-1 created StorageClass Kubernetes \u30af\u30e9\u30b9\u30bf\u306b volumeBindingMode: WaitForFirstConsumer \u306e StorageClass \u304c\u3042\u308b\u3053\u3068 MinIO Operator \u304c\u4f7f\u7528\u3059\u308b Storage Class \u3092\u7528\u610f\u3059\u308b\u65b9\u6cd5\u3068\u3057\u3066 Local Volume \u3068 Rook/Ceph \u306e 2 \u901a\u308a\u7d39\u4ecb\u3057\u307e\u3059\u3002 Local Volume Local Volume \u3092\u4f7f\u3046\u65b9\u6cd5\u306f \u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 \u306b\u3082\u8a18\u8f09\u304c\u3042\u308a\u307e\u3059\u3002 \u307e\u305a\u3001Kubernetes \u306e \u5404 Worker \u30ce\u30fc\u30c9\u306e\u30ed\u30fc\u30ab\u30eb\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u4f7f\u7528\u3059\u308b StorageClass \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : local-storage provisioner : kubernetes.io/no-provisioner volumeBindingMode : WaitForFirstConsumer Local Volume \u306f\u30c0\u30a4\u30ca\u30df\u30c3\u30af\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u304c\u3067\u304d\u306a\u3044\u305f\u3081\u3001\u500b\u5225\u306b PersistentVolume \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 apiVersion : v1 kind : PersistentVolume metadata : name : PV-NAME spec : capacity : storage : 4Gi volumeMode : Filesystem accessModes : - ReadWriteOnce persistentVolumeReclaimPolicy : Retain storageClass : local-storage local : path : /mnt/minio nodeAffinity : required : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/hostname operator : In values : - NODE-NAME metadata.name PersistentVolume \u306e\u540d\u524d\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 spec.capacity.storage PersistentVolume \u3067\u78ba\u4fdd\u3059\u308b\u30b9\u30c8\u30ec\u30fc\u30b8\u30b5\u30a4\u30ba\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002 spec.storage-class \u4f5c\u6210\u3057\u305f StorageClass \u306e\u540d\u524d\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002 spec.nodeAffinity \u30ed\u30fc\u30ab\u30eb\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u6301\u3064\u30ce\u30fc\u30c9\u306e\u540d\u524d\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002 \u3053\u3053\u3067\u6307\u5b9a\u3055\u308c\u305f\u30ce\u30fc\u30c9\u306e\u30b9\u30c8\u30ec\u30fc\u30b8\u304c\u6d88\u8cbb\u3055\u308c\u307e\u3059\u3002 spec.local.path PersistentVolume \u3092\u30ce\u30fc\u30c9\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u30d1\u30b9\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002 Rook/Ceph Rook/Ceph \u3092\u5c0e\u5165\u6e08\u307f\u306e\u74b0\u5883\u3067\u3042\u308c\u3070\u3001MinIO Operator \u7528\u306e CephBlockPool \u3068 StorageClass \u3092\u4f5c\u6210\u3057\u3066\u4f7f\u3046\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 apiVersion : ceph.rook.io/v1 kind : CephBlockPool metadata : name : minio-pool namespace : rook-ceph spec : failureDomain : host replicated : size : 3 requireSafeReplicaSize : true --- apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : minio-rook-ceph-block provisioner : rook-ceph.rbd.csi.ceph.com parameters : clusterID : rook-ceph pool : replicapool imageFormat : \"2\" imageFeatures : layering csi.storage.k8s.io/provisioner-secret-name : rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace : rook-ceph csi.storage.k8s.io/controller-expand-secret-name : rook-csi-rbd-provisioner csi.storage.k8s.io/controller-expand-secret-namespace : rook-ceph csi.storage.k8s.io/node-stage-secret-name : rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace : rook-ceph csi.storage.k8s.io/fstype : ext4 allowVolumeExpansion : true reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer Krew Krew \u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u3053\u3068 \u3053\u3061\u3089\u306e\u30da\u30fc\u30b8 \u3092\u53c2\u7167\u3057\u3066 Krew \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Krew \u3092\u4f7f\u3063\u3066 MinIO Operator \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u3044\u304d\u307e\u3059\u3002 $ kubectl krew update Updated the local copy of plugin index. $ kubectl krew install minio Updated the local copy of plugin index. Installing plugin: minio Installed plugin: minio \\ | Use this plugin: | kubectl minio | Documentation: | https://github.com/minio/operator/tree/master/kubectl-minio | Caveats: | \\ | | * For resources that are not in default namespace, currently you must | | specify -n/--namespace explicitly ( the current namespace setting is not | | yet used ) . | / / WARNING: You installed plugin \"minio\" from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u305f\u3089\u3001Operator \u3092\u521d\u671f\u5316\u3057\u307e\u3059\u3002 $ kubectl minio init namespace/minio-operator created serviceaccount/minio-operator created clusterrole.rbac.authorization.k8s.io/minio-operator-role created clusterrolebinding.rbac.authorization.k8s.io/minio-operator-binding created customresourcedefinition.apiextensions.k8s.io/tenants.minio.min.io created service/operator created deployment.apps/minio-operator created serviceaccount/console-sa created clusterrole.rbac.authorization.k8s.io/console-sa-role created clusterrolebinding.rbac.authorization.k8s.io/console-sa-binding created configmap/console-env created service/console created deployment.apps/console created ----------------- To open Operator UI, start a port forward using this command: kubectl minio proxy -n minio-operator ----------------- \u4e0a\u8a18\u51fa\u529b\u306e\u901a\u308a\u3001proxy \u7d4c\u7531\u3067 MinIO Operator \u306e\u7ba1\u7406\u753b\u9762\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002 $ kubectl minio proxy -n minio-operator Starting port forward of the Console UI. To connect open a browser and go to http://localhost:9090 Current JWT to login: eyJhbGciOiJSUzI1NiIsImtpZCI6Ii0zbFc4cm51LXNPTWliSUVOYXdTNmI3ejRWeWRuVEF0dU9aenpKY0xaNGMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtaW5pby1vcGVyYXRvciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjb25zb2xlLXNhLXRva2VuLWJmNDZyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNvbnNvbGUtc2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3ZDgwZDNjOS0wMzUyLTRjNDItOWQ5Yy0xM2I3ZDg2ZjA4OWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6bWluaW8tb3BlcmF0b3I6Y29uc29sZS1zYSJ9.F8XsI2buT71U4QUi7tfShaDYAOs0rcgjPmZO5HiJYyWYZ0ZYN5LDZbIRf4il23CGRjGVUWksKsfeUqRFD0iG2lP21WxvTgIbUov3DbZtItB3Sj1dO4xI5WH7PLjyYAyE5eYGRQwd3K0qSpaexh4cKsG8vj5pxzd8g-f6vwWjE1Co7Ax83PzP_ATmNnOWVQE-6lI287QijtH-PfdK3UEHM1d1WU67yvei0YGLTyMgC3t6F8xKwqay0B2OLEUGirDcq74xZA5qB_Ipuvys5K05nOCKbQiWujv0J_gyyaEAwszRpGc4Gy8Mrc-bhAYe7EGLh8_BoQiwJ2d4UVcBEuEmmw Forwarding from 0 .0.0.0:9090 -> 9090 \u30d6\u30e9\u30a6\u30b6\u3067 kubectl minio proxy \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u30b5\u30fc\u30d0\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3068\u30ed\u30b0\u30a4\u30f3\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u3001\u30b3\u30de\u30f3\u30c9\u5b9f\u884c\u6642\u306b\u51fa\u529b\u3055\u308c\u305f\u6587\u5b57\u5217\u3092\u5165\u529b\u3057\u3066\u30ed\u30b0\u30a4\u30f3\u3057\u307e\u3059\u3002 \u30c6\u30ca\u30f3\u30c8\u4f5c\u6210 MinIO Operator \u3092\u4f7f\u3063\u3066\u30c6\u30ca\u30f3\u30c8\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 $ kubectl minio tenant create minio-tenant-1 \\ --servers 1 \\ --volumes 4 \\ --capacity 4Gi \\ --namespace minio-tenant-1 \\ --storage-class minio-rook-ceph-block Tenant 'minio-tenant-1' created in 'minio-tenant-1' Namespace Username: admin Password: 4c467817-1786-418e-a346-370b3c7c50ae Note: Copy the credentials to a secure location. MinIO will not display these again. +-------------+------------------------+----------------+--------------+--------------+ | APPLICATION | SERVICE NAME | NAMESPACE | SERVICE TYPE | SERVICE PORT | +-------------+------------------------+----------------+--------------+--------------+ | MinIO | minio | minio-tenant-1 | ClusterIP | 443 | | Console | minio-tenant-1-console | minio-tenant-1 | ClusterIP | 9443 | +-------------+------------------------+----------------+--------------+--------------+ --volumes \u304c 4 \u672a\u6e80\u3060\u3063\u305f\u5834\u5408\u3001\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u3002 $ kubectl minio tenant create minio-tenant-1 \\ --servers 1 \\ --volumes 3 \\ --capacity 3Gi \\ --namespace minio-tenant-1 \\ --storage-class minio-rook-ceph-block Error: pool #0 setup must have a minimum of 4 volumes per server \u30c8\u30e9\u30d6\u30eb \u30c6\u30ca\u30f3\u30c8\u4f5c\u6210\u30b3\u30de\u30f3\u30c9\u5b9f\u884c\u5f8c\u3001MinIO Operator \u306b\u3088\u3063\u3066 Pod \u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u304c\u3001 minio-tenant-1-tls Secret \u3092\u30de\u30a6\u30f3\u30c8\u3067\u304d\u305a\u306b Pod \u304c\u8d77\u52d5\u3057\u3066\u304d\u307e\u305b\u3093\u3067\u3057\u305f\u3002 $ kubectl describe pods minio-tenant-1-ss-0-0 -n minio-tenant-1 ( snip ) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m17s default-scheduler Successfully assigned minio-tenant-1/minio-tenant-1-ss-0-0 to kubeadm-worker3.nnstt1.work Normal SuccessfulAttachVolume 2m17s attachdetach-controller AttachVolume.Attach succeeded for volume \"pvc-894acc42-ca13-4a25-87eb-d396acab50cc\" Normal SuccessfulAttachVolume 2m17s attachdetach-controller AttachVolume.Attach succeeded for volume \"pvc-1af6963b-0658-49f1-a4a0-18488ad5d6e7\" Normal SuccessfulAttachVolume 2m17s attachdetach-controller AttachVolume.Attach succeeded for volume \"pvc-6875cba8-5c7b-4bfe-95bb-ad491c2940ab\" Normal SuccessfulAttachVolume 2m17s attachdetach-controller AttachVolume.Attach succeeded for volume \"pvc-153554c7-b5e8-4e4a-95dd-28f625af163f\" Warning FailedMount 14s kubelet Unable to attach or mount volumes: unmounted volumes =[ minio-tenant-1-tls ] , unattached volumes =[ 3 minio-tenant-1-tls kube-api-access-7s52p 0 1 2 ] : timed out waiting for the condition Warning FailedMount 8s ( x9 over 2m16s ) kubelet MountVolume.SetUp failed for volume \"minio-tenant-1-tls\" : secret \"minio-tenant-1-tls\" not found 2021/5/13 Issue \u3092\u53c2\u8003\u306b\u30c7\u30d0\u30c3\u30b0\u7528\u30b3\u30f3\u30c6\u30ca\u3067\u8a3c\u660e\u66f8\u30b3\u30d4\u30fc\u3057\u3066\u307f\u307e\u3057\u305f\u3002 https://github.com/minio/operator/issues/459#issuecomment-774319087 $ kubectl run my-shell -i --tty --image miniodev/debugger -- bash root@my-shell:/ \\# cp /var/run/secrets/kubernetes.io/serviceaccount/ca.crt /etc/ssl/certs/ root@my-shell:/ \\# mc config host add myminio https://minio.minio-tenant-1.svc.cluster.local admin b4e71593-29b4-4f93-8673-cf43009f74bf mc: <ERROR> Unable to initialize new alias from the provided credentials. Get \"https://minio.minio-tenant-1.svc.cluster.local/probe-bucket-sign-ezqrq20bdldg/?location=\" : dial tcp 10 .97.131.201:443: connect: connection refused. \u30d1\u30b9\u30ef\u30fc\u30c9\u304c\u3042\u3063\u3066\u3044\u306a\u304b\u3063\u305f\u3088\u3046\u306a\u306e\u3067\u3001\u30c6\u30ca\u30f3\u30c8\u518d\u4f5c\u6210\u3057\u305f\u3068\u3053\u308d\u3001\u65b0\u3057\u3044\u30c6\u30ca\u30f3\u30c8\u3067\u306f minio-tenant-1-tls Sercret \u304c\u4f5c\u6210\u3055\u308c\u3066 Pod \u3082\u6b63\u5e38\u306b\u8d77\u52d5\u3057\u307e\u3057\u305f\u3002 2021/5/14 Operator \u3092\u524a\u9664\u3057\u3066\u304b\u3089\u3084\u308a\u76f4\u3057\u305f\u3068\u3053\u308d\u3001 kubectl minio tenant create \u3092\u5b9f\u884c\u3057\u3066 4min \u307b\u3069\u653e\u7f6e\u3057\u305f\u3089 minio-tenant-1-tls \u3082\u4f5c\u6210\u3055\u308c\u307e\u3057\u305f\u3002 $ kubectl get secret NAME TYPE DATA AGE default-token-ntlzm kubernetes.io/service-account-token 3 2m24s minio-tenant-1-console-secret Opaque 4 2m22s minio-tenant-1-console-tls Opaque 2 62s minio-tenant-1-creds-secret Opaque 2 2m22s minio-tenant-1-tls Opaque 2 2m2s operator-tls Opaque 1 2m17s operator-webhook-secret Opaque 3 2m17s $ kubectl get pods NAME READY STATUS RESTARTS AGE minio-tenant-1-console-6b7488946f-2mvkf 1 /1 Running 0 2m43s minio-tenant-1-console-6b7488946f-6djht 1 /1 Running 0 2m43s minio-tenant-1-ss-0-0 1 /1 Running 0 3m43s \u518d\u5ea6\u8a66\u3057\u305f\u3068\u3053\u308d\u3001\u4eca\u5ea6\u306f\u4f5c\u3089\u308c\u307e\u305b\u3093\u3067\u3057\u305f\u3002 CSR \u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u8a8d\u3057\u305f\u3068\u3053\u308d\u3001\u524d\u56de\u4f5c\u3089\u308c\u305f\u3068\u601d\u308f\u308c\u308b\u3082\u306e\u304c\u6b8b\u3063\u3066\u3044\u307e\u3057\u305f\u3002 $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION minio-tenant-1-console-minio-tenant-1-csr 44m kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued minio-tenant-1-minio-tenant-1-csr 45m kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued operator-minio-operator-csr 46m kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued CSR \u30ea\u30bd\u30fc\u30b9\u3092\u524a\u9664\u3057\u3066\u518d\u3005\u5ea6\u30c1\u30e3\u30ec\u30f3\u30b8\u3001 kubectl minio init \u5f8c\u306e CSR \u30ea\u30bd\u30fc\u30b9\u3002 $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION operator-minio-operator-csr 96s kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued kubectl minio tenant create \u5f8c\u3002 $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION minio-tenant-1-console-minio-tenant-1-csr 17s kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued minio-tenant-1-minio-tenant-1-csr 77s kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued operator-minio-operator-csr 4m5s kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued \u65b0\u3057\u304f CSR \u30ea\u30bd\u30fc\u30b9\u304c\u4f5c\u3089\u308c\u3001 minio-tenant-1-tls \u3082\u4f5c\u6210\u3055\u308c\u307e\u3057\u305f\u3002 \u5916\u90e8\u516c\u958b\u7528\u30b5\u30fc\u30d3\u30b9 Kubernetes \u30af\u30e9\u30b9\u30bf\u5916\u306e\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304b\u3089 MinIO \u30c6\u30ca\u30f3\u30c8\u306b\u63a5\u7d9a\u3059\u308b\u305f\u3081\u306b\u306f Ingress \u3084 Load Balancer \u304c\u5fc5\u8981\u3067\u3059\u3002 \u4eca\u56de\u306f MinIO \u7528\u306e Load Balancer \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 MinIO apiVersion : v1 kind : Service metadata : namespace : minio-tenant-1 name : minio-loadbalancer labels : component : minio-tenant-1 annotations : external-dns.alpha.kubernetes.io/hostname : minio-tenant-1.nnstt1.work. # ExternalDNS \u7528\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3 spec : type : LoadBalancer ports : - port : 9000 targetPort : 9000 protocol : TCP selector : v1.min.io/tenant : minio-tenant-1 Console apiVersion : v1 kind : Service metadata : namespace : minio-tenant-1 name : console-loadbalancer annotations : external-dns.alpha.kubernetes.io/hostname : minio-console.nnstt1.work. # ExternalDNS \u7528\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3 spec : type : LoadBalancer ports : - name : https-console port : 9443 protocol : TCP targetPort : 9443 selector : v1.min.io/console : minio-tenant-1-console \u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u30c6\u30ca\u30f3\u30c8\u524a\u9664 MinIO Operator \u3067\u4f5c\u6210\u3057\u305f\u30c6\u30ca\u30f3\u30c8\u3092\u524a\u9664\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 tenant \u30ea\u30bd\u30fc\u30b9\u306f namespace \u6bce\u306b\u4f5c\u6210\u3055\u308c\u308b\u305f\u3081\u3001 -n \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u5bfe\u8c61\u306e namespace \u3092\u6307\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002 # \u30c6\u30ca\u30f3\u30c8\u524a\u9664 $ kubectl minio tenant delete minio-tenant-1 -n minio-tenant-1 This will delete the Tenant minio-tenant-1 and ALL its data. Do you want to proceed?: y Deleting MinIO Tenant minio-tenant-1 Deleting MinIO Tenant Credentials Secret minio-tenant-1-creds-secret Deleting MinIO Tenant Console Secret minio-tenant-1-console-secret # namespace \u78ba\u8a8d $ kubectl get pods -n minio-tenant-1 No resources found in minio-tenant-1 namespace. $ kubectl get secret -n minio-tenant-1 NAME TYPE DATA AGE default-token-swx2m kubernetes.io/service-account-token 3 2d Operator \u524a\u9664 Kubernetes \u30af\u30e9\u30b9\u30bf\u304b\u3089 MinIO Operator \u3092\u524a\u9664\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 $ kubectl minio delete Are you sure you want to delete ALL the MinIO Tenants and MinIO Operator?: y namespace \"minio-operator\" deleted serviceaccount \"minio-operator\" deleted clusterrole.rbac.authorization.k8s.io \"minio-operator-role\" deleted clusterrolebinding.rbac.authorization.k8s.io \"minio-operator-binding\" deleted customresourcedefinition.apiextensions.k8s.io \"tenants.minio.min.io\" deleted service \"operator\" deleted deployment.apps \"minio-operator\" deleted serviceaccount \"console-sa\" deleted clusterrole.rbac.authorization.k8s.io \"console-sa-role\" deleted clusterrolebinding.rbac.authorization.k8s.io \"console-sa-binding\" deleted configmap \"console-env\" deleted service \"console\" deleted deployment.apps \"console\" deleted","title":"MinIO Operator"},{"location":"kubernetes/minio-operator/#minio-operator","text":"S3 \u4e92\u63db\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3067\u3042\u308b MinIO \u3092 Kubernetes \u4e0a\u3067\u904b\u7528\u3059\u308b\u305f\u3081\u306e Minio Operator \u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002","title":"MinIO Operator"},{"location":"kubernetes/minio-operator/#_1","text":"","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"kubernetes/minio-operator/#_2","text":"Kubernetes \u30af\u30e9\u30b9\u30bf\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304c 1.17.0 \u4ee5\u4e0a\u3067\u3042\u308b\u3053\u3068 1.17.0 \u672a\u6e80\u3060\u3063\u305f\u3089 Kubernetes \u516c\u5f0f\u3092\u53c2\u7167\u3057\u3066\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3057\u307e\u3059\u3002 $ kubectl version Client Version: version.Info { Major: \"1\" , Minor: \"21\" , GitVersion: \"v1.21.0\" , GitCommit: \"cb303e613a121a29364f75cc67d3d580833a7479\" , GitTreeState: \"clean\" , BuildDate: \"2021-04-08T16:31:21Z\" , GoVersion: \"go1.16.1\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } Server Version: version.Info { Major: \"1\" , Minor: \"21\" , GitVersion: \"v1.21.0\" , GitCommit: \"cb303e613a121a29364f75cc67d3d580833a7479\" , GitTreeState: \"clean\" , BuildDate: \"2021-04-08T16:25:06Z\" , GoVersion: \"go1.16.1\" , Compiler: \"gc\" , Platform: \"linux/amd64\" }","title":"\u30d0\u30fc\u30b8\u30e7\u30f3"},{"location":"kubernetes/minio-operator/#namespace","text":"Kubernetes \u30af\u30e9\u30b9\u30bf\u306b MinIO \u30c6\u30ca\u30f3\u30c8\u7528\u306e namespace \u304c\u3042\u308b\u3053\u3068 \u30c6\u30ca\u30f3\u30c8\u6bce\u306b namespace \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 $ kubectl create namespace minio-tenant-1 namespace/minio-tenant-1 created","title":"namespace"},{"location":"kubernetes/minio-operator/#storageclass","text":"Kubernetes \u30af\u30e9\u30b9\u30bf\u306b volumeBindingMode: WaitForFirstConsumer \u306e StorageClass \u304c\u3042\u308b\u3053\u3068 MinIO Operator \u304c\u4f7f\u7528\u3059\u308b Storage Class \u3092\u7528\u610f\u3059\u308b\u65b9\u6cd5\u3068\u3057\u3066 Local Volume \u3068 Rook/Ceph \u306e 2 \u901a\u308a\u7d39\u4ecb\u3057\u307e\u3059\u3002 Local Volume Local Volume \u3092\u4f7f\u3046\u65b9\u6cd5\u306f \u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 \u306b\u3082\u8a18\u8f09\u304c\u3042\u308a\u307e\u3059\u3002 \u307e\u305a\u3001Kubernetes \u306e \u5404 Worker \u30ce\u30fc\u30c9\u306e\u30ed\u30fc\u30ab\u30eb\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u4f7f\u7528\u3059\u308b StorageClass \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : local-storage provisioner : kubernetes.io/no-provisioner volumeBindingMode : WaitForFirstConsumer Local Volume \u306f\u30c0\u30a4\u30ca\u30df\u30c3\u30af\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u304c\u3067\u304d\u306a\u3044\u305f\u3081\u3001\u500b\u5225\u306b PersistentVolume \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 apiVersion : v1 kind : PersistentVolume metadata : name : PV-NAME spec : capacity : storage : 4Gi volumeMode : Filesystem accessModes : - ReadWriteOnce persistentVolumeReclaimPolicy : Retain storageClass : local-storage local : path : /mnt/minio nodeAffinity : required : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/hostname operator : In values : - NODE-NAME metadata.name PersistentVolume \u306e\u540d\u524d\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 spec.capacity.storage PersistentVolume \u3067\u78ba\u4fdd\u3059\u308b\u30b9\u30c8\u30ec\u30fc\u30b8\u30b5\u30a4\u30ba\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002 spec.storage-class \u4f5c\u6210\u3057\u305f StorageClass \u306e\u540d\u524d\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002 spec.nodeAffinity \u30ed\u30fc\u30ab\u30eb\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u6301\u3064\u30ce\u30fc\u30c9\u306e\u540d\u524d\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002 \u3053\u3053\u3067\u6307\u5b9a\u3055\u308c\u305f\u30ce\u30fc\u30c9\u306e\u30b9\u30c8\u30ec\u30fc\u30b8\u304c\u6d88\u8cbb\u3055\u308c\u307e\u3059\u3002 spec.local.path PersistentVolume \u3092\u30ce\u30fc\u30c9\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u30d1\u30b9\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002 Rook/Ceph Rook/Ceph \u3092\u5c0e\u5165\u6e08\u307f\u306e\u74b0\u5883\u3067\u3042\u308c\u3070\u3001MinIO Operator \u7528\u306e CephBlockPool \u3068 StorageClass \u3092\u4f5c\u6210\u3057\u3066\u4f7f\u3046\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 apiVersion : ceph.rook.io/v1 kind : CephBlockPool metadata : name : minio-pool namespace : rook-ceph spec : failureDomain : host replicated : size : 3 requireSafeReplicaSize : true --- apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : minio-rook-ceph-block provisioner : rook-ceph.rbd.csi.ceph.com parameters : clusterID : rook-ceph pool : replicapool imageFormat : \"2\" imageFeatures : layering csi.storage.k8s.io/provisioner-secret-name : rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace : rook-ceph csi.storage.k8s.io/controller-expand-secret-name : rook-csi-rbd-provisioner csi.storage.k8s.io/controller-expand-secret-namespace : rook-ceph csi.storage.k8s.io/node-stage-secret-name : rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace : rook-ceph csi.storage.k8s.io/fstype : ext4 allowVolumeExpansion : true reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer","title":"StorageClass"},{"location":"kubernetes/minio-operator/#krew","text":"Krew \u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u3053\u3068 \u3053\u3061\u3089\u306e\u30da\u30fc\u30b8 \u3092\u53c2\u7167\u3057\u3066 Krew \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002","title":"Krew"},{"location":"kubernetes/minio-operator/#_3","text":"Krew \u3092\u4f7f\u3063\u3066 MinIO Operator \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u3044\u304d\u307e\u3059\u3002 $ kubectl krew update Updated the local copy of plugin index. $ kubectl krew install minio Updated the local copy of plugin index. Installing plugin: minio Installed plugin: minio \\ | Use this plugin: | kubectl minio | Documentation: | https://github.com/minio/operator/tree/master/kubectl-minio | Caveats: | \\ | | * For resources that are not in default namespace, currently you must | | specify -n/--namespace explicitly ( the current namespace setting is not | | yet used ) . | / / WARNING: You installed plugin \"minio\" from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u305f\u3089\u3001Operator \u3092\u521d\u671f\u5316\u3057\u307e\u3059\u3002 $ kubectl minio init namespace/minio-operator created serviceaccount/minio-operator created clusterrole.rbac.authorization.k8s.io/minio-operator-role created clusterrolebinding.rbac.authorization.k8s.io/minio-operator-binding created customresourcedefinition.apiextensions.k8s.io/tenants.minio.min.io created service/operator created deployment.apps/minio-operator created serviceaccount/console-sa created clusterrole.rbac.authorization.k8s.io/console-sa-role created clusterrolebinding.rbac.authorization.k8s.io/console-sa-binding created configmap/console-env created service/console created deployment.apps/console created ----------------- To open Operator UI, start a port forward using this command: kubectl minio proxy -n minio-operator ----------------- \u4e0a\u8a18\u51fa\u529b\u306e\u901a\u308a\u3001proxy \u7d4c\u7531\u3067 MinIO Operator \u306e\u7ba1\u7406\u753b\u9762\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002 $ kubectl minio proxy -n minio-operator Starting port forward of the Console UI. To connect open a browser and go to http://localhost:9090 Current JWT to login: eyJhbGciOiJSUzI1NiIsImtpZCI6Ii0zbFc4cm51LXNPTWliSUVOYXdTNmI3ejRWeWRuVEF0dU9aenpKY0xaNGMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtaW5pby1vcGVyYXRvciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjb25zb2xlLXNhLXRva2VuLWJmNDZyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNvbnNvbGUtc2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3ZDgwZDNjOS0wMzUyLTRjNDItOWQ5Yy0xM2I3ZDg2ZjA4OWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6bWluaW8tb3BlcmF0b3I6Y29uc29sZS1zYSJ9.F8XsI2buT71U4QUi7tfShaDYAOs0rcgjPmZO5HiJYyWYZ0ZYN5LDZbIRf4il23CGRjGVUWksKsfeUqRFD0iG2lP21WxvTgIbUov3DbZtItB3Sj1dO4xI5WH7PLjyYAyE5eYGRQwd3K0qSpaexh4cKsG8vj5pxzd8g-f6vwWjE1Co7Ax83PzP_ATmNnOWVQE-6lI287QijtH-PfdK3UEHM1d1WU67yvei0YGLTyMgC3t6F8xKwqay0B2OLEUGirDcq74xZA5qB_Ipuvys5K05nOCKbQiWujv0J_gyyaEAwszRpGc4Gy8Mrc-bhAYe7EGLh8_BoQiwJ2d4UVcBEuEmmw Forwarding from 0 .0.0.0:9090 -> 9090 \u30d6\u30e9\u30a6\u30b6\u3067 kubectl minio proxy \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u30b5\u30fc\u30d0\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3068\u30ed\u30b0\u30a4\u30f3\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u3001\u30b3\u30de\u30f3\u30c9\u5b9f\u884c\u6642\u306b\u51fa\u529b\u3055\u308c\u305f\u6587\u5b57\u5217\u3092\u5165\u529b\u3057\u3066\u30ed\u30b0\u30a4\u30f3\u3057\u307e\u3059\u3002","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"kubernetes/minio-operator/#_4","text":"MinIO Operator \u3092\u4f7f\u3063\u3066\u30c6\u30ca\u30f3\u30c8\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 $ kubectl minio tenant create minio-tenant-1 \\ --servers 1 \\ --volumes 4 \\ --capacity 4Gi \\ --namespace minio-tenant-1 \\ --storage-class minio-rook-ceph-block Tenant 'minio-tenant-1' created in 'minio-tenant-1' Namespace Username: admin Password: 4c467817-1786-418e-a346-370b3c7c50ae Note: Copy the credentials to a secure location. MinIO will not display these again. +-------------+------------------------+----------------+--------------+--------------+ | APPLICATION | SERVICE NAME | NAMESPACE | SERVICE TYPE | SERVICE PORT | +-------------+------------------------+----------------+--------------+--------------+ | MinIO | minio | minio-tenant-1 | ClusterIP | 443 | | Console | minio-tenant-1-console | minio-tenant-1 | ClusterIP | 9443 | +-------------+------------------------+----------------+--------------+--------------+ --volumes \u304c 4 \u672a\u6e80\u3060\u3063\u305f\u5834\u5408\u3001\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u3002 $ kubectl minio tenant create minio-tenant-1 \\ --servers 1 \\ --volumes 3 \\ --capacity 3Gi \\ --namespace minio-tenant-1 \\ --storage-class minio-rook-ceph-block Error: pool #0 setup must have a minimum of 4 volumes per server","title":"\u30c6\u30ca\u30f3\u30c8\u4f5c\u6210"},{"location":"kubernetes/minio-operator/#_5","text":"\u30c6\u30ca\u30f3\u30c8\u4f5c\u6210\u30b3\u30de\u30f3\u30c9\u5b9f\u884c\u5f8c\u3001MinIO Operator \u306b\u3088\u3063\u3066 Pod \u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u304c\u3001 minio-tenant-1-tls Secret \u3092\u30de\u30a6\u30f3\u30c8\u3067\u304d\u305a\u306b Pod \u304c\u8d77\u52d5\u3057\u3066\u304d\u307e\u305b\u3093\u3067\u3057\u305f\u3002 $ kubectl describe pods minio-tenant-1-ss-0-0 -n minio-tenant-1 ( snip ) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m17s default-scheduler Successfully assigned minio-tenant-1/minio-tenant-1-ss-0-0 to kubeadm-worker3.nnstt1.work Normal SuccessfulAttachVolume 2m17s attachdetach-controller AttachVolume.Attach succeeded for volume \"pvc-894acc42-ca13-4a25-87eb-d396acab50cc\" Normal SuccessfulAttachVolume 2m17s attachdetach-controller AttachVolume.Attach succeeded for volume \"pvc-1af6963b-0658-49f1-a4a0-18488ad5d6e7\" Normal SuccessfulAttachVolume 2m17s attachdetach-controller AttachVolume.Attach succeeded for volume \"pvc-6875cba8-5c7b-4bfe-95bb-ad491c2940ab\" Normal SuccessfulAttachVolume 2m17s attachdetach-controller AttachVolume.Attach succeeded for volume \"pvc-153554c7-b5e8-4e4a-95dd-28f625af163f\" Warning FailedMount 14s kubelet Unable to attach or mount volumes: unmounted volumes =[ minio-tenant-1-tls ] , unattached volumes =[ 3 minio-tenant-1-tls kube-api-access-7s52p 0 1 2 ] : timed out waiting for the condition Warning FailedMount 8s ( x9 over 2m16s ) kubelet MountVolume.SetUp failed for volume \"minio-tenant-1-tls\" : secret \"minio-tenant-1-tls\" not found","title":"\u30c8\u30e9\u30d6\u30eb"},{"location":"kubernetes/minio-operator/#2021513","text":"Issue \u3092\u53c2\u8003\u306b\u30c7\u30d0\u30c3\u30b0\u7528\u30b3\u30f3\u30c6\u30ca\u3067\u8a3c\u660e\u66f8\u30b3\u30d4\u30fc\u3057\u3066\u307f\u307e\u3057\u305f\u3002 https://github.com/minio/operator/issues/459#issuecomment-774319087 $ kubectl run my-shell -i --tty --image miniodev/debugger -- bash root@my-shell:/ \\# cp /var/run/secrets/kubernetes.io/serviceaccount/ca.crt /etc/ssl/certs/ root@my-shell:/ \\# mc config host add myminio https://minio.minio-tenant-1.svc.cluster.local admin b4e71593-29b4-4f93-8673-cf43009f74bf mc: <ERROR> Unable to initialize new alias from the provided credentials. Get \"https://minio.minio-tenant-1.svc.cluster.local/probe-bucket-sign-ezqrq20bdldg/?location=\" : dial tcp 10 .97.131.201:443: connect: connection refused. \u30d1\u30b9\u30ef\u30fc\u30c9\u304c\u3042\u3063\u3066\u3044\u306a\u304b\u3063\u305f\u3088\u3046\u306a\u306e\u3067\u3001\u30c6\u30ca\u30f3\u30c8\u518d\u4f5c\u6210\u3057\u305f\u3068\u3053\u308d\u3001\u65b0\u3057\u3044\u30c6\u30ca\u30f3\u30c8\u3067\u306f minio-tenant-1-tls Sercret \u304c\u4f5c\u6210\u3055\u308c\u3066 Pod \u3082\u6b63\u5e38\u306b\u8d77\u52d5\u3057\u307e\u3057\u305f\u3002","title":"2021/5/13"},{"location":"kubernetes/minio-operator/#2021514","text":"Operator \u3092\u524a\u9664\u3057\u3066\u304b\u3089\u3084\u308a\u76f4\u3057\u305f\u3068\u3053\u308d\u3001 kubectl minio tenant create \u3092\u5b9f\u884c\u3057\u3066 4min \u307b\u3069\u653e\u7f6e\u3057\u305f\u3089 minio-tenant-1-tls \u3082\u4f5c\u6210\u3055\u308c\u307e\u3057\u305f\u3002 $ kubectl get secret NAME TYPE DATA AGE default-token-ntlzm kubernetes.io/service-account-token 3 2m24s minio-tenant-1-console-secret Opaque 4 2m22s minio-tenant-1-console-tls Opaque 2 62s minio-tenant-1-creds-secret Opaque 2 2m22s minio-tenant-1-tls Opaque 2 2m2s operator-tls Opaque 1 2m17s operator-webhook-secret Opaque 3 2m17s $ kubectl get pods NAME READY STATUS RESTARTS AGE minio-tenant-1-console-6b7488946f-2mvkf 1 /1 Running 0 2m43s minio-tenant-1-console-6b7488946f-6djht 1 /1 Running 0 2m43s minio-tenant-1-ss-0-0 1 /1 Running 0 3m43s \u518d\u5ea6\u8a66\u3057\u305f\u3068\u3053\u308d\u3001\u4eca\u5ea6\u306f\u4f5c\u3089\u308c\u307e\u305b\u3093\u3067\u3057\u305f\u3002 CSR \u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u8a8d\u3057\u305f\u3068\u3053\u308d\u3001\u524d\u56de\u4f5c\u3089\u308c\u305f\u3068\u601d\u308f\u308c\u308b\u3082\u306e\u304c\u6b8b\u3063\u3066\u3044\u307e\u3057\u305f\u3002 $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION minio-tenant-1-console-minio-tenant-1-csr 44m kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued minio-tenant-1-minio-tenant-1-csr 45m kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued operator-minio-operator-csr 46m kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued CSR \u30ea\u30bd\u30fc\u30b9\u3092\u524a\u9664\u3057\u3066\u518d\u3005\u5ea6\u30c1\u30e3\u30ec\u30f3\u30b8\u3001 kubectl minio init \u5f8c\u306e CSR \u30ea\u30bd\u30fc\u30b9\u3002 $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION operator-minio-operator-csr 96s kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued kubectl minio tenant create \u5f8c\u3002 $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION minio-tenant-1-console-minio-tenant-1-csr 17s kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued minio-tenant-1-minio-tenant-1-csr 77s kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued operator-minio-operator-csr 4m5s kubernetes.io/legacy-unknown system:serviceaccount:minio-operator:minio-operator Approved,Issued \u65b0\u3057\u304f CSR \u30ea\u30bd\u30fc\u30b9\u304c\u4f5c\u3089\u308c\u3001 minio-tenant-1-tls \u3082\u4f5c\u6210\u3055\u308c\u307e\u3057\u305f\u3002","title":"2021/5/14"},{"location":"kubernetes/minio-operator/#_6","text":"Kubernetes \u30af\u30e9\u30b9\u30bf\u5916\u306e\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304b\u3089 MinIO \u30c6\u30ca\u30f3\u30c8\u306b\u63a5\u7d9a\u3059\u308b\u305f\u3081\u306b\u306f Ingress \u3084 Load Balancer \u304c\u5fc5\u8981\u3067\u3059\u3002 \u4eca\u56de\u306f MinIO \u7528\u306e Load Balancer \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002","title":"\u5916\u90e8\u516c\u958b\u7528\u30b5\u30fc\u30d3\u30b9"},{"location":"kubernetes/minio-operator/#minio","text":"apiVersion : v1 kind : Service metadata : namespace : minio-tenant-1 name : minio-loadbalancer labels : component : minio-tenant-1 annotations : external-dns.alpha.kubernetes.io/hostname : minio-tenant-1.nnstt1.work. # ExternalDNS \u7528\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3 spec : type : LoadBalancer ports : - port : 9000 targetPort : 9000 protocol : TCP selector : v1.min.io/tenant : minio-tenant-1","title":"MinIO"},{"location":"kubernetes/minio-operator/#console","text":"apiVersion : v1 kind : Service metadata : namespace : minio-tenant-1 name : console-loadbalancer annotations : external-dns.alpha.kubernetes.io/hostname : minio-console.nnstt1.work. # ExternalDNS \u7528\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3 spec : type : LoadBalancer ports : - name : https-console port : 9443 protocol : TCP targetPort : 9443 selector : v1.min.io/console : minio-tenant-1-console","title":"Console"},{"location":"kubernetes/minio-operator/#_7","text":"","title":"\u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"kubernetes/minio-operator/#_8","text":"MinIO Operator \u3067\u4f5c\u6210\u3057\u305f\u30c6\u30ca\u30f3\u30c8\u3092\u524a\u9664\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 tenant \u30ea\u30bd\u30fc\u30b9\u306f namespace \u6bce\u306b\u4f5c\u6210\u3055\u308c\u308b\u305f\u3081\u3001 -n \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u5bfe\u8c61\u306e namespace \u3092\u6307\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002 # \u30c6\u30ca\u30f3\u30c8\u524a\u9664 $ kubectl minio tenant delete minio-tenant-1 -n minio-tenant-1 This will delete the Tenant minio-tenant-1 and ALL its data. Do you want to proceed?: y Deleting MinIO Tenant minio-tenant-1 Deleting MinIO Tenant Credentials Secret minio-tenant-1-creds-secret Deleting MinIO Tenant Console Secret minio-tenant-1-console-secret # namespace \u78ba\u8a8d $ kubectl get pods -n minio-tenant-1 No resources found in minio-tenant-1 namespace. $ kubectl get secret -n minio-tenant-1 NAME TYPE DATA AGE default-token-swx2m kubernetes.io/service-account-token 3 2d","title":"\u30c6\u30ca\u30f3\u30c8\u524a\u9664"},{"location":"kubernetes/minio-operator/#operator","text":"Kubernetes \u30af\u30e9\u30b9\u30bf\u304b\u3089 MinIO Operator \u3092\u524a\u9664\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 $ kubectl minio delete Are you sure you want to delete ALL the MinIO Tenants and MinIO Operator?: y namespace \"minio-operator\" deleted serviceaccount \"minio-operator\" deleted clusterrole.rbac.authorization.k8s.io \"minio-operator-role\" deleted clusterrolebinding.rbac.authorization.k8s.io \"minio-operator-binding\" deleted customresourcedefinition.apiextensions.k8s.io \"tenants.minio.min.io\" deleted service \"operator\" deleted deployment.apps \"minio-operator\" deleted serviceaccount \"console-sa\" deleted clusterrole.rbac.authorization.k8s.io \"console-sa-role\" deleted clusterrolebinding.rbac.authorization.k8s.io \"console-sa-binding\" deleted configmap \"console-env\" deleted service \"console\" deleted deployment.apps \"console\" deleted","title":"Operator \u524a\u9664"},{"location":"kubernetes/rook/","text":"Rook Rook \u306f Kubernetes \u306b\u5404\u7a2e\u30b9\u30c8\u30ec\u30fc\u30b8\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3 (Ceph, Cassandra, etc...) \u3092\u63d0\u4f9b\u3057\u3066\u304f\u308c\u308b\u30b9\u30c8\u30ec\u30fc\u30b8\u30aa\u30fc\u30b1\u30b9\u30c8\u30ec\u30fc\u30bf\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001Rook \u3092\u4f7f\u3063\u3066 Ceph \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u3001K8s \u30af\u30e9\u30b9\u30bf\u306b\u30d6\u30ed\u30c3\u30af\u30b9\u30c8\u30ec\u30fc\u30b8\u3068\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u306e StorageClass \u3092\u63d0\u4f9b\u3059\u308b\u624b\u9806\u3092\u8aac\u660e\u3057\u307e\u3059\u3002 Note \u672c\u624b\u9806\u306f v1.5 \u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u3044\u307e\u3059\u3002 Install Ceph Storage Quickstart \u6700\u65b0\u7248\u306e Rook \u3092\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089 clone \u3057\u3066\u3001Rook \u3092 K8s \u30af\u30e9\u30b9\u30bf\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ git clone --single-branch --branch v1.5.7 https://github.com/rook/rook.git $ cd rook/cluster/examples/kubernetes/ceph $ kubectl apply -f crds.yaml $ kubectl apply -f common.yaml $ kubectl apply -f operator.yaml $ kubectl apply -f cluster.yaml Rook \u306b\u3088\u308b OSD \u306e\u30c7\u30d7\u30ed\u30a4\u65b9\u6cd5\u306f2\u901a\u308a\u3042\u308a\u307e\u3059\u3002 Host-based OSDs/MONs PVC-based OSDs/MONs (OSD on PVC) \u30c7\u30d7\u30ed\u30a4\u65b9\u6cd5\u306e\u9055\u3044\u306b\u3064\u3044\u3066\u306f\u3001\u3053\u3061\u3089\u306e\u30b9\u30e9\u30a4\u30c9\u304c\u53c2\u8003\u306b\u306a\u308a\u307e\u3059\u3002 Rook v1.1\u306a\u3089\u51fa\u6765\u308bPVC-based\u306a\u5206\u6563\u30b9\u30c8\u30ec\u30fc\u30b8 Rook/Ceph OSD on PVC in practice Block Storage RBD \u7528\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 \u3053\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306b\u306f StorageClass \u30ea\u30bd\u30fc\u30b9\u306e\u4ed6\u306b\u3001 CephBlockPool \u3068\u3044\u3046\u30ab\u30b9\u30bf\u30e0\u30ea\u30bd\u30fc\u30b9\u304c\u4f7f\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002 $ kubectl apply -f csi/rbd/storageclass.yaml # ceph-tools \u30b3\u30f3\u30c6\u30ca\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3066 Ceph \u306e\u69cb\u7bc9\u72b6\u6cc1\u3092\u78ba\u8a8d $ kubectl apply -f toolbox.yaml $ kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) -- bash $ ceph status \u69cb\u7bc9\u5b8c\u4e86\u5f8c\u306b\u306f\u4ee5\u4e0b\u306e StorageClass \u304c\u5229\u7528\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002 $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate true 57d Dashboard $ kubectl apply -f rook/cluster/examples/kubernetes/ceph/dashboard-loadbalancer.yaml # \u30d1\u30b9\u30ef\u30fc\u30c9\u78ba\u8a8d $ kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath = \"{['data']['password']}\" | base64 --decode && echo Cleanup Rook \u3092\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7\u3059\u308b\u5834\u5408\u306e\u624b\u9806\u3067\u3059\u3002 \u5404 Worker Node \u3067\u5b9f\u65bd\u3057\u307e\u3059\u3002 Rook \u95a2\u9023\u30c7\u30fc\u30bf\u524a\u9664 $ rm -rf /var/lib/rook \u30d6\u30ed\u30c3\u30af\u30c7\u30d0\u30a4\u30b9\u521d\u671f\u5316 \u521d\u671f\u5316\u3059\u308b\u30d6\u30ed\u30c3\u30af\u30c7\u30d0\u30a4\u30b9\u6bce\u306b\u4e0b\u8a18\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 #!/usr/bin/env bash # https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md $ DISK = \"/dev/sdb\" # Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean) # You will have to run this step for all disks. $ sgdisk --zap-all $DISK # Clean hdds with dd $ dd if = /dev/zero of = \" $DISK \" bs = 1M count = 100 oflag = direct,dsync # Clean disks such as ssd with blkdiscard instead of dd $ blkdiscard $DISK # These steps only have to be run once on each node # If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks. $ ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % # ceph-volume setup can leave ceph-<UUID> directories in /dev (unnecessary clutter) $ rm -rf /dev/ceph-* Kubernetes v1.20 \u3067\u52d5\u304b\u306a\u3044\u5834\u5408 Kubernetes v1.20 \u304b\u3089 Feature Gate CSIVolumeFSGroupPolicy \u304c Beta \u306b\u6607\u683c\u3057\u3066\u304a\u308a\u3001\u6a5f\u80fd\u304c\u6709\u52b9\u72b6\u614b\u3068\u306a\u308a\u307e\u3057\u305f\u3002 \u305d\u306e\u5f71\u97ff\u3067 Rook \u304c\u52d5\u4f5c\u3057\u306a\u3044\u5834\u5408\u304c\u3042\u308a\u3001\u66ab\u5b9a\u5bfe\u51e6\u3068\u3057\u3066\u65e7\u30d0\u30fc\u30b8\u30e7\u30f3\u3068\u540c\u3058\u72b6\u614b\u306b\u3059\u308b --feature-gates=CSIVolumeFSGroupPolicy=false \u3092 kubelet \u306b\u8a2d\u5b9a\u3057\u307e\u3057\u305f\u3002 # /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS = \"--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --feature-gates=CSIVolumeFSGroupPolicy=false\" Extenal Cluster $ kubectl apply -f crds.yaml $ kubectl apply -f common.yaml $ kubectl apply -f operator.yaml $ kubectl apply -f common-external.yaml \u74b0\u5883\u5909\u6570 NAMESPACE ROOK_EXTERNAL_FSID : Ceph \u30af\u30e9\u30b9\u30bf\u3067 ceph fsid \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066 fsid \u3092\u78ba\u8a8d\u3057\u3066\u8a2d\u5b9a\u3057\u307e\u3059\u3002 ROOK_EXTERNAL_CEPH_MON_DATA : MON \u306e\u60c5\u5831\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 ROOK_EXTERNAL_ADMIN_SECRET : Ceph \u30af\u30e9\u30b9\u30bf\u3067 ceph auth get-key client.admin \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3001Ceph \u30af\u30e9\u30b9\u30bf\u306e admin \u30b7\u30fc\u30af\u30ec\u30c3\u30c8\u30ad\u30fc\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002 Ceph \u30ce\u30fc\u30c9\u3067 External Cluster \u74b0\u5883\u5909\u6570\u3068\u3057\u3066\u4f7f\u3046\u60c5\u5831\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002 $ bash create-external-cluster-resources.sh export ROOK_EXTERNAL_USER_SECRET = AQBMvFBgH++mJBAAJKv/9LAWvc4I09WfycMLUg == export ROOK_EXTERNAL_USERNAME = client.healthchecker export CSI_RBD_NODE_SECRET_SECRET = AQB6aU1gcIRuHRAAM20ehPo8QwTrPUy2Eea4DA == export CSI_RBD_PROVISIONER_SECRET = AQB5aU1gSl1dOxAAV81b2Puiih2VBajLlHq7Cw == export CSI_CEPHFS_NODE_SECRET = AQB7aU1gcfuQHBAAqliipwmEdNo6jdFNJaI5oA == export CSI_CEPHFS_PROVISIONER_SECRET = AQB6aU1gW3RzOhAAp39Bq0a0sMBnyWd7Bs9bbg == successfully created users and keys, execute the above commands and run import-external-cluster.sh to inject them in your Kubernetes cluster. $ export NAMESPACE = rook-ceph-external $ export ROOK_EXTERNAL_FSID = c70278a6-8275-11eb-ab03-dca6329a21a7 $ export ROOK_EXTERNAL_CEPH_MON_DATA = a = 192 .168.1.21:6789,b = 192 .168.1.22:6789,c = 192 .168.1.23:6789 $ export ROOK_EXTERNAL_ADMIN_SECRET = AQCdKEpglwQYNhAAd0rszoHkeCD+JJrz2YsSzA == $ bash import-external-cluster.sh CephCluster \u30c7\u30d7\u30ed\u30a4 https://github.com/rook/rook/issues/5732#issuecomment-756042524 $ kubectl apply -f cluster-external.yaml Remove an OSD https://rook.io/docs/rook/v1.5/ceph-osd-mgmt.html#remove-an-osd Host-based \u30af\u30e9\u30b9\u30bf\u304b\u3089 OSD \u3092\u9664\u5916\u3057\u307e\u3059\u3002 osd-purge.yaml \u5185\u306e <OSD-IDs> \u306b\u9664\u5916\u3059\u308b OSD \u306e ID \u3092\u6307\u5b9a\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 kubectl scale deploy rook-ceph-operator --replicas = 0 kubectl apply -f osd-purge.yml kubectl scale deploy rook-ceph-operator --replicas = 1","title":"Rook"},{"location":"kubernetes/rook/#rook","text":"Rook \u306f Kubernetes \u306b\u5404\u7a2e\u30b9\u30c8\u30ec\u30fc\u30b8\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3 (Ceph, Cassandra, etc...) \u3092\u63d0\u4f9b\u3057\u3066\u304f\u308c\u308b\u30b9\u30c8\u30ec\u30fc\u30b8\u30aa\u30fc\u30b1\u30b9\u30c8\u30ec\u30fc\u30bf\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001Rook \u3092\u4f7f\u3063\u3066 Ceph \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u3001K8s \u30af\u30e9\u30b9\u30bf\u306b\u30d6\u30ed\u30c3\u30af\u30b9\u30c8\u30ec\u30fc\u30b8\u3068\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u306e StorageClass \u3092\u63d0\u4f9b\u3059\u308b\u624b\u9806\u3092\u8aac\u660e\u3057\u307e\u3059\u3002 Note \u672c\u624b\u9806\u306f v1.5 \u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u3044\u307e\u3059\u3002","title":"Rook"},{"location":"kubernetes/rook/#install","text":"Ceph Storage Quickstart \u6700\u65b0\u7248\u306e Rook \u3092\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089 clone \u3057\u3066\u3001Rook \u3092 K8s \u30af\u30e9\u30b9\u30bf\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ git clone --single-branch --branch v1.5.7 https://github.com/rook/rook.git $ cd rook/cluster/examples/kubernetes/ceph $ kubectl apply -f crds.yaml $ kubectl apply -f common.yaml $ kubectl apply -f operator.yaml $ kubectl apply -f cluster.yaml Rook \u306b\u3088\u308b OSD \u306e\u30c7\u30d7\u30ed\u30a4\u65b9\u6cd5\u306f2\u901a\u308a\u3042\u308a\u307e\u3059\u3002 Host-based OSDs/MONs PVC-based OSDs/MONs (OSD on PVC) \u30c7\u30d7\u30ed\u30a4\u65b9\u6cd5\u306e\u9055\u3044\u306b\u3064\u3044\u3066\u306f\u3001\u3053\u3061\u3089\u306e\u30b9\u30e9\u30a4\u30c9\u304c\u53c2\u8003\u306b\u306a\u308a\u307e\u3059\u3002 Rook v1.1\u306a\u3089\u51fa\u6765\u308bPVC-based\u306a\u5206\u6563\u30b9\u30c8\u30ec\u30fc\u30b8 Rook/Ceph OSD on PVC in practice","title":"Install"},{"location":"kubernetes/rook/#block-storage","text":"RBD \u7528\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 \u3053\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306b\u306f StorageClass \u30ea\u30bd\u30fc\u30b9\u306e\u4ed6\u306b\u3001 CephBlockPool \u3068\u3044\u3046\u30ab\u30b9\u30bf\u30e0\u30ea\u30bd\u30fc\u30b9\u304c\u4f7f\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002 $ kubectl apply -f csi/rbd/storageclass.yaml # ceph-tools \u30b3\u30f3\u30c6\u30ca\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3066 Ceph \u306e\u69cb\u7bc9\u72b6\u6cc1\u3092\u78ba\u8a8d $ kubectl apply -f toolbox.yaml $ kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) -- bash $ ceph status \u69cb\u7bc9\u5b8c\u4e86\u5f8c\u306b\u306f\u4ee5\u4e0b\u306e StorageClass \u304c\u5229\u7528\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002 $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate true 57d","title":"Block Storage"},{"location":"kubernetes/rook/#dashboard","text":"$ kubectl apply -f rook/cluster/examples/kubernetes/ceph/dashboard-loadbalancer.yaml # \u30d1\u30b9\u30ef\u30fc\u30c9\u78ba\u8a8d $ kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath = \"{['data']['password']}\" | base64 --decode && echo","title":"Dashboard"},{"location":"kubernetes/rook/#cleanup","text":"Rook \u3092\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7\u3059\u308b\u5834\u5408\u306e\u624b\u9806\u3067\u3059\u3002 \u5404 Worker Node \u3067\u5b9f\u65bd\u3057\u307e\u3059\u3002","title":"Cleanup"},{"location":"kubernetes/rook/#rook_1","text":"$ rm -rf /var/lib/rook","title":"Rook \u95a2\u9023\u30c7\u30fc\u30bf\u524a\u9664"},{"location":"kubernetes/rook/#_1","text":"\u521d\u671f\u5316\u3059\u308b\u30d6\u30ed\u30c3\u30af\u30c7\u30d0\u30a4\u30b9\u6bce\u306b\u4e0b\u8a18\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 #!/usr/bin/env bash # https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md $ DISK = \"/dev/sdb\" # Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean) # You will have to run this step for all disks. $ sgdisk --zap-all $DISK # Clean hdds with dd $ dd if = /dev/zero of = \" $DISK \" bs = 1M count = 100 oflag = direct,dsync # Clean disks such as ssd with blkdiscard instead of dd $ blkdiscard $DISK # These steps only have to be run once on each node # If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks. $ ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % # ceph-volume setup can leave ceph-<UUID> directories in /dev (unnecessary clutter) $ rm -rf /dev/ceph-*","title":"\u30d6\u30ed\u30c3\u30af\u30c7\u30d0\u30a4\u30b9\u521d\u671f\u5316"},{"location":"kubernetes/rook/#kubernetes-v120","text":"Kubernetes v1.20 \u304b\u3089 Feature Gate CSIVolumeFSGroupPolicy \u304c Beta \u306b\u6607\u683c\u3057\u3066\u304a\u308a\u3001\u6a5f\u80fd\u304c\u6709\u52b9\u72b6\u614b\u3068\u306a\u308a\u307e\u3057\u305f\u3002 \u305d\u306e\u5f71\u97ff\u3067 Rook \u304c\u52d5\u4f5c\u3057\u306a\u3044\u5834\u5408\u304c\u3042\u308a\u3001\u66ab\u5b9a\u5bfe\u51e6\u3068\u3057\u3066\u65e7\u30d0\u30fc\u30b8\u30e7\u30f3\u3068\u540c\u3058\u72b6\u614b\u306b\u3059\u308b --feature-gates=CSIVolumeFSGroupPolicy=false \u3092 kubelet \u306b\u8a2d\u5b9a\u3057\u307e\u3057\u305f\u3002 # /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS = \"--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --feature-gates=CSIVolumeFSGroupPolicy=false\"","title":"Kubernetes v1.20 \u3067\u52d5\u304b\u306a\u3044\u5834\u5408"},{"location":"kubernetes/rook/#extenal-cluster","text":"$ kubectl apply -f crds.yaml $ kubectl apply -f common.yaml $ kubectl apply -f operator.yaml $ kubectl apply -f common-external.yaml","title":"Extenal Cluster"},{"location":"kubernetes/rook/#_2","text":"NAMESPACE ROOK_EXTERNAL_FSID : Ceph \u30af\u30e9\u30b9\u30bf\u3067 ceph fsid \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066 fsid \u3092\u78ba\u8a8d\u3057\u3066\u8a2d\u5b9a\u3057\u307e\u3059\u3002 ROOK_EXTERNAL_CEPH_MON_DATA : MON \u306e\u60c5\u5831\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 ROOK_EXTERNAL_ADMIN_SECRET : Ceph \u30af\u30e9\u30b9\u30bf\u3067 ceph auth get-key client.admin \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3001Ceph \u30af\u30e9\u30b9\u30bf\u306e admin \u30b7\u30fc\u30af\u30ec\u30c3\u30c8\u30ad\u30fc\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002 Ceph \u30ce\u30fc\u30c9\u3067 External Cluster \u74b0\u5883\u5909\u6570\u3068\u3057\u3066\u4f7f\u3046\u60c5\u5831\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002 $ bash create-external-cluster-resources.sh export ROOK_EXTERNAL_USER_SECRET = AQBMvFBgH++mJBAAJKv/9LAWvc4I09WfycMLUg == export ROOK_EXTERNAL_USERNAME = client.healthchecker export CSI_RBD_NODE_SECRET_SECRET = AQB6aU1gcIRuHRAAM20ehPo8QwTrPUy2Eea4DA == export CSI_RBD_PROVISIONER_SECRET = AQB5aU1gSl1dOxAAV81b2Puiih2VBajLlHq7Cw == export CSI_CEPHFS_NODE_SECRET = AQB7aU1gcfuQHBAAqliipwmEdNo6jdFNJaI5oA == export CSI_CEPHFS_PROVISIONER_SECRET = AQB6aU1gW3RzOhAAp39Bq0a0sMBnyWd7Bs9bbg == successfully created users and keys, execute the above commands and run import-external-cluster.sh to inject them in your Kubernetes cluster. $ export NAMESPACE = rook-ceph-external $ export ROOK_EXTERNAL_FSID = c70278a6-8275-11eb-ab03-dca6329a21a7 $ export ROOK_EXTERNAL_CEPH_MON_DATA = a = 192 .168.1.21:6789,b = 192 .168.1.22:6789,c = 192 .168.1.23:6789 $ export ROOK_EXTERNAL_ADMIN_SECRET = AQCdKEpglwQYNhAAd0rszoHkeCD+JJrz2YsSzA == $ bash import-external-cluster.sh","title":"\u74b0\u5883\u5909\u6570"},{"location":"kubernetes/rook/#cephcluster","text":"https://github.com/rook/rook/issues/5732#issuecomment-756042524 $ kubectl apply -f cluster-external.yaml","title":"CephCluster \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/rook/#remove-an-osd","text":"https://rook.io/docs/rook/v1.5/ceph-osd-mgmt.html#remove-an-osd Host-based \u30af\u30e9\u30b9\u30bf\u304b\u3089 OSD \u3092\u9664\u5916\u3057\u307e\u3059\u3002 osd-purge.yaml \u5185\u306e <OSD-IDs> \u306b\u9664\u5916\u3059\u308b OSD \u306e ID \u3092\u6307\u5b9a\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 kubectl scale deploy rook-ceph-operator --replicas = 0 kubectl apply -f osd-purge.yml kubectl scale deploy rook-ceph-operator --replicas = 1","title":"Remove an OSD"},{"location":"kubernetes/velero/","text":"Velero Velero \u306f\u3001Kubernetes \u306e\u30ea\u30bd\u30fc\u30b9\u3084 PV (Persistent Volume) \u306e\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30fb\u30ea\u30b9\u30c8\u30a2\u3092\u3059\u308b\u305f\u3081\u306e OSS \u3067\u3059\u3002 Velero \u306f\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u306b\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3057\u307e\u3059\u3002 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u7528\u610f\u3059\u308b\u65b9\u6cd5\u306f\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3059\u3002 MinIO \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092 MinIO \u3067\u7528\u610f\u3057\u307e\u3059\u3002(\u53c2\u8003: Quick start evaluation install with Minio ) Velero \u30c7\u30d7\u30ed\u30a4 velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.0.0 \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-volume-snapshots = false \\ --backup-location-config region = minio,s3ForcePathStyle = \"true\" ,s3Url = http://minio.minio.svc:9000 Velero & Rook Ceph Velero \u304c\u4f7f\u7528\u3059\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092 Rook Ceph \u3092\u4f7f\u3063\u3066\u7528\u610f\u3057\u307e\u3059\u3002 Rook Ceph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8 Rook \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\u3067\u3042\u308c\u3070\u3001\u30b5\u30f3\u30d7\u30eb\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u3060\u3051\u3002 kubectl apply -f rook/cluster/examples/kubernetes/ceph/object-test.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml export AWS_HOST = $( kubectl get cm ceph-delete-bucket -o jsonpath = '{.data.BUCKET_HOST}' ) export AWS_ACCESS_KEY_ID = $( kubectl get secret ceph-delete-bucket -o jsonpath = '{.data.AWS_ACCESS_KEY_ID}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl get secret ceph-delete-bucket -o jsonpath = '{.data.AWS_SECRET_ACCESS_KEY}' | base64 --decode ) rook-ceph-tools Pod \u3092\u4f7f\u3063\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u304c\u4f7f\u3048\u308b\u304b\u78ba\u8a8d\u3092\u3057\u307e\u3059\u3002 kubectl exec -it $( kubectl get pods -l app = rook-ceph-tools -o = jsonpath ={ .items [ * ] .metadata.name } ) -- bash export AWS_HOST = <host> export AWS_ENDPOINT = <endpoint> export AWS_ACCESS_KEY_ID = <accessKey> export AWS_SECRET_ACCESS_KEY = <secretKey> echo \"Hello Rook\" > /tmp/rookObj # Upload # <bucket-name> \u306f ObjectBucketClame \u304b\u3089\u53c2\u7167\u53ef\u80fd s3cmd put /tmp/rookObj --no-ssl --host = ${ AWS_HOST } --host-bucket = s3://<bucket-name> # Download s3cmd get s3://<bucket-name>/rookObj /tmp/rookObj-download --no-ssl --host = ${ AWS_HOST } --host-bucket = Velero \u30c7\u30d7\u30ed\u30a4 velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.0.0 \\ --bucket ceph-bkt-c71df44b-8657-4f02-b680-fe0894debc07 \\ --secret-file ./credentials-velero \\ --use-volume-snapshots = false \\ --backup-location-config region = us-east-1,s3ForcePathStyle = \"true\" ,s3Url = http://rook-ceph-rgw-my-store.rook-ceph.svc \u691c\u8a3c # \u30c6\u30b9\u30c8\u7528\u30ea\u30bd\u30fc\u30b9\u306e\u6e96\u5099 kubectl apply -f nginx-base.yaml kubectl get deployments -l component = velero --namespace = velero kubectl get deployments --namespace = nginx-example # \u30d0\u30c3\u30af\u30a2\u30c3\u30d7 velero backup create nginx-backup2 --selector app = nginx # \u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30ea\u30bd\u30fc\u30b9\u306e\u78ba\u8a8d velero backup get velero backup describe nginx-backup velero backup logs nginx-backup # \u30c6\u30b9\u30c8\u7528\u30ea\u30bd\u30fc\u30b9\u306e\u524a\u9664 kubectl delete namespace nginx-example kubectl get deployments --namespace = nginx-example # \u30ea\u30b9\u30c8\u30a2 velero restore create --from-backup nginx-backup2 # \u30ea\u30b9\u30c8\u30a2\u30ea\u30bd\u30fc\u30b9\u306e\u78ba\u8a8d velero restore get kubectl get deployments --namespace = nginx-example \u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u30af\u30e9\u30b9\u30bf\u304b\u3089 Velero \u3092\u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u624b\u9806\u3067\u3059\u3002 Uninstalling Velero kubectl delete namespace/velero clusterrolebinding/velero kubectl delete crds -l component = velero","title":"Velero"},{"location":"kubernetes/velero/#velero","text":"Velero \u306f\u3001Kubernetes \u306e\u30ea\u30bd\u30fc\u30b9\u3084 PV (Persistent Volume) \u306e\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30fb\u30ea\u30b9\u30c8\u30a2\u3092\u3059\u308b\u305f\u3081\u306e OSS \u3067\u3059\u3002 Velero \u306f\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u306b\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3057\u307e\u3059\u3002 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u7528\u610f\u3059\u308b\u65b9\u6cd5\u306f\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3059\u3002","title":"Velero"},{"location":"kubernetes/velero/#minio","text":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092 MinIO \u3067\u7528\u610f\u3057\u307e\u3059\u3002(\u53c2\u8003: Quick start evaluation install with Minio )","title":"MinIO"},{"location":"kubernetes/velero/#velero_1","text":"velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.0.0 \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-volume-snapshots = false \\ --backup-location-config region = minio,s3ForcePathStyle = \"true\" ,s3Url = http://minio.minio.svc:9000","title":"Velero \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/velero/#velero-rook-ceph","text":"Velero \u304c\u4f7f\u7528\u3059\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092 Rook Ceph \u3092\u4f7f\u3063\u3066\u7528\u610f\u3057\u307e\u3059\u3002","title":"Velero &amp; Rook Ceph"},{"location":"kubernetes/velero/#rook-ceph","text":"Rook \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\u3067\u3042\u308c\u3070\u3001\u30b5\u30f3\u30d7\u30eb\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u3060\u3051\u3002 kubectl apply -f rook/cluster/examples/kubernetes/ceph/object-test.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml export AWS_HOST = $( kubectl get cm ceph-delete-bucket -o jsonpath = '{.data.BUCKET_HOST}' ) export AWS_ACCESS_KEY_ID = $( kubectl get secret ceph-delete-bucket -o jsonpath = '{.data.AWS_ACCESS_KEY_ID}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl get secret ceph-delete-bucket -o jsonpath = '{.data.AWS_SECRET_ACCESS_KEY}' | base64 --decode ) rook-ceph-tools Pod \u3092\u4f7f\u3063\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u304c\u4f7f\u3048\u308b\u304b\u78ba\u8a8d\u3092\u3057\u307e\u3059\u3002 kubectl exec -it $( kubectl get pods -l app = rook-ceph-tools -o = jsonpath ={ .items [ * ] .metadata.name } ) -- bash export AWS_HOST = <host> export AWS_ENDPOINT = <endpoint> export AWS_ACCESS_KEY_ID = <accessKey> export AWS_SECRET_ACCESS_KEY = <secretKey> echo \"Hello Rook\" > /tmp/rookObj # Upload # <bucket-name> \u306f ObjectBucketClame \u304b\u3089\u53c2\u7167\u53ef\u80fd s3cmd put /tmp/rookObj --no-ssl --host = ${ AWS_HOST } --host-bucket = s3://<bucket-name> # Download s3cmd get s3://<bucket-name>/rookObj /tmp/rookObj-download --no-ssl --host = ${ AWS_HOST } --host-bucket =","title":"Rook Ceph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8"},{"location":"kubernetes/velero/#velero_2","text":"velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.0.0 \\ --bucket ceph-bkt-c71df44b-8657-4f02-b680-fe0894debc07 \\ --secret-file ./credentials-velero \\ --use-volume-snapshots = false \\ --backup-location-config region = us-east-1,s3ForcePathStyle = \"true\" ,s3Url = http://rook-ceph-rgw-my-store.rook-ceph.svc","title":"Velero \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/velero/#_1","text":"# \u30c6\u30b9\u30c8\u7528\u30ea\u30bd\u30fc\u30b9\u306e\u6e96\u5099 kubectl apply -f nginx-base.yaml kubectl get deployments -l component = velero --namespace = velero kubectl get deployments --namespace = nginx-example # \u30d0\u30c3\u30af\u30a2\u30c3\u30d7 velero backup create nginx-backup2 --selector app = nginx # \u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30ea\u30bd\u30fc\u30b9\u306e\u78ba\u8a8d velero backup get velero backup describe nginx-backup velero backup logs nginx-backup # \u30c6\u30b9\u30c8\u7528\u30ea\u30bd\u30fc\u30b9\u306e\u524a\u9664 kubectl delete namespace nginx-example kubectl get deployments --namespace = nginx-example # \u30ea\u30b9\u30c8\u30a2 velero restore create --from-backup nginx-backup2 # \u30ea\u30b9\u30c8\u30a2\u30ea\u30bd\u30fc\u30b9\u306e\u78ba\u8a8d velero restore get kubectl get deployments --namespace = nginx-example","title":"\u691c\u8a3c"},{"location":"kubernetes/velero/#_2","text":"\u30af\u30e9\u30b9\u30bf\u304b\u3089 Velero \u3092\u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u624b\u9806\u3067\u3059\u3002 Uninstalling Velero kubectl delete namespace/velero clusterrolebinding/velero kubectl delete crds -l component = velero","title":"\u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"kubernetes/zalando-postgres-operator/","text":"Zalando Postgres Operator Zalando \u793e\u304c\u958b\u767a\u3057\u3066\u3044\u308b Postgres Operator \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u624b\u9806\u3067\u3059\u3002 Operator \u30c7\u30d7\u30ed\u30a4 \u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 3\u7a2e\u985e\u306e\u30c7\u30d7\u30ed\u30a4\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002 \u30de\u30cb\u30e5\u30a2\u30eb Kustomize Helm chart \u30de\u30cb\u30e5\u30a2\u30eb git clone https://github.com/zalando/postgres-operator.git cd postgres-operator/ kubectl create namespace zalando-postgres config set-context $( kubectl config current-context ) --namespace = zalando-postgres kubectl apply -f manifests/configmap.yaml kubectl apply -f manifests/operator-service-account-rbac.yaml # namespace: defualt \u306e\u3082\u306e\u304c\u3042\u308b\u306e\u3067\u6ce8\u610f kubectl apply -f manifests/postgres-operator.yaml kubectl apply -f manifests/api-service.yaml Web UI \u30c7\u30d7\u30ed\u30a4 Web UI \u304b\u3089 PostgreSQL \u30af\u30e9\u30b9\u30bf\u3092\u4f5c\u6210\u30fb\u7ba1\u7406\u3067\u304d\u307e\u3059\u3002 \u30de\u30cb\u30e5\u30a2\u30eb Web UI \u7528\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4f7f\u3063\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 kubectl apply -f ui/manifests/ Web UI \u306f\u4efb\u610f\u306e namespace \u3092\u7ba1\u7406\u5bfe\u8c61\u3068\u3057\u307e\u3059\u3002 deployment.yaml \u306e TARGET_NAMESPACE \u306b \"*\" \u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u5168 namespace \u3092\u5bfe\u8c61\u3068\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 # deployment.yaml env : - name : \"TARGET_NAMESPACE\" value : \"default\" # \"*\" \u306b\u5909\u66f4 Attention kustomize \u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u30c7\u30d7\u30ed\u30a4\u3067\u304d\u3066\u307e\u305b\u3093\u3002 error: unable to recognize \"manifests/kustomization.yaml\" : no matches for kind \"Kustomization\" in version \"kustomize.config.k8s.io/v1beta1\" Note \u81ea\u5b85\u30e9\u30dc\u3067\u306f Ingress \u3067\u306f\u306a\u304f type: Loadbalancer \u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002 PostgreSQL \u30c7\u30d7\u30ed\u30a4 Postgres Operator \u3092\u4f7f\u3063\u3066 PostgreSQL \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u65b9\u6cd5\u306f2\u7a2e\u985e\u3042\u308a\u307e\u3059\u3002 \u30de\u30cb\u30d5\u30a7\u30b9\u30c8 Web UI \u30de\u30cb\u30d5\u30a7\u30b9\u30c8 \u516c\u5f0f\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30b5\u30f3\u30d7\u30eb\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u304c\u3042\u308b\u306e\u3067\u3001\u53c2\u7167\u3057\u306a\u304c\u3089 Postgres \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u7528\u610f\u3057\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 # example apiVersion : acid.zalan.do/v1 kind : postgresql metadata : name : acid-example namespace : default labels : team : acid spec : teamId : acid postgresql : version : \"13\" # note: String numberOfInstances : 1 volume : size : 1Gi storageClass : rook-ceph-block users : example : [] databases : example : example allowedSourceRanges : resources : requests : cpu : 100m memory : 100Mi limits : cpu : 500m memory : 500Mi Web UI \u30c7\u30d7\u30ed\u30a4\u3057\u305f Service \u307e\u305f\u306f Ingress \u306b\u30a2\u30af\u30bb\u30b9\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b9\u30bf\u4f5c\u6210\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u3001\u9805\u76ee\u5165\u529b\u3057\u3066 Create cluster\u3002 Attention Web UI \u7d4c\u7531\u3067\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u5834\u5408\u306b volume.storageClass \u3092\u6307\u5b9a\u3067\u304d\u307e\u305b\u3093\u3002 Kubernetes \u30af\u30e9\u30b9\u30bf\u306b\u30c7\u30d5\u30a9\u30eb\u30c8 StorageClass \u3092\u8a2d\u5b9a\u3057\u3066\u3044\u306a\u3044\u3068 PVC \u304c Pending \u306e\u307e\u307e\u3067\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u3002 Change the default StorageClass \u3092\u53c2\u7167\u3057\u3066\u30c7\u30d5\u30a9\u30eb\u30c8 StorageClass \u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 \u78ba\u8a8d \u4f5c\u6210\u3057\u305f Postgresql \u30af\u30e9\u30b9\u30bf\u306b\u63a5\u7d9a\u3067\u304d\u308b\u304b\u78ba\u8a8d\u3057\u307e\u3059\u3002 \u30e6\u30fc\u30b6\u30d1\u30b9\u30ef\u30fc\u30c9 \u30e6\u30fc\u30b6\u30d1\u30b9\u30ef\u30fc\u30c9\u306f Operator \u306b\u3088\u308a\u81ea\u52d5\u7684\u306b\u4f5c\u6210\u3055\u308c\u3001 {username}.{team}-{clustername}.credentials.postgresql.acid.zalan.do \u3068\u3044\u3046 Secret \u306b\u683c\u7d0d\u3055\u308c\u307e\u3059\u3002 # namespace: default export PGMASTER = $( kubectl get pods -o jsonpath ={ .items..metadata.name } -l application = spilo ) kubectl port-forward $PGMASTER 6432 :5432 kubectl get secret postgres.acid-example.credentials -o 'jsonpath={.data.password}' | base64 -d export PGSSLMODE = require psql -U postgres -h localhost -p 6432 CentOS 7 \u3078\u306e psql \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo yum install -y postgresql13 \u8a3c\u660e\u66f8 Custom TLS certificates \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u30af\u30e9\u30b9\u30bf\u4f5c\u6210\u6642\u306b TLS \u8a3c\u660e\u66f8\u304c\u81ea\u52d5\u7684\u306b\u4f5c\u6210\u3055\u308c\u308b\u306e\u3067\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u30ab\u30b9\u30bf\u30e0\u8a3c\u660e\u66f8\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 # Secret kubectl create secret tls pg-tls \\ --key pg-tls.key \\ # \u79d8\u5bc6\u9375 --cert pg-tls.crt # \u8a3c\u660e\u66f8 # kind: postgresql spec : tls : secretName : pg-tls pg_hba.conf \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067 pg_hba.conf \u306e\u8a2d\u5b9a\u3092\u4e0a\u66f8\u304d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 spec : patroni : pg_hba : - local all all trust - hostssl all all 0.0.0.0/0 trust - host all all 0.0.0.0/0 trust \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002 root@acid-awx-0:/home/postgres/pgdata/pgroot/data# cat pg_hba.conf # Do not edit this file manually! # It will be overwritten by Patroni! local all all trust hostssl all +zalandos 127 .0.0.1/32 pam host all all 127 .0.0.1/32 md5 hostssl all +zalandos ::1/128 pam host all all ::1/128 md5 hostssl replication standby all md5 hostnossl all all all reject hostssl all +zalandos all pam hostssl all all all md5","title":"Zalando Postgres Operator"},{"location":"kubernetes/zalando-postgres-operator/#zalando-postgres-operator","text":"Zalando \u793e\u304c\u958b\u767a\u3057\u3066\u3044\u308b Postgres Operator \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u624b\u9806\u3067\u3059\u3002","title":"Zalando Postgres Operator"},{"location":"kubernetes/zalando-postgres-operator/#operator","text":"\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 3\u7a2e\u985e\u306e\u30c7\u30d7\u30ed\u30a4\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002 \u30de\u30cb\u30e5\u30a2\u30eb Kustomize Helm chart","title":"Operator \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/zalando-postgres-operator/#_1","text":"git clone https://github.com/zalando/postgres-operator.git cd postgres-operator/ kubectl create namespace zalando-postgres config set-context $( kubectl config current-context ) --namespace = zalando-postgres kubectl apply -f manifests/configmap.yaml kubectl apply -f manifests/operator-service-account-rbac.yaml # namespace: defualt \u306e\u3082\u306e\u304c\u3042\u308b\u306e\u3067\u6ce8\u610f kubectl apply -f manifests/postgres-operator.yaml kubectl apply -f manifests/api-service.yaml","title":"\u30de\u30cb\u30e5\u30a2\u30eb"},{"location":"kubernetes/zalando-postgres-operator/#web-ui","text":"Web UI \u304b\u3089 PostgreSQL \u30af\u30e9\u30b9\u30bf\u3092\u4f5c\u6210\u30fb\u7ba1\u7406\u3067\u304d\u307e\u3059\u3002","title":"Web UI \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/zalando-postgres-operator/#_2","text":"Web UI \u7528\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4f7f\u3063\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 kubectl apply -f ui/manifests/ Web UI \u306f\u4efb\u610f\u306e namespace \u3092\u7ba1\u7406\u5bfe\u8c61\u3068\u3057\u307e\u3059\u3002 deployment.yaml \u306e TARGET_NAMESPACE \u306b \"*\" \u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u5168 namespace \u3092\u5bfe\u8c61\u3068\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 # deployment.yaml env : - name : \"TARGET_NAMESPACE\" value : \"default\" # \"*\" \u306b\u5909\u66f4 Attention kustomize \u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u30c7\u30d7\u30ed\u30a4\u3067\u304d\u3066\u307e\u305b\u3093\u3002 error: unable to recognize \"manifests/kustomization.yaml\" : no matches for kind \"Kustomization\" in version \"kustomize.config.k8s.io/v1beta1\" Note \u81ea\u5b85\u30e9\u30dc\u3067\u306f Ingress \u3067\u306f\u306a\u304f type: Loadbalancer \u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002","title":"\u30de\u30cb\u30e5\u30a2\u30eb"},{"location":"kubernetes/zalando-postgres-operator/#postgresql","text":"Postgres Operator \u3092\u4f7f\u3063\u3066 PostgreSQL \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u65b9\u6cd5\u306f2\u7a2e\u985e\u3042\u308a\u307e\u3059\u3002 \u30de\u30cb\u30d5\u30a7\u30b9\u30c8 Web UI","title":"PostgreSQL \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/zalando-postgres-operator/#_3","text":"\u516c\u5f0f\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30b5\u30f3\u30d7\u30eb\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u304c\u3042\u308b\u306e\u3067\u3001\u53c2\u7167\u3057\u306a\u304c\u3089 Postgres \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u7528\u610f\u3057\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 # example apiVersion : acid.zalan.do/v1 kind : postgresql metadata : name : acid-example namespace : default labels : team : acid spec : teamId : acid postgresql : version : \"13\" # note: String numberOfInstances : 1 volume : size : 1Gi storageClass : rook-ceph-block users : example : [] databases : example : example allowedSourceRanges : resources : requests : cpu : 100m memory : 100Mi limits : cpu : 500m memory : 500Mi","title":"\u30de\u30cb\u30d5\u30a7\u30b9\u30c8"},{"location":"kubernetes/zalando-postgres-operator/#web-ui_1","text":"\u30c7\u30d7\u30ed\u30a4\u3057\u305f Service \u307e\u305f\u306f Ingress \u306b\u30a2\u30af\u30bb\u30b9\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b9\u30bf\u4f5c\u6210\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u3001\u9805\u76ee\u5165\u529b\u3057\u3066 Create cluster\u3002 Attention Web UI \u7d4c\u7531\u3067\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u5834\u5408\u306b volume.storageClass \u3092\u6307\u5b9a\u3067\u304d\u307e\u305b\u3093\u3002 Kubernetes \u30af\u30e9\u30b9\u30bf\u306b\u30c7\u30d5\u30a9\u30eb\u30c8 StorageClass \u3092\u8a2d\u5b9a\u3057\u3066\u3044\u306a\u3044\u3068 PVC \u304c Pending \u306e\u307e\u307e\u3067\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u3002 Change the default StorageClass \u3092\u53c2\u7167\u3057\u3066\u30c7\u30d5\u30a9\u30eb\u30c8 StorageClass \u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002","title":"Web UI"},{"location":"kubernetes/zalando-postgres-operator/#_4","text":"\u4f5c\u6210\u3057\u305f Postgresql \u30af\u30e9\u30b9\u30bf\u306b\u63a5\u7d9a\u3067\u304d\u308b\u304b\u78ba\u8a8d\u3057\u307e\u3059\u3002","title":"\u78ba\u8a8d"},{"location":"kubernetes/zalando-postgres-operator/#_5","text":"\u30e6\u30fc\u30b6\u30d1\u30b9\u30ef\u30fc\u30c9\u306f Operator \u306b\u3088\u308a\u81ea\u52d5\u7684\u306b\u4f5c\u6210\u3055\u308c\u3001 {username}.{team}-{clustername}.credentials.postgresql.acid.zalan.do \u3068\u3044\u3046 Secret \u306b\u683c\u7d0d\u3055\u308c\u307e\u3059\u3002 # namespace: default export PGMASTER = $( kubectl get pods -o jsonpath ={ .items..metadata.name } -l application = spilo ) kubectl port-forward $PGMASTER 6432 :5432 kubectl get secret postgres.acid-example.credentials -o 'jsonpath={.data.password}' | base64 -d export PGSSLMODE = require psql -U postgres -h localhost -p 6432","title":"\u30e6\u30fc\u30b6\u30d1\u30b9\u30ef\u30fc\u30c9"},{"location":"kubernetes/zalando-postgres-operator/#centos-7-psql","text":"sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo yum install -y postgresql13","title":"CentOS 7 \u3078\u306e psql \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"kubernetes/zalando-postgres-operator/#_6","text":"Custom TLS certificates \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u30af\u30e9\u30b9\u30bf\u4f5c\u6210\u6642\u306b TLS \u8a3c\u660e\u66f8\u304c\u81ea\u52d5\u7684\u306b\u4f5c\u6210\u3055\u308c\u308b\u306e\u3067\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u30ab\u30b9\u30bf\u30e0\u8a3c\u660e\u66f8\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 # Secret kubectl create secret tls pg-tls \\ --key pg-tls.key \\ # \u79d8\u5bc6\u9375 --cert pg-tls.crt # \u8a3c\u660e\u66f8 # kind: postgresql spec : tls : secretName : pg-tls","title":"\u8a3c\u660e\u66f8"},{"location":"kubernetes/zalando-postgres-operator/#pg_hbaconf","text":"\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067 pg_hba.conf \u306e\u8a2d\u5b9a\u3092\u4e0a\u66f8\u304d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 spec : patroni : pg_hba : - local all all trust - hostssl all all 0.0.0.0/0 trust - host all all 0.0.0.0/0 trust \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002 root@acid-awx-0:/home/postgres/pgdata/pgroot/data# cat pg_hba.conf # Do not edit this file manually! # It will be overwritten by Patroni! local all all trust hostssl all +zalandos 127 .0.0.1/32 pam host all all 127 .0.0.1/32 md5 hostssl all +zalandos ::1/128 pam host all all ::1/128 md5 hostssl replication standby all md5 hostnossl all all all reject hostssl all +zalandos all pam hostssl all all all md5","title":"pg_hba.conf"},{"location":"prometheus/kube-prometheus/","text":"kube-prometheus kube-prometheus \u3068\u3044\u3046 Prometheus Operator \u3092\u4f7f\u3063\u305f Prometheus Stack \u3092 Kubernetes \u30af\u30e9\u30b9\u30bf\u306b\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u624b\u9806\u3067\u3059\u3002 Install Golang \u3068 Jsonnet \u3068 Jsonnet-bundler \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 sudo yum install epel-release sudo yum install golang export GOPATH = /home/nnstt1/go export PATH = $PATH : $GOPATH /bin GO111MODULE = \"on\" go get github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb go get github.com/google/go-jsonnet/cmd/jsonnet go get github.com/brancz/gojsontoyaml \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5b8c\u4e86\u3057\u305f\u3089\u516c\u5f0f\u306e\u30b5\u30f3\u30d7\u30eb Jsonnet \u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u3063\u3066\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u751f\u6210\u3057\u307e\u3059\u3002 $ mkdir my-kube-prometheus ; cd my-kube-prometheus $ RELEASE_VER = release-0.8 $ jb init $ jb install github.com/prometheus-operator/kube-prometheus/jsonnet/kube-prometheus@ $RELEASE_VER $ wget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/ $RELEASE_VER /example.jsonnet -O example.jsonnet $ wget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/ $RELEASE_VER /build.sh -O build.sh $ ./build.sh example.jsonnet \u30d3\u30eb\u30c9\u304c\u5b8c\u4e86\u3057\u305f\u3089 manifests/ \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u4e0b\u306b\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002 Namespace \u3084 CRD \u3068\u3044\u3063\u305f\u5148\u306b\u30af\u30e9\u30b9\u30bf\u3078\u30c7\u30d7\u30ed\u30a4\u3059\u3079\u304d\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u304c manifests/setup/ \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u542b\u307e\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u5148\u306b\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 $ kubectl apply -f manifests/setup $ kubectl apply -f manifests/ \u30ab\u30b9\u30bf\u30e0 Prometheus \u3084 Grafana \u306e\u30b5\u30fc\u30d3\u30b9\u306b\u30af\u30e9\u30b9\u30bf\u5916\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306b Ingress \u3092\u4f7f\u3044\u307e\u3059\u3002 Ingress \u7528\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3082 Jsonnet \u304b\u3089\u751f\u6210\u3059\u308b\u305f\u3081\u3001 example.jsonnet \u3092\u7de8\u96c6\u3057\u307e\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u30ab\u30b9\u30bf\u30e0\u3057\u305f Jsonnet \u3068\u5206\u304b\u308b\u3088\u3046\u306b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 $ cp example.jsonnet home-cluster.jsonnet Ingress \u30b5\u30f3\u30d7\u30eb Jsonnet \u3067\u306f Prometheus \u3084 Grafana \u306b\u30af\u30e9\u30b9\u30bf\u5916\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3059\u308b\u305f\u3081\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306f\u51fa\u529b\u3055\u308c\u307e\u305b\u3093\u3002 \u5404\u30b5\u30fc\u30d3\u30b9\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u305f\u3081\u306e Ingress \u30ea\u30bd\u30fc\u30b9\u3092\u51fa\u529b\u3059\u308b\u3088\u3046\u306b Jsonnet \u3092\u7de8\u96c6\u3057\u307e\u3059\u3002 \u4e8b\u524d\u306b Nginx Ingress Controller \u3092\u30af\u30e9\u30b9\u30bf\u306b\u30c7\u30d7\u30ed\u30a4\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u6761\u4ef6\u3067\u3059\u3002 ingress+:: { 'prometheus-k8s': { apiVersion: 'networking.k8s.io/v1', kind: 'Ingress', metadata: { name: $.prometheus.prometheus.metadata.name, namespace: $.prometheus.prometheus.metadata.namespace, annotations: { 'kubernetes.io/ingress.class': 'nginx', }, }, spec: { rules: [{ host: 'prometheus.nnstt1.work', http: { paths: [{ backend: { service: { name: $.prometheus.service.metadata.name, port: { number: 9090 }, }, }, path: '/', pathType: 'Prefix', }], }, }], }, }, 'grafana': { apiVersion: 'networking.k8s.io/v1', kind: 'Ingress', metadata: { name: $.grafana.service.metadata.name, namespace: $.grafana.service.metadata.namespace, annotations: { 'kubernetes.io/ingress.class': 'nginx', }, }, spec: { rules: [{ host: 'grafana.nnstt1.work', http: { paths: [{ backend: { service: { name: $.grafana.service.metadata.name, port: { number: 3000 }, }, }, path: '/', pathType: 'Prefix', }], }, }], }, }, }, \u4e0a\u8a18\u3092\u8ffd\u52a0\u3057\u3066\u30d3\u30eb\u30c9\u3059\u308b\u3068\u3001 ingress-prometheus-k8s.yaml \u3068 ingress-grafana.yaml \u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002 PVC Prometheus \u3067\u53d6\u5f97\u3057\u305f\u30e1\u30c8\u30ea\u30af\u30b9\u3092\u6c38\u7d9a\u5316\u3059\u308b\u305f\u3081\u3001Rook Ceph \u3067\u69cb\u7bc9\u3057\u3066\u3044\u308b StorageClass \u3092\u4f7f\u3063\u305f volumeClaimTemplates \u3092 Prometheus \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306b\u8ffd\u52a0\u3057\u307e\u3059\u3002 \u306a\u304a\u3001Grafana \u306e\u30c7\u30fc\u30bf\u6c38\u7d9a\u5316\u306f kube-prometheus \u306e\u30dd\u30ea\u30b7\u30fc\u5916\u306e\u3088\u3046\u3067 ( \u53c2\u8003 )\u3001\u305d\u308c\u306b\u5023\u3063\u3066 Grafana \u306e volumeClaimTemplates \u306f\u8a2d\u5b9a\u3057\u307e\u305b\u3093\u3002 prometheus+:: { prometheus+: { spec+: { externalUrl: 'http://prometheus.nnstt1.work', retention: '30d', storage: { volumeClaimTemplate: { apiVersion: 'v1', kind: 'PersistentVolumeClaim', spec: { accessModes: ['ReadWriteOnce'], resources: { requests: { storage: '10Gi' }, }, storageClassName: 'rook-ceph-block', }, }, }, }, }, }, ServiceMonitor \u65b0\u898f\u306b\u4f5c\u6210\u3057\u305f ServiceMonitor \u3092 Prometheus \u304c\u76e3\u8996\u5bfe\u8c61\u3068\u3059\u308b\u3088\u3046\u306b Jsonnet \u3092\u5909\u66f4\u3057\u307e\u3059\u3002 \u5177\u4f53\u7684\u306b\u306f\u3001Prometheus Operator \u306b\u3088\u308b\u76e3\u8996\u5bfe\u8c61\u3092\u5168 Namespace \u306b\u3059\u308b\u3088\u3046 Jsonnet \u3067\u5b9a\u7fa9\u3057\u307e\u3059\u3002 https://github.com/prometheus-operator/kube-prometheus/blob/main/examples/all-namespaces.jsonnet local kp = (import 'kube-prometheus/main.libsonnet') + (import 'kube-prometheus/addons/all-namespaces.libsonnet') + { # \u8ffd\u52a0 values+:: { common+: { namespace: 'monitoring', }, prometheus+: { # \u8ffd\u52a0 namespaces: [], }, }, }; { ['00namespace-' + name]: kp.kubePrometheus[name] for name in std.objectFields(kp.kubePrometheus) } + { ['0prometheus-operator-' + name]: kp.prometheusOperator[name] for name in std.objectFields(kp.prometheusOperator) } + { ['node-exporter-' + name]: kp.nodeExporter[name] for name in std.objectFields(kp.nodeExporter) } + { ['kube-state-metrics-' + name]: kp.kubeStateMetrics[name] for name in std.objectFields(kp.kubeStateMetrics) } + { ['alertmanager-' + name]: kp.alertmanager[name] for name in std.objectFields(kp.alertmanager) } + { ['prometheus-' + name]: kp.prometheus[name] for name in std.objectFields(kp.prometheus) } + { ['grafana-' + name]: kp.grafana[name] for name in std.objectFields(kp.grafana) }","title":"kube-prometheus"},{"location":"prometheus/kube-prometheus/#kube-prometheus","text":"kube-prometheus \u3068\u3044\u3046 Prometheus Operator \u3092\u4f7f\u3063\u305f Prometheus Stack \u3092 Kubernetes \u30af\u30e9\u30b9\u30bf\u306b\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u624b\u9806\u3067\u3059\u3002","title":"kube-prometheus"},{"location":"prometheus/kube-prometheus/#install","text":"Golang \u3068 Jsonnet \u3068 Jsonnet-bundler \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 sudo yum install epel-release sudo yum install golang export GOPATH = /home/nnstt1/go export PATH = $PATH : $GOPATH /bin GO111MODULE = \"on\" go get github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb go get github.com/google/go-jsonnet/cmd/jsonnet go get github.com/brancz/gojsontoyaml \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5b8c\u4e86\u3057\u305f\u3089\u516c\u5f0f\u306e\u30b5\u30f3\u30d7\u30eb Jsonnet \u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u3063\u3066\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u751f\u6210\u3057\u307e\u3059\u3002 $ mkdir my-kube-prometheus ; cd my-kube-prometheus $ RELEASE_VER = release-0.8 $ jb init $ jb install github.com/prometheus-operator/kube-prometheus/jsonnet/kube-prometheus@ $RELEASE_VER $ wget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/ $RELEASE_VER /example.jsonnet -O example.jsonnet $ wget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/ $RELEASE_VER /build.sh -O build.sh $ ./build.sh example.jsonnet \u30d3\u30eb\u30c9\u304c\u5b8c\u4e86\u3057\u305f\u3089 manifests/ \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u4e0b\u306b\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002 Namespace \u3084 CRD \u3068\u3044\u3063\u305f\u5148\u306b\u30af\u30e9\u30b9\u30bf\u3078\u30c7\u30d7\u30ed\u30a4\u3059\u3079\u304d\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u304c manifests/setup/ \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u542b\u307e\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u5148\u306b\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 $ kubectl apply -f manifests/setup $ kubectl apply -f manifests/","title":"Install"},{"location":"prometheus/kube-prometheus/#_1","text":"Prometheus \u3084 Grafana \u306e\u30b5\u30fc\u30d3\u30b9\u306b\u30af\u30e9\u30b9\u30bf\u5916\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306b Ingress \u3092\u4f7f\u3044\u307e\u3059\u3002 Ingress \u7528\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3082 Jsonnet \u304b\u3089\u751f\u6210\u3059\u308b\u305f\u3081\u3001 example.jsonnet \u3092\u7de8\u96c6\u3057\u307e\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u30ab\u30b9\u30bf\u30e0\u3057\u305f Jsonnet \u3068\u5206\u304b\u308b\u3088\u3046\u306b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 $ cp example.jsonnet home-cluster.jsonnet","title":"\u30ab\u30b9\u30bf\u30e0"},{"location":"prometheus/kube-prometheus/#ingress","text":"\u30b5\u30f3\u30d7\u30eb Jsonnet \u3067\u306f Prometheus \u3084 Grafana \u306b\u30af\u30e9\u30b9\u30bf\u5916\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3059\u308b\u305f\u3081\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306f\u51fa\u529b\u3055\u308c\u307e\u305b\u3093\u3002 \u5404\u30b5\u30fc\u30d3\u30b9\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u305f\u3081\u306e Ingress \u30ea\u30bd\u30fc\u30b9\u3092\u51fa\u529b\u3059\u308b\u3088\u3046\u306b Jsonnet \u3092\u7de8\u96c6\u3057\u307e\u3059\u3002 \u4e8b\u524d\u306b Nginx Ingress Controller \u3092\u30af\u30e9\u30b9\u30bf\u306b\u30c7\u30d7\u30ed\u30a4\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u6761\u4ef6\u3067\u3059\u3002 ingress+:: { 'prometheus-k8s': { apiVersion: 'networking.k8s.io/v1', kind: 'Ingress', metadata: { name: $.prometheus.prometheus.metadata.name, namespace: $.prometheus.prometheus.metadata.namespace, annotations: { 'kubernetes.io/ingress.class': 'nginx', }, }, spec: { rules: [{ host: 'prometheus.nnstt1.work', http: { paths: [{ backend: { service: { name: $.prometheus.service.metadata.name, port: { number: 9090 }, }, }, path: '/', pathType: 'Prefix', }], }, }], }, }, 'grafana': { apiVersion: 'networking.k8s.io/v1', kind: 'Ingress', metadata: { name: $.grafana.service.metadata.name, namespace: $.grafana.service.metadata.namespace, annotations: { 'kubernetes.io/ingress.class': 'nginx', }, }, spec: { rules: [{ host: 'grafana.nnstt1.work', http: { paths: [{ backend: { service: { name: $.grafana.service.metadata.name, port: { number: 3000 }, }, }, path: '/', pathType: 'Prefix', }], }, }], }, }, }, \u4e0a\u8a18\u3092\u8ffd\u52a0\u3057\u3066\u30d3\u30eb\u30c9\u3059\u308b\u3068\u3001 ingress-prometheus-k8s.yaml \u3068 ingress-grafana.yaml \u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002","title":"Ingress"},{"location":"prometheus/kube-prometheus/#pvc","text":"Prometheus \u3067\u53d6\u5f97\u3057\u305f\u30e1\u30c8\u30ea\u30af\u30b9\u3092\u6c38\u7d9a\u5316\u3059\u308b\u305f\u3081\u3001Rook Ceph \u3067\u69cb\u7bc9\u3057\u3066\u3044\u308b StorageClass \u3092\u4f7f\u3063\u305f volumeClaimTemplates \u3092 Prometheus \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306b\u8ffd\u52a0\u3057\u307e\u3059\u3002 \u306a\u304a\u3001Grafana \u306e\u30c7\u30fc\u30bf\u6c38\u7d9a\u5316\u306f kube-prometheus \u306e\u30dd\u30ea\u30b7\u30fc\u5916\u306e\u3088\u3046\u3067 ( \u53c2\u8003 )\u3001\u305d\u308c\u306b\u5023\u3063\u3066 Grafana \u306e volumeClaimTemplates \u306f\u8a2d\u5b9a\u3057\u307e\u305b\u3093\u3002 prometheus+:: { prometheus+: { spec+: { externalUrl: 'http://prometheus.nnstt1.work', retention: '30d', storage: { volumeClaimTemplate: { apiVersion: 'v1', kind: 'PersistentVolumeClaim', spec: { accessModes: ['ReadWriteOnce'], resources: { requests: { storage: '10Gi' }, }, storageClassName: 'rook-ceph-block', }, }, }, }, }, },","title":"PVC"},{"location":"prometheus/kube-prometheus/#servicemonitor","text":"\u65b0\u898f\u306b\u4f5c\u6210\u3057\u305f ServiceMonitor \u3092 Prometheus \u304c\u76e3\u8996\u5bfe\u8c61\u3068\u3059\u308b\u3088\u3046\u306b Jsonnet \u3092\u5909\u66f4\u3057\u307e\u3059\u3002 \u5177\u4f53\u7684\u306b\u306f\u3001Prometheus Operator \u306b\u3088\u308b\u76e3\u8996\u5bfe\u8c61\u3092\u5168 Namespace \u306b\u3059\u308b\u3088\u3046 Jsonnet \u3067\u5b9a\u7fa9\u3057\u307e\u3059\u3002 https://github.com/prometheus-operator/kube-prometheus/blob/main/examples/all-namespaces.jsonnet local kp = (import 'kube-prometheus/main.libsonnet') + (import 'kube-prometheus/addons/all-namespaces.libsonnet') + { # \u8ffd\u52a0 values+:: { common+: { namespace: 'monitoring', }, prometheus+: { # \u8ffd\u52a0 namespaces: [], }, }, }; { ['00namespace-' + name]: kp.kubePrometheus[name] for name in std.objectFields(kp.kubePrometheus) } + { ['0prometheus-operator-' + name]: kp.prometheusOperator[name] for name in std.objectFields(kp.prometheusOperator) } + { ['node-exporter-' + name]: kp.nodeExporter[name] for name in std.objectFields(kp.nodeExporter) } + { ['kube-state-metrics-' + name]: kp.kubeStateMetrics[name] for name in std.objectFields(kp.kubeStateMetrics) } + { ['alertmanager-' + name]: kp.alertmanager[name] for name in std.objectFields(kp.alertmanager) } + { ['prometheus-' + name]: kp.prometheus[name] for name in std.objectFields(kp.prometheus) } + { ['grafana-' + name]: kp.grafana[name] for name in std.objectFields(kp.grafana) }","title":"ServiceMonitor"},{"location":"prometheus/prometheus/","text":"Prometheus kube-prometheus sudo yum install epel-release sudo yum install golang export GOPATH = /home/nnstt1/go export PATH = $PATH : $GOPATH /bin GO111MODULE = \"on\" go get github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb go get github.com/google/go-jsonnet/cmd/jsonnet go get github.com/brancz/gojsontoyaml jb install github.com/prometheus-operator/kube-prometheus/jsonnet/kube-prometheus@release-0.6 kubectl apply -f manifests/setup kubectl apply -f manifests/ kubectl get all -n monitoring kubectl apply -f grafana-service.yaml # grafana-service.yaml apiVersion : v1 kind : Service metadata : name : grafana labels : app : grafana namespace : monitoring spec : ports : - port : 80 targetPort : 3000 selector : app : grafana type : LoadBalancer","title":"Prometheus"},{"location":"prometheus/prometheus/#prometheus","text":"","title":"Prometheus"},{"location":"prometheus/prometheus/#kube-prometheus","text":"sudo yum install epel-release sudo yum install golang export GOPATH = /home/nnstt1/go export PATH = $PATH : $GOPATH /bin GO111MODULE = \"on\" go get github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb go get github.com/google/go-jsonnet/cmd/jsonnet go get github.com/brancz/gojsontoyaml jb install github.com/prometheus-operator/kube-prometheus/jsonnet/kube-prometheus@release-0.6 kubectl apply -f manifests/setup kubectl apply -f manifests/ kubectl get all -n monitoring kubectl apply -f grafana-service.yaml # grafana-service.yaml apiVersion : v1 kind : Service metadata : name : grafana labels : app : grafana namespace : monitoring spec : ports : - port : 80 targetPort : 3000 selector : app : grafana type : LoadBalancer","title":"kube-prometheus"},{"location":"prometheus/snmp-exporter/","text":"SNMP Exporter SNMP Exporter \u306e\u8a2d\u5b9a\u65b9\u6cd5\u3067\u3059\u3002 generator https://github.com/prometheus/snmp_exporter/tree/master/generator Config Generator \u3092\u4f7f\u3063\u3066 SNMP Exporter \u7528\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb snmp.yml \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 generator \u306e\u5229\u7528\u65b9\u6cd5\u306f2\u901a\u308a\u3042\u308a\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304b\u3089\u30d3\u30eb\u30c9 Docker \u30a4\u30e1\u30fc\u30b8 \u3053\u3053\u3067\u306f Docker \u30a4\u30e1\u30fc\u30b8\u3092\u5229\u7528\u3059\u308b\u65b9\u6cd5\u3092\u8a18\u8f09\u3057\u307e\u3059\u3002 Docker Build \u516c\u5f0f\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u3042\u308b Dockerfile \u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u30d3\u30eb\u30c9\u3057\u307e\u3059\u3002 # Dockerfile FROM golang:latest RUN apt-get update && \\ apt-get install -y libsnmp-dev p7zip-full && \\ go get github.com/prometheus/snmp_exporter/generator && \\ cd /go/src/github.com/prometheus/snmp_exporter/generator && \\ go get -v . && \\ go install WORKDIR \"/opt\" ENTRYPOINT [ \"/go/bin/generator\" ] ENV MIBDIRS mibs CMD [ \"generate\" ] git clone https://github.com/prometheus/snmp_exporter.git cd snmp_exporter/generator docker build -t snmp-generator . Docker Run cd <work-dir> docker run -ti \\ -v \" ${ PWD } :/opt/\" \\ snmp-generator generate","title":"SNMP Exporter"},{"location":"prometheus/snmp-exporter/#snmp-exporter","text":"SNMP Exporter \u306e\u8a2d\u5b9a\u65b9\u6cd5\u3067\u3059\u3002","title":"SNMP Exporter"},{"location":"prometheus/snmp-exporter/#generator","text":"https://github.com/prometheus/snmp_exporter/tree/master/generator Config Generator \u3092\u4f7f\u3063\u3066 SNMP Exporter \u7528\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb snmp.yml \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 generator \u306e\u5229\u7528\u65b9\u6cd5\u306f2\u901a\u308a\u3042\u308a\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304b\u3089\u30d3\u30eb\u30c9 Docker \u30a4\u30e1\u30fc\u30b8 \u3053\u3053\u3067\u306f Docker \u30a4\u30e1\u30fc\u30b8\u3092\u5229\u7528\u3059\u308b\u65b9\u6cd5\u3092\u8a18\u8f09\u3057\u307e\u3059\u3002","title":"generator"},{"location":"prometheus/snmp-exporter/#docker-build","text":"\u516c\u5f0f\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u3042\u308b Dockerfile \u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u30d3\u30eb\u30c9\u3057\u307e\u3059\u3002 # Dockerfile FROM golang:latest RUN apt-get update && \\ apt-get install -y libsnmp-dev p7zip-full && \\ go get github.com/prometheus/snmp_exporter/generator && \\ cd /go/src/github.com/prometheus/snmp_exporter/generator && \\ go get -v . && \\ go install WORKDIR \"/opt\" ENTRYPOINT [ \"/go/bin/generator\" ] ENV MIBDIRS mibs CMD [ \"generate\" ] git clone https://github.com/prometheus/snmp_exporter.git cd snmp_exporter/generator docker build -t snmp-generator .","title":"Docker Build"},{"location":"prometheus/snmp-exporter/#docker-run","text":"cd <work-dir> docker run -ti \\ -v \" ${ PWD } :/opt/\" \\ snmp-generator generate","title":"Docker Run"},{"location":"raspi-ceph/ceph-grafana/","text":"Ceph-Grafana \u30b3\u30f3\u30c6\u30ca \u30d3\u30eb\u30c9 ARM \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u7528\u306e ceph-grafana \u30b3\u30f3\u30c6\u30ca\u30a4\u30e1\u30fc\u30b8\u3092\u30d3\u30eb\u30c9\u3057\u307e\u3059\u3002 Docker \u304b Podman \u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u524d\u63d0\u3067\u3059\u3002 $ yum install -y jq make buildah $ echo <<EOF > /etc/sysctl.d/42-rootless.conf user.max_user_namespaces=10000 EOF $ sysctl --system $ vim /etc/default/grub # systemd.unified_cgroup_hierarchy=0 \u3092\u8ffd\u52a0 # GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet systemd.unified_cgroup_hierarchy=0\" $ grub2-mkconfig $ shutdown -r now $ sudo make ceph_version = octopus \u4f7f\u7528\u65b9\u6cd5 \u30ab\u30b9\u30bf\u30e0\u30a4\u30e1\u30fc\u30b8\u3092 Ceph \u3067\u4f7f\u3046\u8a2d\u5b9a\u3067\u3059\u3002 $ ceph config set mgr mgr/cephadm/container_image_grafana nnstt1/ceph-grafana:6.7.4 \u521d\u671f\u69cb\u7bc9\u6642\u306b\u76e3\u8996\u30b9\u30bf\u30c3\u30af\u3092\u69cb\u7bc9\u3057\u306a\u304b\u3063\u305f\u5834\u5408\u306e\u30de\u30cb\u30e5\u30a2\u30eb\u69cb\u7bc9\u3067\u3059\u3002 $ ceph mgr module enable prometheus $ ceph orch apply node-exporter '*' $ ceph orch apply alertmanager 1 $ ceph orch apply prometheus 1 $ ceph orch apply grafana 1","title":"Ceph-Grafana \u30b3\u30f3\u30c6\u30ca"},{"location":"raspi-ceph/ceph-grafana/#ceph-grafana","text":"","title":"Ceph-Grafana \u30b3\u30f3\u30c6\u30ca"},{"location":"raspi-ceph/ceph-grafana/#_1","text":"ARM \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u7528\u306e ceph-grafana \u30b3\u30f3\u30c6\u30ca\u30a4\u30e1\u30fc\u30b8\u3092\u30d3\u30eb\u30c9\u3057\u307e\u3059\u3002 Docker \u304b Podman \u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u524d\u63d0\u3067\u3059\u3002 $ yum install -y jq make buildah $ echo <<EOF > /etc/sysctl.d/42-rootless.conf user.max_user_namespaces=10000 EOF $ sysctl --system $ vim /etc/default/grub # systemd.unified_cgroup_hierarchy=0 \u3092\u8ffd\u52a0 # GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet systemd.unified_cgroup_hierarchy=0\" $ grub2-mkconfig $ shutdown -r now $ sudo make ceph_version = octopus","title":"\u30d3\u30eb\u30c9"},{"location":"raspi-ceph/ceph-grafana/#_2","text":"\u30ab\u30b9\u30bf\u30e0\u30a4\u30e1\u30fc\u30b8\u3092 Ceph \u3067\u4f7f\u3046\u8a2d\u5b9a\u3067\u3059\u3002 $ ceph config set mgr mgr/cephadm/container_image_grafana nnstt1/ceph-grafana:6.7.4 \u521d\u671f\u69cb\u7bc9\u6642\u306b\u76e3\u8996\u30b9\u30bf\u30c3\u30af\u3092\u69cb\u7bc9\u3057\u306a\u304b\u3063\u305f\u5834\u5408\u306e\u30de\u30cb\u30e5\u30a2\u30eb\u69cb\u7bc9\u3067\u3059\u3002 $ ceph mgr module enable prometheus $ ceph orch apply node-exporter '*' $ ceph orch apply alertmanager 1 $ ceph orch apply prometheus 1 $ ceph orch apply grafana 1","title":"\u4f7f\u7528\u65b9\u6cd5"},{"location":"raspi-ceph/cephadm/","text":"Ceph \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9 Cephadm \u3092\u4f7f\u3063\u3066 Ceph \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u3066\u3044\u304d\u307e\u3059\u3002 \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9 \u30d1\u30c3\u30b1\u30fc\u30b8\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Ceph \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9\u306b\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ yum install -y python3 gdisk $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine $ yum install -y yum-utils $ yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo $ yum install -y docker-ce docker-ce-cli containerd.io $ systemctl start docker $ docker run hello-world $ systemctl enable docker Cephadm \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb GitHub \u304b\u3089 cephadm \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm $ curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm $ chmod +x cephadm $ ./cephadm add-repo --release octopus Writing repo to /etc/yum.repos.d/ceph.repo... Enabling EPEL... # \u30a8\u30e9\u30fc\u51fa\u305f\u5834\u5408\u306f\u518d\u5b9f\u884c\u3059\u308c\u3070\u3044\u3051\u308b $ ./cephadm install # ceph cli $ cephadm install ceph-common Bootstrap cephadm bootstrap \u30b3\u30de\u30f3\u30c9\u3067 Ceph \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002 $ cephadm bootstrap --mon-ip 192 .168.2.11 --skip-monitoring-stack --skip-monitoring-stack Prometheus \u306a\u3069\u306e\u76e3\u8996\u7528\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u8ffd\u52a0\u3057\u306a\u3044 \u30db\u30b9\u30c8\u8ffd\u52a0 \u69cb\u7bc9\u3057\u305f Ceph \u30af\u30e9\u30b9\u30bf\u306b\u30ce\u30fc\u30c9\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002 # \u73fe\u72b6\u306e\u78ba\u8a8d $ ceph orch host ls # Ceph \u516c\u958b\u9375\u306e\u767b\u9332 $ ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph2 $ ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph3 # \u30db\u30b9\u30c8\u8ffd\u52a0 $ ceph orch host add ceph2 $ ceph orch host add ceph3 # \u8ffd\u52a0\u5f8c\u306e\u78ba\u8a8d $ ceph orch host ls OSD \u767b\u9332 OSD \u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002 # \u30c7\u30d0\u30a4\u30b9\u306e\u72b6\u614b\u3092\u78ba\u8a8d $ ceph orch device ls --refresh Hostname Path Type Serial Size Health Ident Fault Available ceph1 /dev/sda hdd DB9876543214E 256G Unknown N/A N/A Yes ceph2 /dev/sda hdd DB9876543214E 256G Unknown N/A N/A Yes ceph3 /dev/sda hdd DB9876543214E 256G Unknown N/A N/A Yes # Available: Yes \u306e\u5168\u30c7\u30d0\u30a4\u30b9\u3092\u5bfe\u8c61\u306b OSD \u3092\u8ffd\u52a0 $ ceph orch apply osd --all-available-devices Scheduled osd.all-available-devices update... # OSD Tree \u3067\u72b6\u614b\u78ba\u8a8d $ ceph osd tree service_type : osd service_id : osd_spec_default placement : host_pattern : '*' data_devices : all : true encrypted : false objectstore : bluestore ceph.conf \u306e\u914d\u5e03 https://docs.ceph.com/en/latest/cephadm/operations/#etc-ceph-ceph-conf \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9\u5f8c\u306b\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u30b5\u30d6\u30ce\u30fc\u30c9\u306b\u914d\u5e03\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3002 ceph config set mgr mgr/cephadm/manage_etc_ceph_ceph_conf true RADOSGW https://docs.ceph.com/en/latest/cephadm/rgw/ $ radosgw-admin realm create --rgw-realm = myorg --default $ radosgw-admin zonegroup create --rgw-zonegroup = japan --endpoints http://ceph2:80 --rgw-realm = myorg --master --default $ radosgw-admin zone create --rgw-zonegroup = japan --rgw-zone = japan-east-1 --master --default $ ceph orch apply rgw myorg japan-east-1 --placement = \"2 ceph2 ceph3\" $ radosgw-admin period update --rgw-realm = myorg --commit HA-RADOSGW $ cat <<EOF > /etc/sysctl.d/radosgw_ipv4.conf net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1 EOF $ sysctl -p /etc/sysctl.d/radosgw_ipv4.conf $ ceph orch apply -i radosgw.yaml \u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u753b\u9762\u6709\u52b9\u5316 $ radosgw-admin user create --uid = admin --display-name = admin --system $ radosgw-admin user info --uid = admin $ ceph dashboard set-rgw-api-user-id admin $ ceph dashboard set-rgw-api-access-key A6D1B1JL68472ZH972ZL $ ceph dashboard set-rgw-api-secret-key zb3RBRkLJKBLlzdz7hJNfY6VOY5ZK8vjF5EUabrs \u3084\u308a\u76f4\u3057\u624b\u9806 Cephadm \u3067\u69cb\u7bc9\u3057\u305f\u30af\u30e9\u30b9\u30bf\u3068\u30c7\u30a3\u30b9\u30af\u306e\u521d\u671f\u5316\u3092\u304a\u3053\u306a\u3063\u3066\u3001\u30af\u30e9\u30b9\u30bf\u69cb\u7bc9\u3092\u3084\u308a\u76f4\u3057\u307e\u3059\u3002 # \u30b5\u30d6\u30ce\u30fc\u30c9\u3067\u5b9f\u884c $ systemctl stop docker.socket # \u30e1\u30a4\u30f3\u30ce\u30fc\u30c9\u3067\u5b9f\u884c $ cephadm rm-cluster --fsid <fsid> --force $ rm -i /etc/systemd/system/ceph.target # \u30b5\u30d6\u30ce\u30fc\u30c8\u3067\u5b9f\u884c $ rm -rf /etc/systemd/system/ceph* # \u5404\u30ce\u30fc\u30c9\u3067\u5b9f\u884c $ DISK = \"/dev/sda\" $ sgdisk --zap-all $DISK $ dd if = /dev/zero of = \" $DISK \" bs = 1M count = 100 oflag = direct,dsync $ ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % $ rm -rf /dev/ceph-* \u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c8 OSD \u30b3\u30f3\u30c6\u30ca\u304c activate \u2192 deactivate \u2192 activate \u306e\u7121\u9650\u30eb\u30fc\u30d7\u3067\u8d77\u52d5\u3057\u306a\u3044\u306e\u3067\u8abf\u67fb\u3057\u307e\u3059\u3002 docker \u306e\u30ed\u30ae\u30f3\u30b0\u30c9\u30e9\u30a4\u30d0\u30fc\u3092 json-file \u304b\u3089 journald \u306b\u5909\u66f4\u3002 $ cat <<EOF > etc/docker/daemon.json { \"log-driver\": \"journald\" } EOF $ systemctl restart docker $ docker info $ journalctl -f ( snip ) 3\u6708 02 23 :19:52 ceph2 containerd [ 8412 ] : time = \"2021-03-02T23:19:52.094657155+09:00\" level = error msg = \"add cg to OOM monitor\" error = \"cgroups: memory cgroup not supported on this system\" ( snip ) cgroup memory /boot/cmdline.txt \u306b cgroup_memory=1 cgroup_enable=memory \u3092\u8ffd\u8a18 \u52b9\u679c\u306a\u3057\u3063\u307d\u3044 SELinux /etc/selinux/config OSD \u7528\u306e\u30c7\u30d0\u30a4\u30b9\u3067 bluestore/firestore \u3069\u3063\u3061\u4f7f\u3046\u304b\u5224\u5b9a\u3067\u304d\u3066\u3044\u306a\u3044\u3002 https://github.com/ceph/ceph/blob/v15.2.9/src/ceph_osd.cc#L276 https://github.com/ceph/ceph/blob/master/src/os/bluestore/BlueStore.cc#L5026 https://github.com/ceph/ceph/blob/master/src/os/bluestore/BlueStore.cc#L5093 $ FSID = <fsid> $ OSD = 2 $ cat <<EOF > /var/lib/ceph/${FSID}/osd.${OSD}/type bluestore EOF # \u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092 /var/lib/ceph/${FSID}/osd.${OSD}/unit.run \u306e\u4e2d\u306e osd.activate \u30b3\u30f3\u30c6\u30ca\u3067\u5b9f\u884c\u3059\u308b $ /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-1 $ /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-1 $ /usr/bin/ceph-bluestore-tool --cluster = ceph prime-osd-dir --dev /dev/ceph-959f5584-a212-4e2a-a0d0-71531ab24727/osd-block-40f0f67b-f973-41e2-8390-ed6116bbbd84 --path /var/lib/ceph/osd/ceph-1 --no-mon-config $ /usr/bin/ln -snf /dev/ceph-959f5584-a212-4e2a-a0d0-71531ab24727/osd-block-40f0f67b-f973-41e2-8390-ed6116bbbd84 /var/lib/ceph/osd/ceph-1/block $ /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-1/block $ /usr/bin/chown -R ceph:ceph /dev/mapper/ceph--959f5584--a212--4e2a--a0d0--71531ab24727-osd--block--40f0f67b--f973--41e2--8390--ed6116bbbd84 $ /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-1 ``` text 3\u6708 23 06 :56:38 ceph1 bash [ 12317 ] : debug 2021 -03-22T21:56:38.372+0000 7fae96f200 1 mon.ceph1@0 ( leader ) .osd e6 _set_new_cache_sizes cache_size:1020054731 inc_alloc: 369098752 full_alloc: 369098752 kv_alloc: 272629760 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-2 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-2 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/ceph-bluestore-tool --cluster = ceph prime-osd-dir --dev /dev/ceph-e3a34c4d-3fde-4e66-88a3-edc14c59262b/osd-block-0dbc31f0-333a-47cc-a0b5-53f85c92f2e7 --path /var/lib/ceph/osd/ceph-2 --no-mon-config 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/ln -snf /dev/ceph-e3a34c4d-3fde-4e66-88a3-edc14c59262b/osd-block-0dbc31f0-333a-47cc-a0b5-53f85c92f2e7 /var/lib/ceph/osd/ceph-2/block 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-2/block 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/chown -R ceph:ceph /dev/mapper/ceph--e3a34c4d--3fde--4e66--88a3--edc14c59262b-osd--block--0dbc31f0--333a--47cc--a0b5--53f85c92f2e7 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-2 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : --> ceph-volume lvm activate successful for osd ID: 2","title":"Ceph \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9"},{"location":"raspi-ceph/cephadm/#ceph","text":"Cephadm \u3092\u4f7f\u3063\u3066 Ceph \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u3066\u3044\u304d\u307e\u3059\u3002","title":"Ceph \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9"},{"location":"raspi-ceph/cephadm/#_1","text":"","title":"\u30af\u30e9\u30b9\u30bf\u69cb\u7bc9"},{"location":"raspi-ceph/cephadm/#_2","text":"Ceph \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9\u306b\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ yum install -y python3 gdisk $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine $ yum install -y yum-utils $ yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo $ yum install -y docker-ce docker-ce-cli containerd.io $ systemctl start docker $ docker run hello-world $ systemctl enable docker","title":"\u30d1\u30c3\u30b1\u30fc\u30b8\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"raspi-ceph/cephadm/#cephadm","text":"GitHub \u304b\u3089 cephadm \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm $ curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm $ chmod +x cephadm $ ./cephadm add-repo --release octopus Writing repo to /etc/yum.repos.d/ceph.repo... Enabling EPEL... # \u30a8\u30e9\u30fc\u51fa\u305f\u5834\u5408\u306f\u518d\u5b9f\u884c\u3059\u308c\u3070\u3044\u3051\u308b $ ./cephadm install # ceph cli $ cephadm install ceph-common","title":"Cephadm \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"raspi-ceph/cephadm/#bootstrap","text":"cephadm bootstrap \u30b3\u30de\u30f3\u30c9\u3067 Ceph \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002 $ cephadm bootstrap --mon-ip 192 .168.2.11 --skip-monitoring-stack --skip-monitoring-stack Prometheus \u306a\u3069\u306e\u76e3\u8996\u7528\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u8ffd\u52a0\u3057\u306a\u3044","title":"Bootstrap"},{"location":"raspi-ceph/cephadm/#_3","text":"\u69cb\u7bc9\u3057\u305f Ceph \u30af\u30e9\u30b9\u30bf\u306b\u30ce\u30fc\u30c9\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002 # \u73fe\u72b6\u306e\u78ba\u8a8d $ ceph orch host ls # Ceph \u516c\u958b\u9375\u306e\u767b\u9332 $ ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph2 $ ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph3 # \u30db\u30b9\u30c8\u8ffd\u52a0 $ ceph orch host add ceph2 $ ceph orch host add ceph3 # \u8ffd\u52a0\u5f8c\u306e\u78ba\u8a8d $ ceph orch host ls","title":"\u30db\u30b9\u30c8\u8ffd\u52a0"},{"location":"raspi-ceph/cephadm/#osd","text":"OSD \u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002 # \u30c7\u30d0\u30a4\u30b9\u306e\u72b6\u614b\u3092\u78ba\u8a8d $ ceph orch device ls --refresh Hostname Path Type Serial Size Health Ident Fault Available ceph1 /dev/sda hdd DB9876543214E 256G Unknown N/A N/A Yes ceph2 /dev/sda hdd DB9876543214E 256G Unknown N/A N/A Yes ceph3 /dev/sda hdd DB9876543214E 256G Unknown N/A N/A Yes # Available: Yes \u306e\u5168\u30c7\u30d0\u30a4\u30b9\u3092\u5bfe\u8c61\u306b OSD \u3092\u8ffd\u52a0 $ ceph orch apply osd --all-available-devices Scheduled osd.all-available-devices update... # OSD Tree \u3067\u72b6\u614b\u78ba\u8a8d $ ceph osd tree service_type : osd service_id : osd_spec_default placement : host_pattern : '*' data_devices : all : true encrypted : false objectstore : bluestore","title":"OSD \u767b\u9332"},{"location":"raspi-ceph/cephadm/#cephconf","text":"https://docs.ceph.com/en/latest/cephadm/operations/#etc-ceph-ceph-conf \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9\u5f8c\u306b\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u30b5\u30d6\u30ce\u30fc\u30c9\u306b\u914d\u5e03\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3002 ceph config set mgr mgr/cephadm/manage_etc_ceph_ceph_conf true","title":"ceph.conf \u306e\u914d\u5e03"},{"location":"raspi-ceph/cephadm/#radosgw","text":"https://docs.ceph.com/en/latest/cephadm/rgw/ $ radosgw-admin realm create --rgw-realm = myorg --default $ radosgw-admin zonegroup create --rgw-zonegroup = japan --endpoints http://ceph2:80 --rgw-realm = myorg --master --default $ radosgw-admin zone create --rgw-zonegroup = japan --rgw-zone = japan-east-1 --master --default $ ceph orch apply rgw myorg japan-east-1 --placement = \"2 ceph2 ceph3\" $ radosgw-admin period update --rgw-realm = myorg --commit","title":"RADOSGW"},{"location":"raspi-ceph/cephadm/#ha-radosgw","text":"$ cat <<EOF > /etc/sysctl.d/radosgw_ipv4.conf net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1 EOF $ sysctl -p /etc/sysctl.d/radosgw_ipv4.conf $ ceph orch apply -i radosgw.yaml","title":"HA-RADOSGW"},{"location":"raspi-ceph/cephadm/#_4","text":"$ radosgw-admin user create --uid = admin --display-name = admin --system $ radosgw-admin user info --uid = admin $ ceph dashboard set-rgw-api-user-id admin $ ceph dashboard set-rgw-api-access-key A6D1B1JL68472ZH972ZL $ ceph dashboard set-rgw-api-secret-key zb3RBRkLJKBLlzdz7hJNfY6VOY5ZK8vjF5EUabrs","title":"\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u753b\u9762\u6709\u52b9\u5316"},{"location":"raspi-ceph/cephadm/#_5","text":"Cephadm \u3067\u69cb\u7bc9\u3057\u305f\u30af\u30e9\u30b9\u30bf\u3068\u30c7\u30a3\u30b9\u30af\u306e\u521d\u671f\u5316\u3092\u304a\u3053\u306a\u3063\u3066\u3001\u30af\u30e9\u30b9\u30bf\u69cb\u7bc9\u3092\u3084\u308a\u76f4\u3057\u307e\u3059\u3002 # \u30b5\u30d6\u30ce\u30fc\u30c9\u3067\u5b9f\u884c $ systemctl stop docker.socket # \u30e1\u30a4\u30f3\u30ce\u30fc\u30c9\u3067\u5b9f\u884c $ cephadm rm-cluster --fsid <fsid> --force $ rm -i /etc/systemd/system/ceph.target # \u30b5\u30d6\u30ce\u30fc\u30c8\u3067\u5b9f\u884c $ rm -rf /etc/systemd/system/ceph* # \u5404\u30ce\u30fc\u30c9\u3067\u5b9f\u884c $ DISK = \"/dev/sda\" $ sgdisk --zap-all $DISK $ dd if = /dev/zero of = \" $DISK \" bs = 1M count = 100 oflag = direct,dsync $ ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % $ rm -rf /dev/ceph-*","title":"\u3084\u308a\u76f4\u3057\u624b\u9806"},{"location":"raspi-ceph/cephadm/#_6","text":"OSD \u30b3\u30f3\u30c6\u30ca\u304c activate \u2192 deactivate \u2192 activate \u306e\u7121\u9650\u30eb\u30fc\u30d7\u3067\u8d77\u52d5\u3057\u306a\u3044\u306e\u3067\u8abf\u67fb\u3057\u307e\u3059\u3002 docker \u306e\u30ed\u30ae\u30f3\u30b0\u30c9\u30e9\u30a4\u30d0\u30fc\u3092 json-file \u304b\u3089 journald \u306b\u5909\u66f4\u3002 $ cat <<EOF > etc/docker/daemon.json { \"log-driver\": \"journald\" } EOF $ systemctl restart docker $ docker info $ journalctl -f ( snip ) 3\u6708 02 23 :19:52 ceph2 containerd [ 8412 ] : time = \"2021-03-02T23:19:52.094657155+09:00\" level = error msg = \"add cg to OOM monitor\" error = \"cgroups: memory cgroup not supported on this system\" ( snip ) cgroup memory /boot/cmdline.txt \u306b cgroup_memory=1 cgroup_enable=memory \u3092\u8ffd\u8a18 \u52b9\u679c\u306a\u3057\u3063\u307d\u3044 SELinux /etc/selinux/config OSD \u7528\u306e\u30c7\u30d0\u30a4\u30b9\u3067 bluestore/firestore \u3069\u3063\u3061\u4f7f\u3046\u304b\u5224\u5b9a\u3067\u304d\u3066\u3044\u306a\u3044\u3002 https://github.com/ceph/ceph/blob/v15.2.9/src/ceph_osd.cc#L276 https://github.com/ceph/ceph/blob/master/src/os/bluestore/BlueStore.cc#L5026 https://github.com/ceph/ceph/blob/master/src/os/bluestore/BlueStore.cc#L5093 $ FSID = <fsid> $ OSD = 2 $ cat <<EOF > /var/lib/ceph/${FSID}/osd.${OSD}/type bluestore EOF # \u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092 /var/lib/ceph/${FSID}/osd.${OSD}/unit.run \u306e\u4e2d\u306e osd.activate \u30b3\u30f3\u30c6\u30ca\u3067\u5b9f\u884c\u3059\u308b $ /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-1 $ /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-1 $ /usr/bin/ceph-bluestore-tool --cluster = ceph prime-osd-dir --dev /dev/ceph-959f5584-a212-4e2a-a0d0-71531ab24727/osd-block-40f0f67b-f973-41e2-8390-ed6116bbbd84 --path /var/lib/ceph/osd/ceph-1 --no-mon-config $ /usr/bin/ln -snf /dev/ceph-959f5584-a212-4e2a-a0d0-71531ab24727/osd-block-40f0f67b-f973-41e2-8390-ed6116bbbd84 /var/lib/ceph/osd/ceph-1/block $ /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-1/block $ /usr/bin/chown -R ceph:ceph /dev/mapper/ceph--959f5584--a212--4e2a--a0d0--71531ab24727-osd--block--40f0f67b--f973--41e2--8390--ed6116bbbd84 $ /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-1 ``` text 3\u6708 23 06 :56:38 ceph1 bash [ 12317 ] : debug 2021 -03-22T21:56:38.372+0000 7fae96f200 1 mon.ceph1@0 ( leader ) .osd e6 _set_new_cache_sizes cache_size:1020054731 inc_alloc: 369098752 full_alloc: 369098752 kv_alloc: 272629760 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-2 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-2 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/ceph-bluestore-tool --cluster = ceph prime-osd-dir --dev /dev/ceph-e3a34c4d-3fde-4e66-88a3-edc14c59262b/osd-block-0dbc31f0-333a-47cc-a0b5-53f85c92f2e7 --path /var/lib/ceph/osd/ceph-2 --no-mon-config 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/ln -snf /dev/ceph-e3a34c4d-3fde-4e66-88a3-edc14c59262b/osd-block-0dbc31f0-333a-47cc-a0b5-53f85c92f2e7 /var/lib/ceph/osd/ceph-2/block 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-2/block 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/chown -R ceph:ceph /dev/mapper/ceph--e3a34c4d--3fde--4e66--88a3--edc14c59262b-osd--block--0dbc31f0--333a--47cc--a0b5--53f85c92f2e7 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-2 3\u6708 23 06 :56:38 ceph1 bash [ 23335 ] : --> ceph-volume lvm activate successful for osd ID: 2","title":"\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c8"},{"location":"raspi-ceph/initial-setup/","text":"\u30e9\u30ba\u30d1\u30a4\u521d\u671f\u8a2d\u5b9a Raspberry Pi 4 \u306b Ceph \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3059\u308b\u305f\u3081\u306e\u521d\u671f\u8a2d\u5b9a\u3092\u304a\u3053\u306a\u3044\u307e\u3059\u3002 OS \u30a4\u30e1\u30fc\u30b8 Raspberry Pi Imager \u3092\u4f7f\u3063\u3066\u3001\u30d6\u30fc\u30c8\u7528 MicroSD \u30ab\u30fc\u30c9\u306b CentOS 7 ARM64 \u306e OS \u30a4\u30e1\u30fc\u30b8\u3092\u713c\u304d\u307e\u3059\u3002 rootfs \u62e1\u5f35 df -h /usr/bin/rootfs-expand df -h \u30ed\u30b1\u30fc\u30eb $ localectl set-keymap jp106 $ timedatectl set-timezone Asia/Tokyo \u30cd\u30c3\u30c8\u30ef\u30fc\u30af $ nmcli d # $ nmcli dev wifi list # $ nmcli --ask dev wifi connect <SSID> $ nmcli connection modify \"Wired connection 1\" ipv4.method manual $ nmcli connection modify \"Wired connection 1\" ipv4.addresses 192 .168.1.21/24 $ nmcli connection modify \"Wired connection 1\" ipv4.gateway 192 .168.1.1 $ nmcli connection modify \"Wired connection 1\" ipv4.dns 192 .168.2.3 $ nmcli connection reload $ nmcli connection down \"Wired connection 1\" && nmcli con up \"Wired connection 1\" # IPv6 \u7121\u52b9\u5316 $ cat <<EOF > /etc/sysctl.d/disable_ipv6.conf net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 EOF $ sysctl -p \u30db\u30b9\u30c8\u540d \u30e9\u30ba\u30d1\u30a4\u6bce\u306b\u30db\u30b9\u30c8\u540d\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 $ hostnamectl set-hostname ceph1 \u30e6\u30fc\u30b6 \u30db\u30b9\u30c8 \u30e6\u30fc\u30b6 \u30d1\u30b9\u30ef\u30fc\u30c9 ceph1 root centos ceph2 root rockylinux ceph2 rocky rockylinux ceph3 root almalinux","title":"\u30e9\u30ba\u30d1\u30a4\u521d\u671f\u8a2d\u5b9a"},{"location":"raspi-ceph/initial-setup/#_1","text":"Raspberry Pi 4 \u306b Ceph \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3059\u308b\u305f\u3081\u306e\u521d\u671f\u8a2d\u5b9a\u3092\u304a\u3053\u306a\u3044\u307e\u3059\u3002","title":"\u30e9\u30ba\u30d1\u30a4\u521d\u671f\u8a2d\u5b9a"},{"location":"raspi-ceph/initial-setup/#os","text":"Raspberry Pi Imager \u3092\u4f7f\u3063\u3066\u3001\u30d6\u30fc\u30c8\u7528 MicroSD \u30ab\u30fc\u30c9\u306b CentOS 7 ARM64 \u306e OS \u30a4\u30e1\u30fc\u30b8\u3092\u713c\u304d\u307e\u3059\u3002","title":"OS \u30a4\u30e1\u30fc\u30b8"},{"location":"raspi-ceph/initial-setup/#rootfs","text":"df -h /usr/bin/rootfs-expand df -h","title":"rootfs \u62e1\u5f35"},{"location":"raspi-ceph/initial-setup/#_2","text":"$ localectl set-keymap jp106 $ timedatectl set-timezone Asia/Tokyo","title":"\u30ed\u30b1\u30fc\u30eb"},{"location":"raspi-ceph/initial-setup/#_3","text":"$ nmcli d # $ nmcli dev wifi list # $ nmcli --ask dev wifi connect <SSID> $ nmcli connection modify \"Wired connection 1\" ipv4.method manual $ nmcli connection modify \"Wired connection 1\" ipv4.addresses 192 .168.1.21/24 $ nmcli connection modify \"Wired connection 1\" ipv4.gateway 192 .168.1.1 $ nmcli connection modify \"Wired connection 1\" ipv4.dns 192 .168.2.3 $ nmcli connection reload $ nmcli connection down \"Wired connection 1\" && nmcli con up \"Wired connection 1\" # IPv6 \u7121\u52b9\u5316 $ cat <<EOF > /etc/sysctl.d/disable_ipv6.conf net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 EOF $ sysctl -p","title":"\u30cd\u30c3\u30c8\u30ef\u30fc\u30af"},{"location":"raspi-ceph/initial-setup/#_4","text":"\u30e9\u30ba\u30d1\u30a4\u6bce\u306b\u30db\u30b9\u30c8\u540d\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 $ hostnamectl set-hostname ceph1","title":"\u30db\u30b9\u30c8\u540d"},{"location":"raspi-ceph/initial-setup/#_5","text":"\u30db\u30b9\u30c8 \u30e6\u30fc\u30b6 \u30d1\u30b9\u30ef\u30fc\u30c9 ceph1 root centos ceph2 root rockylinux ceph2 rocky rockylinux ceph3 root almalinux","title":"\u30e6\u30fc\u30b6"}]}