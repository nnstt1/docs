{"config":{"lang":["en","ja"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Author","title":"Author"},{"location":"#author","text":"","title":"Author"},{"location":"mkdocs/","text":"MkDocs \u9759\u7684\u30b5\u30a4\u30c8\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf MkDocs \u306e\u4f7f\u3044\u65b9\u3067\u3059\u3002 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb pip install mkdocs mkdocs-material mkdocs-minify-plugin fontawesome-markdown \u30d3\u30eb\u30c9 \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u5165\u529b\u5143\u306f docs \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3001\u51fa\u529b\u5148\u306f site \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u3057\u3066\u30d3\u30eb\u30c9\u3055\u308c\u307e\u3059\u3002 \u3057\u304b\u3057\u3001GitHub Pages \u3067\u306f docs \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u53c2\u7167\u3057\u3066\u30da\u30fc\u30b8\u304c\u751f\u6210\u3055\u308c\u308b\u305f\u3081\u3001 mkdocs.yml \u3067\u5165\u529b\u5143\u3068\u51fa\u529b\u5148\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 # mkdocs.yml docs_dir : source site_dir : docs mkdocs build","title":"MkDocs"},{"location":"mkdocs/#mkdocs","text":"\u9759\u7684\u30b5\u30a4\u30c8\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf MkDocs \u306e\u4f7f\u3044\u65b9\u3067\u3059\u3002","title":"MkDocs"},{"location":"mkdocs/#_1","text":"pip install mkdocs mkdocs-material mkdocs-minify-plugin fontawesome-markdown","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"mkdocs/#_2","text":"\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u5165\u529b\u5143\u306f docs \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3001\u51fa\u529b\u5148\u306f site \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u3057\u3066\u30d3\u30eb\u30c9\u3055\u308c\u307e\u3059\u3002 \u3057\u304b\u3057\u3001GitHub Pages \u3067\u306f docs \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u53c2\u7167\u3057\u3066\u30da\u30fc\u30b8\u304c\u751f\u6210\u3055\u308c\u308b\u305f\u3081\u3001 mkdocs.yml \u3067\u5165\u529b\u5143\u3068\u51fa\u529b\u5148\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5909\u66f4\u3057\u307e\u3059\u3002 # mkdocs.yml docs_dir : source site_dir : docs mkdocs build","title":"\u30d3\u30eb\u30c9"},{"location":"ansible/","text":"","title":"Index"},{"location":"ansible/tower/","text":"Ansible Tower \u30e9\u30a4\u30bb\u30f3\u30b9 \u8a55\u4fa1\u7248 (Evaluation) Ansible Tower \u304c\u542b\u307e\u308c\u3066\u3044\u308b Red Hat Ansible Automation Platform \u306b\u306f 60 \u65e5\u9593\u5229\u7528\u3067\u304d\u308b\u8a55\u4fa1\u30e9\u30a4\u30bb\u30f3\u30b9\u304c\u3042\u308a\u307e\u3059\u3002 https://www.redhat.com/ja/technologies/management/ansible/try-it Developer Program Red Hat Developer Program \u306b\u767b\u9332\u3059\u308b\u3068\u5229\u7528\u3067\u304d\u308b\u958b\u767a\u7528\u30e9\u30a4\u30bb\u30f3\u30b9\u3067\u3059\u3002 https://developers.redhat.com/products/ansible/getting-started \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb OS \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5148\u306e OS \u306f RHEL 8.3 \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 Ansible Tower \u306e\u6b21\u671f\u30e1\u30b8\u30e3\u30fc\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u306f RHEL 7 \u3084 CentOS \u304c\u30b5\u30dd\u30fc\u30c8\u5bfe\u8c61\u5916\u3068\u306a\u308b\u305f\u3081\u3067\u3059\u3002 #/etc/os-release NAME = \"Red Hat Enterprise Linux\" VERSION = \"8.3 (Ootpa)\" ID = \"rhel\" ID_LIKE = \"fedora\" VERSION_ID = \"8.3\" PLATFORM_ID = \"platform:el8\" PRETTY_NAME = \"Red Hat Enterprise Linux 8.3 (Ootpa)\" ANSI_COLOR = \"0;31\" CPE_NAME = \"cpe:/o:redhat:enterprise_linux:8.3:GA\" HOME_URL = \"https://www.redhat.com/\" BUG_REPORT_URL = \"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT = \"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION = 8 .3 REDHAT_SUPPORT_PRODUCT = \"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION = \"8.3\" \u30a4\u30f3\u30d9\u30f3\u30c8\u30ea\u7de8\u96c6 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u524d\u306b inventory \u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3057\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u8a2d\u5b9a\u306f\u5fc5\u9808\u3067\u3059\u3002 admin_password pg_password \u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u5b9f\u884c ./setup.sh \u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 \u30b9\u30af\u30ea\u30d7\u30c8\u5185\u3067 ansible \u30b3\u30de\u30f3\u30c9\u7b49\u3082\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u307e\u3059\u3002 $ sudo ./setup.sh ( snip ) PLAY RECAP ****************************************************************************************************************** localhost : ok = 191 changed = 79 unreachable = 0 failed = 0 skipped = 86 rescued = 0 ignored = 2 The setup process completed successfully. Setup log saved to /var/log/tower/setup-2021-01-05-01:10:04.log. \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u5931\u6557\u3057\u305f\u5834\u5408 \u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5931\u6557\u3057\u305f\u5834\u5408\u3001en \u306e locale \u304c\u4e0d\u8db3\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 $ sudo ./setup.sh ( snip ) TASK [ restart postgresql when authentication settings changed ] ************************************************************* fatal: [ localhost ] : FAILED! = > { \"changed\" : false, \"msg\" : \"Unable to restart service postgresql: Job for postgresql.service failed because the control process exited with error code.\\nSee \\\"systemctl status postgresql.service\\\" and \\\"journalctl -xe\\\" for details.\\n\" } ( snip ) $ locale -a C C.utf8 POSIX ja_JP.eucjp ja_JP.utf8 \u3053\u306e\u3088\u3046\u306a\u5834\u5408\u306f\u3001 glibc-langpack-en \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ sudo yum install glibc-langpack-en \u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u767b\u9332 Ansible Tower \u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b\u3068\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u767b\u9332\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002 USERNAME \u3068 PASSWORD \u3092\u5165\u529b\u3057\u3066\u300cGET SUBSCRIPTIONS\u300d\u3092\u62bc\u4e0b\u3059\u308b\u3068\u3001\u30e9\u30a4\u30bb\u30f3\u30b9\u9078\u629e\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002","title":"Ansible Tower"},{"location":"ansible/tower/#ansible-tower","text":"","title":"Ansible Tower"},{"location":"ansible/tower/#_1","text":"","title":"\u30e9\u30a4\u30bb\u30f3\u30b9"},{"location":"ansible/tower/#evaluation","text":"Ansible Tower \u304c\u542b\u307e\u308c\u3066\u3044\u308b Red Hat Ansible Automation Platform \u306b\u306f 60 \u65e5\u9593\u5229\u7528\u3067\u304d\u308b\u8a55\u4fa1\u30e9\u30a4\u30bb\u30f3\u30b9\u304c\u3042\u308a\u307e\u3059\u3002 https://www.redhat.com/ja/technologies/management/ansible/try-it","title":"\u8a55\u4fa1\u7248 (Evaluation)"},{"location":"ansible/tower/#developer-program","text":"Red Hat Developer Program \u306b\u767b\u9332\u3059\u308b\u3068\u5229\u7528\u3067\u304d\u308b\u958b\u767a\u7528\u30e9\u30a4\u30bb\u30f3\u30b9\u3067\u3059\u3002 https://developers.redhat.com/products/ansible/getting-started","title":"Developer Program"},{"location":"ansible/tower/#_2","text":"","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"ansible/tower/#os","text":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5148\u306e OS \u306f RHEL 8.3 \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 Ansible Tower \u306e\u6b21\u671f\u30e1\u30b8\u30e3\u30fc\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u306f RHEL 7 \u3084 CentOS \u304c\u30b5\u30dd\u30fc\u30c8\u5bfe\u8c61\u5916\u3068\u306a\u308b\u305f\u3081\u3067\u3059\u3002 #/etc/os-release NAME = \"Red Hat Enterprise Linux\" VERSION = \"8.3 (Ootpa)\" ID = \"rhel\" ID_LIKE = \"fedora\" VERSION_ID = \"8.3\" PLATFORM_ID = \"platform:el8\" PRETTY_NAME = \"Red Hat Enterprise Linux 8.3 (Ootpa)\" ANSI_COLOR = \"0;31\" CPE_NAME = \"cpe:/o:redhat:enterprise_linux:8.3:GA\" HOME_URL = \"https://www.redhat.com/\" BUG_REPORT_URL = \"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT = \"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION = 8 .3 REDHAT_SUPPORT_PRODUCT = \"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION = \"8.3\"","title":"OS"},{"location":"ansible/tower/#_3","text":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u524d\u306b inventory \u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3057\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u8a2d\u5b9a\u306f\u5fc5\u9808\u3067\u3059\u3002 admin_password pg_password","title":"\u30a4\u30f3\u30d9\u30f3\u30c8\u30ea\u7de8\u96c6"},{"location":"ansible/tower/#_4","text":"./setup.sh \u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 \u30b9\u30af\u30ea\u30d7\u30c8\u5185\u3067 ansible \u30b3\u30de\u30f3\u30c9\u7b49\u3082\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u307e\u3059\u3002 $ sudo ./setup.sh ( snip ) PLAY RECAP ****************************************************************************************************************** localhost : ok = 191 changed = 79 unreachable = 0 failed = 0 skipped = 86 rescued = 0 ignored = 2 The setup process completed successfully. Setup log saved to /var/log/tower/setup-2021-01-05-01:10:04.log.","title":"\u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u5b9f\u884c"},{"location":"ansible/tower/#_5","text":"\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5931\u6557\u3057\u305f\u5834\u5408\u3001en \u306e locale \u304c\u4e0d\u8db3\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 $ sudo ./setup.sh ( snip ) TASK [ restart postgresql when authentication settings changed ] ************************************************************* fatal: [ localhost ] : FAILED! = > { \"changed\" : false, \"msg\" : \"Unable to restart service postgresql: Job for postgresql.service failed because the control process exited with error code.\\nSee \\\"systemctl status postgresql.service\\\" and \\\"journalctl -xe\\\" for details.\\n\" } ( snip ) $ locale -a C C.utf8 POSIX ja_JP.eucjp ja_JP.utf8 \u3053\u306e\u3088\u3046\u306a\u5834\u5408\u306f\u3001 glibc-langpack-en \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ sudo yum install glibc-langpack-en","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u5931\u6557\u3057\u305f\u5834\u5408"},{"location":"ansible/tower/#_6","text":"Ansible Tower \u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b\u3068\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u767b\u9332\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002 USERNAME \u3068 PASSWORD \u3092\u5165\u529b\u3057\u3066\u300cGET SUBSCRIPTIONS\u300d\u3092\u62bc\u4e0b\u3059\u308b\u3068\u3001\u30e9\u30a4\u30bb\u30f3\u30b9\u9078\u629e\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002","title":"\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u767b\u9332"},{"location":"kubernetes/argocd/","text":"ArgoCD https://argoproj.github.io/argo-cd/getting_started/ # ArgoCD kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml # CLI VERSION = $( curl --silent \"https://api.github.com/repos/argoproj/argo-cd/releases/latest\" | grep '\"tag_name\"' | sed -E 's/.*\"([^\"]+)\".*/\\1/' ) sudo curl -SL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/ $VERSION /argocd-linux-amd64 sudo chmod +x /usr/local/bin/argocd kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 argocd login 192 .168.2.243 Username: admin Password: 'admin' logged in successfully Context '192.168.2.243' updated argocd account update-password *** Enter current password: *** Enter new password: *** Confirm new password: Password updated Context '192.168.2.243' updated","title":"ArgoCD"},{"location":"kubernetes/argocd/#argocd","text":"https://argoproj.github.io/argo-cd/getting_started/ # ArgoCD kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml # CLI VERSION = $( curl --silent \"https://api.github.com/repos/argoproj/argo-cd/releases/latest\" | grep '\"tag_name\"' | sed -E 's/.*\"([^\"]+)\".*/\\1/' ) sudo curl -SL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/ $VERSION /argocd-linux-amd64 sudo chmod +x /usr/local/bin/argocd kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 argocd login 192 .168.2.243 Username: admin Password: 'admin' logged in successfully Context '192.168.2.243' updated argocd account update-password *** Enter current password: *** Enter new password: *** Confirm new password: Password updated Context '192.168.2.243' updated","title":"ArgoCD"},{"location":"kubernetes/container-runtime/","text":"\u30b3\u30f3\u30c6\u30ca\u30e9\u30f3\u30bf\u30a4\u30e0 containerd Kubernetes \u306e\u30b3\u30f3\u30c6\u30ca\u30e9\u30f3\u30bf\u30a4\u30e0\u3092 containerd \u306b\u5909\u66f4\u3059\u308b\u3002 kubectl drain <k8s-node> --ignore-daemonsets --delete-local-data # k8s-node yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo yum update -y && yum install -y containerd.io mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml systemctl restart containerd yum remove -y docker-ce cat <<EOF > /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS=\"--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock\" EOF systemctl restart kubelet kubectl uncordon <k8s-node>","title":"\u30b3\u30f3\u30c6\u30ca\u30e9\u30f3\u30bf\u30a4\u30e0"},{"location":"kubernetes/container-runtime/#_1","text":"","title":"\u30b3\u30f3\u30c6\u30ca\u30e9\u30f3\u30bf\u30a4\u30e0"},{"location":"kubernetes/container-runtime/#containerd","text":"Kubernetes \u306e\u30b3\u30f3\u30c6\u30ca\u30e9\u30f3\u30bf\u30a4\u30e0\u3092 containerd \u306b\u5909\u66f4\u3059\u308b\u3002 kubectl drain <k8s-node> --ignore-daemonsets --delete-local-data # k8s-node yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo yum update -y && yum install -y containerd.io mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml systemctl restart containerd yum remove -y docker-ce cat <<EOF > /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS=\"--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock\" EOF systemctl restart kubelet kubectl uncordon <k8s-node>","title":"containerd"},{"location":"kubernetes/istio/","text":"Istio curl -L https://istio.io/downloadIstio | sh - cd istio-1.7.0 export PATH = \" $PATH :/home/nnstt1/istio/istio-1.7.0/bin\" istioctl x precheck istioctl install --set profile = demo kubectl label namespace default istio-injection = enabled ## sample kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl exec \" $( kubectl get pod -l app = ratings -o jsonpath = '{.items[0].metadata.name}' ) \" -c ratings -- curl -s productpage:9080/productpage | grep -o \"<title>.*</title>\" kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml istioctl analyze","title":"Istio"},{"location":"kubernetes/istio/#istio","text":"curl -L https://istio.io/downloadIstio | sh - cd istio-1.7.0 export PATH = \" $PATH :/home/nnstt1/istio/istio-1.7.0/bin\" istioctl x precheck istioctl install --set profile = demo kubectl label namespace default istio-injection = enabled ## sample kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl exec \" $( kubectl get pod -l app = ratings -o jsonpath = '{.items[0].metadata.name}' ) \" -c ratings -- curl -s productpage:9080/productpage | grep -o \"<title>.*</title>\" kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml istioctl analyze","title":"Istio"},{"location":"kubernetes/maintenance/","text":"Maintenance worker \u30ce\u30fc\u30c9\u306e\u505c\u6b62 worker \u30ce\u30fc\u30c9\u3092\u505c\u6b62\u3059\u308b\u524d\u306b\u3001 kubectl drain <node name> \u3092\u4f7f\u7528\u3057\u3066\u5bfe\u8c61\u30ce\u30fc\u30c9\u3067\u52d5\u3044\u3066\u3044\u308b Pod \u3092\u4ed6\u306e\u30ce\u30fc\u30c9\u306b\u79fb\u52d5\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 # worker \u30ce\u30fc\u30c9\u306e\u78ba\u8a8d $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kubeadm-master1.nnstt1.work Ready master 58d v1.19.0 192 .168.2.20 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker1.nnstt1.work Ready <none> 58d v1.19.0 192 .168.2.22 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker2.nnstt1.work Ready <none> 58d v1.19.0 192 .168.2.23 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker3.nnstt1.work Ready <none> 57d v1.19.0 192 .168.2.24 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 # drain $ kubectl drain <node name> --delete-local-data --ignore-daemonsets --delete-local-data \u5bfe\u8c61\u30ce\u30fc\u30c9\u3067\u52d5\u3044\u3066\u3044\u308b Pod \u304c local storage \u3092\u4f7f\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3067 drain \u304c\u5931\u6557\u3059\u308b\u3002 cannot delete Pods with local storage (use --delete-local-data to override): arc/logsdb-0, (\u4ee5\u4e0b Pod \u540d) \u5f53\u8a72\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3064\u3051\u308b\u3053\u3068\u3067\u3001local storage \u306e\u30c7\u30fc\u30bf\u3082\u542b\u3081\u3066 Pod \u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308b\u3002 Attention local storage \u306b\u6c38\u7d9a\u30c7\u30fc\u30bf\u3092\u6b8b\u3059\u3053\u3068\u306f\u904b\u7528\u7684\u306b\u4e0d\u9069\u5207\u306a\u306e\u3067\u3001Storage Class \u3092\u4f7f\u7528\u3059\u308b\u3088\u3046\u306b\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4fee\u6b63\u3057\u305f\u307b\u3046\u304c\u3088\u3044 --ignore-daemonsets daemonsets \u30ea\u30bd\u30fc\u30b9\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u30a8\u30e9\u30fc\u3068\u306a\u308b\u3002 error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): arc/metricsdc-rmv74, (\u4ee5\u4e0b Pod \u540d) \u5f53\u8a72\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3064\u3051\u308b\u3053\u3068\u3067\u3001daemonsets \u306e Pod \u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308b\u3002 kubectl drain \u304c\u6210\u529f\u3059\u308c\u3070\u30ce\u30fc\u30c9\u3092\u518d\u8d77\u52d5\u3057\u3066\u3082\u5b89\u5168\u306a\u72b6\u614b\u3068\u306a\u308b\u3002 \u30ce\u30fc\u30c9\u518d\u8d77\u52d5\u5f8c\u306f Pod \u304c\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u306a\u3044\u72b6\u614b\u3068\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3001 kubectl uncordon <node name> \u3067\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3092\u6709\u52b9\u5316\u3059\u308b\u3002 $ kubectl uncordon kubeadm-worker1.nnstt1.work node/kubeadm-worker1.nnstt1.work uncordoned","title":"Maintenance"},{"location":"kubernetes/maintenance/#maintenance","text":"","title":"Maintenance"},{"location":"kubernetes/maintenance/#worker","text":"worker \u30ce\u30fc\u30c9\u3092\u505c\u6b62\u3059\u308b\u524d\u306b\u3001 kubectl drain <node name> \u3092\u4f7f\u7528\u3057\u3066\u5bfe\u8c61\u30ce\u30fc\u30c9\u3067\u52d5\u3044\u3066\u3044\u308b Pod \u3092\u4ed6\u306e\u30ce\u30fc\u30c9\u306b\u79fb\u52d5\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 # worker \u30ce\u30fc\u30c9\u306e\u78ba\u8a8d $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kubeadm-master1.nnstt1.work Ready master 58d v1.19.0 192 .168.2.20 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker1.nnstt1.work Ready <none> 58d v1.19.0 192 .168.2.22 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker2.nnstt1.work Ready <none> 58d v1.19.0 192 .168.2.23 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 kubeadm-worker3.nnstt1.work Ready <none> 57d v1.19.0 192 .168.2.24 <none> CentOS Linux 7 ( Core ) 3 .10.0-1127.19.1.el7.x86_64 docker://19.3.12 # drain $ kubectl drain <node name> --delete-local-data --ignore-daemonsets --delete-local-data \u5bfe\u8c61\u30ce\u30fc\u30c9\u3067\u52d5\u3044\u3066\u3044\u308b Pod \u304c local storage \u3092\u4f7f\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3067 drain \u304c\u5931\u6557\u3059\u308b\u3002 cannot delete Pods with local storage (use --delete-local-data to override): arc/logsdb-0, (\u4ee5\u4e0b Pod \u540d) \u5f53\u8a72\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3064\u3051\u308b\u3053\u3068\u3067\u3001local storage \u306e\u30c7\u30fc\u30bf\u3082\u542b\u3081\u3066 Pod \u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308b\u3002 Attention local storage \u306b\u6c38\u7d9a\u30c7\u30fc\u30bf\u3092\u6b8b\u3059\u3053\u3068\u306f\u904b\u7528\u7684\u306b\u4e0d\u9069\u5207\u306a\u306e\u3067\u3001Storage Class \u3092\u4f7f\u7528\u3059\u308b\u3088\u3046\u306b\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4fee\u6b63\u3057\u305f\u307b\u3046\u304c\u3088\u3044 --ignore-daemonsets daemonsets \u30ea\u30bd\u30fc\u30b9\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u30a8\u30e9\u30fc\u3068\u306a\u308b\u3002 error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): arc/metricsdc-rmv74, (\u4ee5\u4e0b Pod \u540d) \u5f53\u8a72\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3064\u3051\u308b\u3053\u3068\u3067\u3001daemonsets \u306e Pod \u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308b\u3002 kubectl drain \u304c\u6210\u529f\u3059\u308c\u3070\u30ce\u30fc\u30c9\u3092\u518d\u8d77\u52d5\u3057\u3066\u3082\u5b89\u5168\u306a\u72b6\u614b\u3068\u306a\u308b\u3002 \u30ce\u30fc\u30c9\u518d\u8d77\u52d5\u5f8c\u306f Pod \u304c\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u306a\u3044\u72b6\u614b\u3068\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3001 kubectl uncordon <node name> \u3067\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3092\u6709\u52b9\u5316\u3059\u308b\u3002 $ kubectl uncordon kubeadm-worker1.nnstt1.work node/kubeadm-worker1.nnstt1.work uncordoned","title":"worker \u30ce\u30fc\u30c9\u306e\u505c\u6b62"},{"location":"kubernetes/metallb/","text":"MetalLB kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml kubectl create secret generic -n metallb-system memberlist --from-literal = secretkey = \" $( openssl rand -base64 128 ) \" kubectl apply -f layer2-config.yaml # layer2-config.yaml apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools: - name: default protocol: layer2 addresses: - 192.168.2.240/28 # MetalLB \u304c\u6255\u3044\u51fa\u3059\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a2\u30c9\u30ec\u30b9","title":"MetalLB"},{"location":"kubernetes/metallb/#metallb","text":"kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml kubectl create secret generic -n metallb-system memberlist --from-literal = secretkey = \" $( openssl rand -base64 128 ) \" kubectl apply -f layer2-config.yaml # layer2-config.yaml apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools: - name: default protocol: layer2 addresses: - 192.168.2.240/28 # MetalLB \u304c\u6255\u3044\u51fa\u3059\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a2\u30c9\u30ec\u30b9","title":"MetalLB"},{"location":"kubernetes/velero/","text":"Velero Velero \u306f\u3001Kubernetes \u306e\u30ea\u30bd\u30fc\u30b9\u3084 PV (Persistent Volume) \u306e\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30fb\u30ea\u30b9\u30c8\u30a2\u3092\u3059\u308b\u305f\u3081\u306e OSS \u3067\u3059\u3002 https://velero.io/ Velero \u306f\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u306b\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3057\u307e\u3059\u3002 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u7528\u610f\u3059\u308b\u65b9\u6cd5\u306f\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3059\u3002 MinIO \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092 MinIO \u3067\u7528\u610f\u3057\u307e\u3059\u3002 https://velero.io/docs/v1.5/contributions/minio/ Velero \u30c7\u30d7\u30ed\u30a4 velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.0.0 \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-volume-snapshots = false \\ --backup-location-config region = minio,s3ForcePathStyle = \"true\" ,s3Url = http://minio.minio.svc:9000 Velero & Rook Ceph Velero \u304c\u4f7f\u7528\u3059\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092 Rook Ceph \u3092\u4f7f\u3063\u3066\u7528\u610f\u3057\u307e\u3059\u3002 Rook Ceph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8 Rook \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\u3067\u3042\u308c\u3070\u3001\u30b5\u30f3\u30d7\u30eb\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u3060\u3051\u3002 kubectl apply -f rook/cluster/examples/kubernetes/ceph/object-test.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml export AWS_HOST = $( kubectl get cm ceph-delete-bucket -o jsonpath = '{.data.BUCKET_HOST}' ) export AWS_ACCESS_KEY_ID = $( kubectl get secret ceph-delete-bucket -o jsonpath = '{.data.AWS_ACCESS_KEY_ID}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl get secret ceph-delete-bucket -o jsonpath = '{.data.AWS_SECRET_ACCESS_KEY}' | base64 --decode ) rook-ceph-tools Pod \u3092\u4f7f\u3063\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u304c\u4f7f\u3048\u308b\u304b\u78ba\u8a8d\u3092\u3057\u307e\u3059\u3002 kubectl exec -it $( kubectl get pods -l app = rook-ceph-tools -o = jsonpath ={ .items [ * ] .metadata.name } ) -- bash export AWS_HOST = <host> export AWS_ENDPOINT = <endpoint> export AWS_ACCESS_KEY_ID = <accessKey> export AWS_SECRET_ACCESS_KEY = <secretKey> echo \"Hello Rook\" > /tmp/rookObj # Upload # <bucket-name> \u306f ObjectBucketClame \u304b\u3089\u53c2\u7167\u53ef\u80fd s3cmd put /tmp/rookObj --no-ssl --host = ${ AWS_HOST } --host-bucket = s3://<bucket-name> # Download s3cmd get s3://<bucket-name>/rookObj /tmp/rookObj-download --no-ssl --host = ${ AWS_HOST } --host-bucket = Velero \u30c7\u30d7\u30ed\u30a4 velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.0.0 \\ --bucket ceph-bkt-c71df44b-8657-4f02-b680-fe0894debc07 \\ --secret-file ./credentials-velero \\ --use-volume-snapshots = false \\ --backup-location-config region = us-east-1,s3ForcePathStyle = \"true\" ,s3Url = http://rook-ceph-rgw-my-store.rook-ceph.svc \u691c\u8a3c # \u30c6\u30b9\u30c8\u7528\u30ea\u30bd\u30fc\u30b9\u306e\u6e96\u5099 kubectl apply -f nginx-base.yaml kubectl get deployments -l component = velero --namespace = velero kubectl get deployments --namespace = nginx-example # \u30d0\u30c3\u30af\u30a2\u30c3\u30d7 velero backup create nginx-backup2 --selector app = nginx # \u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30ea\u30bd\u30fc\u30b9\u306e\u78ba\u8a8d velero backup get velero backup describe nginx-backup velero backup logs nginx-backup # \u30c6\u30b9\u30c8\u7528\u30ea\u30bd\u30fc\u30b9\u306e\u524a\u9664 kubectl delete namespace nginx-example kubectl get deployments --namespace = nginx-example # \u30ea\u30b9\u30c8\u30a2 velero restore create --from-backup nginx-backup2 # \u30ea\u30b9\u30c8\u30a2\u30ea\u30bd\u30fc\u30b9\u306e\u78ba\u8a8d velero restore get kubectl get deployments --namespace = nginx-example \u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://velero.io/docs/v1.5/uninstalling/ kubectl delete namespace/velero clusterrolebinding/velero kubectl delete crds -l component = velero","title":"Velero"},{"location":"kubernetes/velero/#velero","text":"Velero \u306f\u3001Kubernetes \u306e\u30ea\u30bd\u30fc\u30b9\u3084 PV (Persistent Volume) \u306e\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30fb\u30ea\u30b9\u30c8\u30a2\u3092\u3059\u308b\u305f\u3081\u306e OSS \u3067\u3059\u3002 https://velero.io/ Velero \u306f\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u306b\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3057\u307e\u3059\u3002 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u7528\u610f\u3059\u308b\u65b9\u6cd5\u306f\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3059\u3002","title":"Velero"},{"location":"kubernetes/velero/#minio","text":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092 MinIO \u3067\u7528\u610f\u3057\u307e\u3059\u3002 https://velero.io/docs/v1.5/contributions/minio/","title":"MinIO"},{"location":"kubernetes/velero/#velero_1","text":"velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.0.0 \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-volume-snapshots = false \\ --backup-location-config region = minio,s3ForcePathStyle = \"true\" ,s3Url = http://minio.minio.svc:9000","title":"Velero \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/velero/#velero-rook-ceph","text":"Velero \u304c\u4f7f\u7528\u3059\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u3092 Rook Ceph \u3092\u4f7f\u3063\u3066\u7528\u610f\u3057\u307e\u3059\u3002","title":"Velero &amp; Rook Ceph"},{"location":"kubernetes/velero/#rook-ceph","text":"Rook \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\u3067\u3042\u308c\u3070\u3001\u30b5\u30f3\u30d7\u30eb\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u3060\u3051\u3002 kubectl apply -f rook/cluster/examples/kubernetes/ceph/object-test.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml export AWS_HOST = $( kubectl get cm ceph-delete-bucket -o jsonpath = '{.data.BUCKET_HOST}' ) export AWS_ACCESS_KEY_ID = $( kubectl get secret ceph-delete-bucket -o jsonpath = '{.data.AWS_ACCESS_KEY_ID}' | base64 --decode ) export AWS_SECRET_ACCESS_KEY = $( kubectl get secret ceph-delete-bucket -o jsonpath = '{.data.AWS_SECRET_ACCESS_KEY}' | base64 --decode ) rook-ceph-tools Pod \u3092\u4f7f\u3063\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u304c\u4f7f\u3048\u308b\u304b\u78ba\u8a8d\u3092\u3057\u307e\u3059\u3002 kubectl exec -it $( kubectl get pods -l app = rook-ceph-tools -o = jsonpath ={ .items [ * ] .metadata.name } ) -- bash export AWS_HOST = <host> export AWS_ENDPOINT = <endpoint> export AWS_ACCESS_KEY_ID = <accessKey> export AWS_SECRET_ACCESS_KEY = <secretKey> echo \"Hello Rook\" > /tmp/rookObj # Upload # <bucket-name> \u306f ObjectBucketClame \u304b\u3089\u53c2\u7167\u53ef\u80fd s3cmd put /tmp/rookObj --no-ssl --host = ${ AWS_HOST } --host-bucket = s3://<bucket-name> # Download s3cmd get s3://<bucket-name>/rookObj /tmp/rookObj-download --no-ssl --host = ${ AWS_HOST } --host-bucket =","title":"Rook Ceph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8"},{"location":"kubernetes/velero/#velero_2","text":"velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.0.0 \\ --bucket ceph-bkt-c71df44b-8657-4f02-b680-fe0894debc07 \\ --secret-file ./credentials-velero \\ --use-volume-snapshots = false \\ --backup-location-config region = us-east-1,s3ForcePathStyle = \"true\" ,s3Url = http://rook-ceph-rgw-my-store.rook-ceph.svc","title":"Velero \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/velero/#_1","text":"# \u30c6\u30b9\u30c8\u7528\u30ea\u30bd\u30fc\u30b9\u306e\u6e96\u5099 kubectl apply -f nginx-base.yaml kubectl get deployments -l component = velero --namespace = velero kubectl get deployments --namespace = nginx-example # \u30d0\u30c3\u30af\u30a2\u30c3\u30d7 velero backup create nginx-backup2 --selector app = nginx # \u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30ea\u30bd\u30fc\u30b9\u306e\u78ba\u8a8d velero backup get velero backup describe nginx-backup velero backup logs nginx-backup # \u30c6\u30b9\u30c8\u7528\u30ea\u30bd\u30fc\u30b9\u306e\u524a\u9664 kubectl delete namespace nginx-example kubectl get deployments --namespace = nginx-example # \u30ea\u30b9\u30c8\u30a2 velero restore create --from-backup nginx-backup2 # \u30ea\u30b9\u30c8\u30a2\u30ea\u30bd\u30fc\u30b9\u306e\u78ba\u8a8d velero restore get kubectl get deployments --namespace = nginx-example","title":"\u691c\u8a3c"},{"location":"kubernetes/velero/#_2","text":"https://velero.io/docs/v1.5/uninstalling/ kubectl delete namespace/velero clusterrolebinding/velero kubectl delete crds -l component = velero","title":"\u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"kubernetes/operator/rook/","text":"Rook Rook \u306f Kubernetes \u306b\u5404\u7a2e\u30b9\u30c8\u30ec\u30fc\u30b8\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3 (Ceph, Cassandra, etc...) \u3092\u63d0\u4f9b\u3057\u3066\u304f\u308c\u308b\u30b9\u30c8\u30ec\u30fc\u30b8\u30aa\u30fc\u30b1\u30b9\u30c8\u30ec\u30fc\u30bf\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001Rook \u3092\u4f7f\u3063\u3066 Ceph \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u3001K8s \u30af\u30e9\u30b9\u30bf\u306b\u30d6\u30ed\u30c3\u30af\u30b9\u30c8\u30ec\u30fc\u30b8\u3068\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u306e StorageClass \u3092\u63d0\u4f9b\u3059\u308b\u624b\u9806\u3092\u8aac\u660e\u3057\u307e\u3059\u3002 Install \u6700\u65b0\u7248\u306e Rook \u3092\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089 clone \u3057\u3066\u3001Rook \u3092 K8s \u30af\u30e9\u30b9\u30bf\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ git clone --single-branch --branch release-1.5 https://github.com/rook/rook.git $ cd rook/cluster/examples/kubernetes/ $ kubectl apply -f rook/cluster/examples/kubernetes/crds.yaml $ kubectl apply -f rook/cluster/examples/kubernetes/common.yaml $ kubectl apply -f rook/cluster/examples/kubernetes/operator.yaml Block Storage $ kubectl apply -f rook/cluster/examples/kubernetes/cluster.yaml $ kubectl apply -f rook/cluster/examples/kubernetes/storageclass.yaml # ceph-tools \u30b3\u30f3\u30c6\u30ca\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3066 Ceph \u306e\u69cb\u7bc9\u72b6\u6cc1\u3092\u78ba\u8a8d $ kubectl apply -f toolbox.yaml $ kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) -- bash $ ceph status \u69cb\u7bc9\u5b8c\u4e86\u5f8c\u306b\u306f StorageClass \u304c\u5229\u7528\u53ef\u80fd\u3068\u306a\u308b\u3002 \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306e PersistentVolume \u3067\u4ee5\u4e0b\u306e StorageClass \u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001Ceph Block Storage \u3092\u5229\u7528\u3067\u304d\u308b\u3002 kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate true 57d Object Storage Dashboard Deploy kubectl apply -f rook/cluster/examples/kubernetes/ceph/dashboard-loadbalancer.yaml Login # \u30d1\u30b9\u30ef\u30fc\u30c9\u78ba\u8a8d kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath = \"{['data']['password']}\" | base64 --decode && echo https://192.168.2.244:8443 \u7ba1\u7406\u8005\u30e6\u30fc\u30b6\u306f admin Cleanup Rook \u3092\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7\u3059\u308b\u5834\u5408\u306e\u624b\u9806\u3067\u3059\u3002 \u5404 Worker Node \u3067\u5b9f\u65bd\u3057\u307e\u3059\u3002 Rook \u95a2\u9023\u30c7\u30fc\u30bf\u524a\u9664 rm -rf /var/lib/rook \u30d6\u30ed\u30c3\u30af\u30c7\u30d0\u30a4\u30b9\u521d\u671f\u5316 \u521d\u671f\u5316\u3059\u308b\u30d6\u30ed\u30c3\u30af\u30c7\u30d0\u30a4\u30b9\u6bce\u306b\u4e0b\u8a18\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 #!/usr/bin/env bash # https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md DISK = \"/dev/sdb\" # Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean) # You will have to run this step for all disks. sgdisk --zap-all $DISK # Clean hdds with dd dd if = /dev/zero of = \" $DISK \" bs = 1M count = 100 oflag = direct,dsync # Clean disks such as ssd with blkdiscard instead of dd blkdiscard $DISK # These steps only have to be run once on each node # If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks. ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % # ceph-volume setup can leave ceph-<UUID> directories in /dev (unnecessary clutter) rm -rf /dev/ceph-* Kubernetes v1.20 \u3067\u52d5\u304b\u306a\u3044\u5834\u5408 Kubernetes v1.20 \u304b\u3089 Feature Gate CSIVolumeFSGroupPolicy \u304c Beta \u306b\u6607\u683c\u3057\u3066\u304a\u308a\u3001\u6a5f\u80fd\u304c\u6709\u52b9\u72b6\u614b\u3068\u306a\u308a\u307e\u3057\u305f\u3002 \u305d\u306e\u5f71\u97ff\u3067 Rook \u304c\u52d5\u4f5c\u3057\u306a\u3044\u5834\u5408\u304c\u3042\u308a\u3001\u66ab\u5b9a\u5bfe\u51e6\u3068\u3057\u3066\u65e7\u30d0\u30fc\u30b8\u30e7\u30f3\u3068\u540c\u3058\u72b6\u614b\u306b\u3059\u308b --feature-gates=CSIVolumeFSGroupPolicy=false \u3092 kubelet \u306b\u8a2d\u5b9a\u3057\u307e\u3057\u305f\u3002 # /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS = \"--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --feature-gates=CSIVolumeFSGroupPolicy=false\"","title":"Rook"},{"location":"kubernetes/operator/rook/#rook","text":"Rook \u306f Kubernetes \u306b\u5404\u7a2e\u30b9\u30c8\u30ec\u30fc\u30b8\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3 (Ceph, Cassandra, etc...) \u3092\u63d0\u4f9b\u3057\u3066\u304f\u308c\u308b\u30b9\u30c8\u30ec\u30fc\u30b8\u30aa\u30fc\u30b1\u30b9\u30c8\u30ec\u30fc\u30bf\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001Rook \u3092\u4f7f\u3063\u3066 Ceph \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u3001K8s \u30af\u30e9\u30b9\u30bf\u306b\u30d6\u30ed\u30c3\u30af\u30b9\u30c8\u30ec\u30fc\u30b8\u3068\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u306e StorageClass \u3092\u63d0\u4f9b\u3059\u308b\u624b\u9806\u3092\u8aac\u660e\u3057\u307e\u3059\u3002","title":"Rook"},{"location":"kubernetes/operator/rook/#install","text":"\u6700\u65b0\u7248\u306e Rook \u3092\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089 clone \u3057\u3066\u3001Rook \u3092 K8s \u30af\u30e9\u30b9\u30bf\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 $ git clone --single-branch --branch release-1.5 https://github.com/rook/rook.git $ cd rook/cluster/examples/kubernetes/ $ kubectl apply -f rook/cluster/examples/kubernetes/crds.yaml $ kubectl apply -f rook/cluster/examples/kubernetes/common.yaml $ kubectl apply -f rook/cluster/examples/kubernetes/operator.yaml","title":"Install"},{"location":"kubernetes/operator/rook/#block-storage","text":"$ kubectl apply -f rook/cluster/examples/kubernetes/cluster.yaml $ kubectl apply -f rook/cluster/examples/kubernetes/storageclass.yaml # ceph-tools \u30b3\u30f3\u30c6\u30ca\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3066 Ceph \u306e\u69cb\u7bc9\u72b6\u6cc1\u3092\u78ba\u8a8d $ kubectl apply -f toolbox.yaml $ kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) -- bash $ ceph status \u69cb\u7bc9\u5b8c\u4e86\u5f8c\u306b\u306f StorageClass \u304c\u5229\u7528\u53ef\u80fd\u3068\u306a\u308b\u3002 \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306e PersistentVolume \u3067\u4ee5\u4e0b\u306e StorageClass \u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001Ceph Block Storage \u3092\u5229\u7528\u3067\u304d\u308b\u3002 kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate true 57d","title":"Block Storage"},{"location":"kubernetes/operator/rook/#object-storage","text":"","title":"Object Storage"},{"location":"kubernetes/operator/rook/#dashboard","text":"","title":"Dashboard"},{"location":"kubernetes/operator/rook/#deploy","text":"kubectl apply -f rook/cluster/examples/kubernetes/ceph/dashboard-loadbalancer.yaml","title":"Deploy"},{"location":"kubernetes/operator/rook/#login","text":"# \u30d1\u30b9\u30ef\u30fc\u30c9\u78ba\u8a8d kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath = \"{['data']['password']}\" | base64 --decode && echo https://192.168.2.244:8443 \u7ba1\u7406\u8005\u30e6\u30fc\u30b6\u306f admin","title":"Login"},{"location":"kubernetes/operator/rook/#cleanup","text":"Rook \u3092\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7\u3059\u308b\u5834\u5408\u306e\u624b\u9806\u3067\u3059\u3002 \u5404 Worker Node \u3067\u5b9f\u65bd\u3057\u307e\u3059\u3002","title":"Cleanup"},{"location":"kubernetes/operator/rook/#rook_1","text":"rm -rf /var/lib/rook","title":"Rook \u95a2\u9023\u30c7\u30fc\u30bf\u524a\u9664"},{"location":"kubernetes/operator/rook/#_1","text":"\u521d\u671f\u5316\u3059\u308b\u30d6\u30ed\u30c3\u30af\u30c7\u30d0\u30a4\u30b9\u6bce\u306b\u4e0b\u8a18\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 #!/usr/bin/env bash # https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md DISK = \"/dev/sdb\" # Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean) # You will have to run this step for all disks. sgdisk --zap-all $DISK # Clean hdds with dd dd if = /dev/zero of = \" $DISK \" bs = 1M count = 100 oflag = direct,dsync # Clean disks such as ssd with blkdiscard instead of dd blkdiscard $DISK # These steps only have to be run once on each node # If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks. ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % # ceph-volume setup can leave ceph-<UUID> directories in /dev (unnecessary clutter) rm -rf /dev/ceph-*","title":"\u30d6\u30ed\u30c3\u30af\u30c7\u30d0\u30a4\u30b9\u521d\u671f\u5316"},{"location":"kubernetes/operator/rook/#kubernetes-v120","text":"Kubernetes v1.20 \u304b\u3089 Feature Gate CSIVolumeFSGroupPolicy \u304c Beta \u306b\u6607\u683c\u3057\u3066\u304a\u308a\u3001\u6a5f\u80fd\u304c\u6709\u52b9\u72b6\u614b\u3068\u306a\u308a\u307e\u3057\u305f\u3002 \u305d\u306e\u5f71\u97ff\u3067 Rook \u304c\u52d5\u4f5c\u3057\u306a\u3044\u5834\u5408\u304c\u3042\u308a\u3001\u66ab\u5b9a\u5bfe\u51e6\u3068\u3057\u3066\u65e7\u30d0\u30fc\u30b8\u30e7\u30f3\u3068\u540c\u3058\u72b6\u614b\u306b\u3059\u308b --feature-gates=CSIVolumeFSGroupPolicy=false \u3092 kubelet \u306b\u8a2d\u5b9a\u3057\u307e\u3057\u305f\u3002 # /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS = \"--network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --feature-gates=CSIVolumeFSGroupPolicy=false\"","title":"Kubernetes v1.20 \u3067\u52d5\u304b\u306a\u3044\u5834\u5408"},{"location":"kubernetes/operator/zalando-postgres-operator/","text":"Zalando Postgres Operator Zalando \u793e\u304c\u958b\u767a\u3057\u3066\u3044\u308b Postgres Operator \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u624b\u9806\u3067\u3059\u3002 Operator \u30c7\u30d7\u30ed\u30a4 \u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 3\u7a2e\u985e\u306e\u30c7\u30d7\u30ed\u30a4\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002 \u30de\u30cb\u30e5\u30a2\u30eb Kustomize Helm chart \u30de\u30cb\u30e5\u30a2\u30eb git clone https://github.com/zalando/postgres-operator.git cd postgres-operator/ kubectl create namespace zalando-postgres config set-context $( kubectl config current-context ) --namespace = zalando-postgres kubectl apply -f manifests/configmap.yaml kubectl apply -f manifests/operator-service-account-rbac.yaml # namespace: defualt \u306e\u3082\u306e\u304c\u3042\u308b\u306e\u3067\u6ce8\u610f kubectl apply -f manifests/postgres-operator.yaml kubectl apply -f manifests/api-service.yaml Web UI \u30c7\u30d7\u30ed\u30a4 Web UI \u304b\u3089 PostgreSQL \u30af\u30e9\u30b9\u30bf\u3092\u4f5c\u6210\u30fb\u7ba1\u7406\u3067\u304d\u307e\u3059\u3002 \u30de\u30cb\u30e5\u30a2\u30eb Web UI \u7528\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4f7f\u3063\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 kubectl apply -f ui/manifests/ Web UI \u306f\u4efb\u610f\u306e namespace \u3092\u7ba1\u7406\u5bfe\u8c61\u3068\u3057\u307e\u3059\u3002 deployment.yaml \u306e TARGET_NAMESPACE \u306b \"*\" \u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u5168 namespace \u3092\u5bfe\u8c61\u3068\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 # deployment.yaml env : - name : \"TARGET_NAMESPACE\" value : \"default\" # \"*\" \u306b\u5909\u66f4 Attention kustomize \u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u30c7\u30d7\u30ed\u30a4\u3067\u304d\u3066\u307e\u305b\u3093\u3002 error: unable to recognize \"manifests/kustomization.yaml\" : no matches for kind \"Kustomization\" in version \"kustomize.config.k8s.io/v1beta1\" Note \u81ea\u5b85\u30e9\u30dc\u3067\u306f Ingress \u3067\u306f\u306a\u304f type: Loadbalancer \u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002 PostgreSQL \u30c7\u30d7\u30ed\u30a4 Postgres Operator \u3092\u4f7f\u3063\u3066 PostgreSQL \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u65b9\u6cd5\u306f2\u7a2e\u985e\u3042\u308a\u307e\u3059\u3002 \u30de\u30cb\u30d5\u30a7\u30b9\u30c8 Web UI \u30de\u30cb\u30d5\u30a7\u30b9\u30c8 \u516c\u5f0f\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30b5\u30f3\u30d7\u30eb\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u304c\u3042\u308b\u306e\u3067\u3001\u53c2\u7167\u3057\u306a\u304c\u3089 Postgres \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u7528\u610f\u3057\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 # example apiVersion : acid.zalan.do/v1 kind : postgresql metadata : name : acid-example namespace : default labels : team : acid spec : teamId : acid postgresql : version : \"13\" # note: String numberOfInstances : 1 volume : size : 1Gi storageClass : rook-ceph-block users : example : [] databases : example : example allowedSourceRanges : resources : requests : cpu : 100m memory : 100Mi limits : cpu : 500m memory : 500Mi Web UI \u30c7\u30d7\u30ed\u30a4\u3057\u305f Service \u307e\u305f\u306f Ingress \u306b\u30a2\u30af\u30bb\u30b9\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b9\u30bf\u4f5c\u6210\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u3001\u9805\u76ee\u5165\u529b\u3057\u3066 Create cluster\u3002 Attention Web UI \u7d4c\u7531\u3067\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u5834\u5408\u306b volume.storageClass \u3092\u6307\u5b9a\u3067\u304d\u307e\u305b\u3093\u3002 \u305d\u306e\u307e\u307e\u3067\u306f PVC \u304c Pending \u306e\u307e\u307e\u3067\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u3002 \u305d\u306e\u305f\u3081\u3001Rook Ceph \u3067\u4f5c\u6210\u3057\u305f storageClass \u306a\u3069\u3092\u6307\u5b9a\u3059\u308b\u5834\u5408\u306f\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4f7f\u3044\u307e\u3059\u3002 Web UI \u3067\u306f\u57fa\u672c\u7684\u306a\u60c5\u5831\u3092\u57cb\u3081\u8fbc\u3093\u3060\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4f5c\u6210\u3057\u3001\u7d30\u304b\u3044\u8a2d\u5b9a\u306f\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306b\u76f4\u63a5\u8a18\u8ff0\u3059\u308b\u306e\u304c\u826f\u3055\u305d\u3046\u3067\u3059\u3002 \u78ba\u8a8d \u4f5c\u6210\u3057\u305f Postgresql \u30af\u30e9\u30b9\u30bf\u306b\u63a5\u7d9a\u3067\u304d\u308b\u304b\u78ba\u8a8d\u3057\u307e\u3059\u3002 \u30e6\u30fc\u30b6\u30d1\u30b9\u30ef\u30fc\u30c9 \u30e6\u30fc\u30b6\u30d1\u30b9\u30ef\u30fc\u30c9\u306f Operator \u306b\u3088\u308a\u81ea\u52d5\u7684\u306b\u4f5c\u6210\u3055\u308c\u3001 {username}.{team}-{clustername}.credentials.postgresql.acid.zalan.do \u3068\u3044\u3046 Secret \u306b\u683c\u7d0d\u3055\u308c\u307e\u3059\u3002 # namespace: default export PGMASTER = $( kubectl get pods -o jsonpath ={ .items..metadata.name } -l application = spilo ) kubectl port-forward $PGMASTER 6432 :5432 kubectl get secret postgres.acid-example.credentials -o 'jsonpath={.data.password}' | base64 -d export PGSSLMODE = require psql -U postgres -h localhost -p 6432 CentOS 7 \u3078\u306e psql \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo yum install -y postgresql13 \u8a3c\u660e\u66f8 Custom TLS certificates \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u30af\u30e9\u30b9\u30bf\u4f5c\u6210\u6642\u306b TLS \u8a3c\u660e\u66f8\u304c\u81ea\u52d5\u7684\u306b\u4f5c\u6210\u3055\u308c\u308b\u306e\u3067\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u30ab\u30b9\u30bf\u30e0\u8a3c\u660e\u66f8\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 # Secret kubectl create secret tls pg-tls \\ --key pg-tls.key \\ # \u79d8\u5bc6\u9375 --cert pg-tls.crt # \u8a3c\u660e\u66f8 # kind: postgresql spec : tls : secretName : pg-tls pg_hba.conf \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067 pg_hba.conf \u306e\u8a2d\u5b9a\u3092\u4e0a\u66f8\u304d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 spec : patroni : pg_hba : - local all all trust - hostssl all all 0.0.0.0/0 trust - host all all 0.0.0.0/0 trust \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002 root@acid-awx-0:/home/postgres/pgdata/pgroot/data# cat pg_hba.conf # Do not edit this file manually! # It will be overwritten by Patroni! local all all trust hostssl all +zalandos 127 .0.0.1/32 pam host all all 127 .0.0.1/32 md5 hostssl all +zalandos ::1/128 pam host all all ::1/128 md5 hostssl replication standby all md5 hostnossl all all all reject hostssl all +zalandos all pam hostssl all all all md5","title":"Zalando Postgres Operator"},{"location":"kubernetes/operator/zalando-postgres-operator/#zalando-postgres-operator","text":"Zalando \u793e\u304c\u958b\u767a\u3057\u3066\u3044\u308b Postgres Operator \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u624b\u9806\u3067\u3059\u3002","title":"Zalando Postgres Operator"},{"location":"kubernetes/operator/zalando-postgres-operator/#operator","text":"\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 3\u7a2e\u985e\u306e\u30c7\u30d7\u30ed\u30a4\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002 \u30de\u30cb\u30e5\u30a2\u30eb Kustomize Helm chart","title":"Operator \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/operator/zalando-postgres-operator/#_1","text":"git clone https://github.com/zalando/postgres-operator.git cd postgres-operator/ kubectl create namespace zalando-postgres config set-context $( kubectl config current-context ) --namespace = zalando-postgres kubectl apply -f manifests/configmap.yaml kubectl apply -f manifests/operator-service-account-rbac.yaml # namespace: defualt \u306e\u3082\u306e\u304c\u3042\u308b\u306e\u3067\u6ce8\u610f kubectl apply -f manifests/postgres-operator.yaml kubectl apply -f manifests/api-service.yaml","title":"\u30de\u30cb\u30e5\u30a2\u30eb"},{"location":"kubernetes/operator/zalando-postgres-operator/#web-ui","text":"Web UI \u304b\u3089 PostgreSQL \u30af\u30e9\u30b9\u30bf\u3092\u4f5c\u6210\u30fb\u7ba1\u7406\u3067\u304d\u307e\u3059\u3002","title":"Web UI \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/operator/zalando-postgres-operator/#_2","text":"Web UI \u7528\u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4f7f\u3063\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 kubectl apply -f ui/manifests/ Web UI \u306f\u4efb\u610f\u306e namespace \u3092\u7ba1\u7406\u5bfe\u8c61\u3068\u3057\u307e\u3059\u3002 deployment.yaml \u306e TARGET_NAMESPACE \u306b \"*\" \u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u5168 namespace \u3092\u5bfe\u8c61\u3068\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 # deployment.yaml env : - name : \"TARGET_NAMESPACE\" value : \"default\" # \"*\" \u306b\u5909\u66f4 Attention kustomize \u306e\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u30c7\u30d7\u30ed\u30a4\u3067\u304d\u3066\u307e\u305b\u3093\u3002 error: unable to recognize \"manifests/kustomization.yaml\" : no matches for kind \"Kustomization\" in version \"kustomize.config.k8s.io/v1beta1\" Note \u81ea\u5b85\u30e9\u30dc\u3067\u306f Ingress \u3067\u306f\u306a\u304f type: Loadbalancer \u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002","title":"\u30de\u30cb\u30e5\u30a2\u30eb"},{"location":"kubernetes/operator/zalando-postgres-operator/#postgresql","text":"Postgres Operator \u3092\u4f7f\u3063\u3066 PostgreSQL \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u65b9\u6cd5\u306f2\u7a2e\u985e\u3042\u308a\u307e\u3059\u3002 \u30de\u30cb\u30d5\u30a7\u30b9\u30c8 Web UI","title":"PostgreSQL \u30c7\u30d7\u30ed\u30a4"},{"location":"kubernetes/operator/zalando-postgres-operator/#_3","text":"\u516c\u5f0f\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30b5\u30f3\u30d7\u30eb\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u304c\u3042\u308b\u306e\u3067\u3001\u53c2\u7167\u3057\u306a\u304c\u3089 Postgres \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u7528\u610f\u3057\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002 # example apiVersion : acid.zalan.do/v1 kind : postgresql metadata : name : acid-example namespace : default labels : team : acid spec : teamId : acid postgresql : version : \"13\" # note: String numberOfInstances : 1 volume : size : 1Gi storageClass : rook-ceph-block users : example : [] databases : example : example allowedSourceRanges : resources : requests : cpu : 100m memory : 100Mi limits : cpu : 500m memory : 500Mi","title":"\u30de\u30cb\u30d5\u30a7\u30b9\u30c8"},{"location":"kubernetes/operator/zalando-postgres-operator/#web-ui_1","text":"\u30c7\u30d7\u30ed\u30a4\u3057\u305f Service \u307e\u305f\u306f Ingress \u306b\u30a2\u30af\u30bb\u30b9\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b9\u30bf\u4f5c\u6210\u753b\u9762\u304c\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u3001\u9805\u76ee\u5165\u529b\u3057\u3066 Create cluster\u3002 Attention Web UI \u7d4c\u7531\u3067\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u5834\u5408\u306b volume.storageClass \u3092\u6307\u5b9a\u3067\u304d\u307e\u305b\u3093\u3002 \u305d\u306e\u307e\u307e\u3067\u306f PVC \u304c Pending \u306e\u307e\u307e\u3067\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u3002 \u305d\u306e\u305f\u3081\u3001Rook Ceph \u3067\u4f5c\u6210\u3057\u305f storageClass \u306a\u3069\u3092\u6307\u5b9a\u3059\u308b\u5834\u5408\u306f\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4f7f\u3044\u307e\u3059\u3002 Web UI \u3067\u306f\u57fa\u672c\u7684\u306a\u60c5\u5831\u3092\u57cb\u3081\u8fbc\u3093\u3060\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u4f5c\u6210\u3057\u3001\u7d30\u304b\u3044\u8a2d\u5b9a\u306f\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u306b\u76f4\u63a5\u8a18\u8ff0\u3059\u308b\u306e\u304c\u826f\u3055\u305d\u3046\u3067\u3059\u3002","title":"Web UI"},{"location":"kubernetes/operator/zalando-postgres-operator/#_4","text":"\u4f5c\u6210\u3057\u305f Postgresql \u30af\u30e9\u30b9\u30bf\u306b\u63a5\u7d9a\u3067\u304d\u308b\u304b\u78ba\u8a8d\u3057\u307e\u3059\u3002","title":"\u78ba\u8a8d"},{"location":"kubernetes/operator/zalando-postgres-operator/#_5","text":"\u30e6\u30fc\u30b6\u30d1\u30b9\u30ef\u30fc\u30c9\u306f Operator \u306b\u3088\u308a\u81ea\u52d5\u7684\u306b\u4f5c\u6210\u3055\u308c\u3001 {username}.{team}-{clustername}.credentials.postgresql.acid.zalan.do \u3068\u3044\u3046 Secret \u306b\u683c\u7d0d\u3055\u308c\u307e\u3059\u3002 # namespace: default export PGMASTER = $( kubectl get pods -o jsonpath ={ .items..metadata.name } -l application = spilo ) kubectl port-forward $PGMASTER 6432 :5432 kubectl get secret postgres.acid-example.credentials -o 'jsonpath={.data.password}' | base64 -d export PGSSLMODE = require psql -U postgres -h localhost -p 6432","title":"\u30e6\u30fc\u30b6\u30d1\u30b9\u30ef\u30fc\u30c9"},{"location":"kubernetes/operator/zalando-postgres-operator/#centos-7-psql","text":"sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo yum install -y postgresql13","title":"CentOS 7 \u3078\u306e psql \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"kubernetes/operator/zalando-postgres-operator/#_6","text":"Custom TLS certificates \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u30af\u30e9\u30b9\u30bf\u4f5c\u6210\u6642\u306b TLS \u8a3c\u660e\u66f8\u304c\u81ea\u52d5\u7684\u306b\u4f5c\u6210\u3055\u308c\u308b\u306e\u3067\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u30ab\u30b9\u30bf\u30e0\u8a3c\u660e\u66f8\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 # Secret kubectl create secret tls pg-tls \\ --key pg-tls.key \\ # \u79d8\u5bc6\u9375 --cert pg-tls.crt # \u8a3c\u660e\u66f8 # kind: postgresql spec : tls : secretName : pg-tls","title":"\u8a3c\u660e\u66f8"},{"location":"kubernetes/operator/zalando-postgres-operator/#pg_hbaconf","text":"\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067 pg_hba.conf \u306e\u8a2d\u5b9a\u3092\u4e0a\u66f8\u304d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 spec : patroni : pg_hba : - local all all trust - hostssl all all 0.0.0.0/0 trust - host all all 0.0.0.0/0 trust \u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002 root@acid-awx-0:/home/postgres/pgdata/pgroot/data# cat pg_hba.conf # Do not edit this file manually! # It will be overwritten by Patroni! local all all trust hostssl all +zalandos 127 .0.0.1/32 pam host all all 127 .0.0.1/32 md5 hostssl all +zalandos ::1/128 pam host all all ::1/128 md5 hostssl replication standby all md5 hostnossl all all all reject hostssl all +zalandos all pam hostssl all all all md5","title":"pg_hba.conf"},{"location":"prometheus/prometheus/","text":"Prometheus kube-prometheus sudo yum install epel-release sudo yum install golang export GOPATH = /home/nnstt1/go export PATH = $PATH : $GOPATH /bin GO111MODULE = \"on\" go get github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb go get github.com/google/go-jsonnet/cmd/jsonnet go get github.com/brancz/gojsontoyaml jb install github.com/prometheus-operator/kube-prometheus/jsonnet/kube-prometheus@release-0.6 kubectl apply -f manifests/setup kubectl apply -f manifests/ kubectl get all -n monitoring kubectl apply -f grafana-service.yaml # grafana-service.yaml apiVersion : v1 kind : Service metadata : name : grafana labels : app : grafana namespace : monitoring spec : ports : - port : 80 targetPort : 3000 selector : app : grafana type : LoadBalancer","title":"Prometheus"},{"location":"prometheus/prometheus/#prometheus","text":"","title":"Prometheus"},{"location":"prometheus/prometheus/#kube-prometheus","text":"sudo yum install epel-release sudo yum install golang export GOPATH = /home/nnstt1/go export PATH = $PATH : $GOPATH /bin GO111MODULE = \"on\" go get github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb go get github.com/google/go-jsonnet/cmd/jsonnet go get github.com/brancz/gojsontoyaml jb install github.com/prometheus-operator/kube-prometheus/jsonnet/kube-prometheus@release-0.6 kubectl apply -f manifests/setup kubectl apply -f manifests/ kubectl get all -n monitoring kubectl apply -f grafana-service.yaml # grafana-service.yaml apiVersion : v1 kind : Service metadata : name : grafana labels : app : grafana namespace : monitoring spec : ports : - port : 80 targetPort : 3000 selector : app : grafana type : LoadBalancer","title":"kube-prometheus"},{"location":"prometheus/snmp-exporter/","text":"SNMP Exporter SNMP Exporter \u306e\u8a2d\u5b9a\u65b9\u6cd5\u3067\u3059\u3002 generator https://github.com/prometheus/snmp_exporter/tree/master/generator Config Generator \u3092\u4f7f\u3063\u3066 SNMP Exporter \u7528\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb snmp.yml \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 generator \u306e\u5229\u7528\u65b9\u6cd5\u306f2\u901a\u308a\u3042\u308a\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304b\u3089\u30d3\u30eb\u30c9 Docker \u30a4\u30e1\u30fc\u30b8 \u3053\u3053\u3067\u306f Docker \u30a4\u30e1\u30fc\u30b8\u3092\u5229\u7528\u3059\u308b\u65b9\u6cd5\u3092\u8a18\u8f09\u3057\u307e\u3059\u3002 Docker Build \u516c\u5f0f\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u3042\u308b Dockerfile \u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u30d3\u30eb\u30c9\u3057\u307e\u3059\u3002 # Dockerfile FROM golang:latest RUN apt-get update && \\ apt-get install -y libsnmp-dev p7zip-full && \\ go get github.com/prometheus/snmp_exporter/generator && \\ cd /go/src/github.com/prometheus/snmp_exporter/generator && \\ go get -v . && \\ go install WORKDIR \"/opt\" ENTRYPOINT [ \"/go/bin/generator\" ] ENV MIBDIRS mibs CMD [ \"generate\" ] git clone https://github.com/prometheus/snmp_exporter.git cd snmp_exporter/generator docker build -t snmp-generator . Docker Run cd <work-dir> docker run -ti \\ -v \" ${ PWD } :/opt/\" \\ snmp-generator generate","title":"SNMP Exporter"},{"location":"prometheus/snmp-exporter/#snmp-exporter","text":"SNMP Exporter \u306e\u8a2d\u5b9a\u65b9\u6cd5\u3067\u3059\u3002","title":"SNMP Exporter"},{"location":"prometheus/snmp-exporter/#generator","text":"https://github.com/prometheus/snmp_exporter/tree/master/generator Config Generator \u3092\u4f7f\u3063\u3066 SNMP Exporter \u7528\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb snmp.yml \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 generator \u306e\u5229\u7528\u65b9\u6cd5\u306f2\u901a\u308a\u3042\u308a\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304b\u3089\u30d3\u30eb\u30c9 Docker \u30a4\u30e1\u30fc\u30b8 \u3053\u3053\u3067\u306f Docker \u30a4\u30e1\u30fc\u30b8\u3092\u5229\u7528\u3059\u308b\u65b9\u6cd5\u3092\u8a18\u8f09\u3057\u307e\u3059\u3002","title":"generator"},{"location":"prometheus/snmp-exporter/#docker-build","text":"\u516c\u5f0f\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u3042\u308b Dockerfile \u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u30d3\u30eb\u30c9\u3057\u307e\u3059\u3002 # Dockerfile FROM golang:latest RUN apt-get update && \\ apt-get install -y libsnmp-dev p7zip-full && \\ go get github.com/prometheus/snmp_exporter/generator && \\ cd /go/src/github.com/prometheus/snmp_exporter/generator && \\ go get -v . && \\ go install WORKDIR \"/opt\" ENTRYPOINT [ \"/go/bin/generator\" ] ENV MIBDIRS mibs CMD [ \"generate\" ] git clone https://github.com/prometheus/snmp_exporter.git cd snmp_exporter/generator docker build -t snmp-generator .","title":"Docker Build"},{"location":"prometheus/snmp-exporter/#docker-run","text":"cd <work-dir> docker run -ti \\ -v \" ${ PWD } :/opt/\" \\ snmp-generator generate","title":"Docker Run"},{"location":"vmware/esxi/","text":"ESXi SNMP ssh \u3067 ESXi \u306b\u30ed\u30b0\u30a4\u30f3 esxcli system snmp set --communities public esxcli system snmp set --enable true esxcli network firewall ruleset set --ruleset-id = snmp --allowed-all true esxcli network firewall ruleset set --ruleset-id = snmp --enabled true /etc/init.d/snmpd start","title":"ESXi"},{"location":"vmware/esxi/#esxi","text":"","title":"ESXi"},{"location":"vmware/esxi/#snmp","text":"ssh \u3067 ESXi \u306b\u30ed\u30b0\u30a4\u30f3 esxcli system snmp set --communities public esxcli system snmp set --enable true esxcli network firewall ruleset set --ruleset-id = snmp --allowed-all true esxcli network firewall ruleset set --ruleset-id = snmp --enabled true /etc/init.d/snmpd start","title":"SNMP"}]}